{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"TorchTrade Documentation","text":"<p>Welcome to the TorchTrade documentation! TorchTrade is a machine learning framework for algorithmic trading built on TorchRL.</p> <p>TorchTrade's goal is to provide accessible deployment of RL methods to trading. The framework supports various RL methodologies including online RL, offline RL, model-based RL, contrastive learning, and many more areas of reinforcement learning research. Beyond RL, TorchTrade integrates traditional trading methods such as rule-based strategies, as well as modern approaches including LLMs (both local models and frontier model integrations) as trading actors.</p>"},{"location":"#what-is-torchtrade","title":"What is TorchTrade?","text":"<p>TorchTrade provides modular environments for both live trading with major exchanges and offline backtesting. The framework supports:</p> <ul> <li>\ud83c\udfaf Multi-Timeframe Observations - Train on 1m, 5m, 15m, 1h bars simultaneously</li> <li>\ud83e\udd16 Multiple RL Algorithms - PPO, IQL, GRPO, DSAC implementations</li> <li>\ud83d\udcca Feature Engineering - Add technical indicators and custom features</li> <li>\ud83d\udd34 Live Trading - Direct API integration with major exchanges</li> <li>\ud83d\udcc9 Risk Management - Stop-loss/take-profit, margin, leverage, liquidation mechanics</li> <li>\ud83d\udd2e Futures Trading - Up to 125x leverage with proper margin management</li> <li>\ud83d\udce6 Ready-to-Use Datasets - Pre-processed OHLCV data available at HuggingFace/Torch-Trade</li> </ul>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation &amp; Setup - Get up and running in minutes</li> <li>First Environment - Create and run your first trading environment</li> <li>First Training Run - Train a PPO policy</li> </ul>"},{"location":"#environments","title":"Environments","text":"<ul> <li>Offline Environments - Backtesting with historical data</li> <li>SeqLongOnlyEnv, SeqFuturesEnv, OneStepEnv variants</li> <li>Online Environments - Live trading with exchange APIs</li> <li>Alpaca, Binance, Bitget integrations</li> </ul>"},{"location":"#components","title":"Components","text":"<ul> <li>Loss Functions - Training objectives (GRPOLoss, CTRLLoss, CTRLPPOLoss)</li> <li>Transforms - Data preprocessing (CoverageTracker, ChronosEmbeddingTransform)</li> <li>Actors - Trading policies (RuleBasedActor, LLMActor, LocalLLMActor, HumanActor)</li> </ul>"},{"location":"#advanced-customization","title":"Advanced Customization","text":"<ul> <li>Feature Engineering - Add technical indicators and features</li> <li>Reward Functions - Design reward functions for your strategy</li> <li>Performance Metrics - Evaluate and customize trading performance metrics</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#multi-timeframe-support","title":"Multi-Timeframe Support","text":"<p>Observe market data at multiple time scales simultaneously:</p> <pre><code>config = SeqLongOnlyEnvConfig(\n    time_frames=[\"1min\", \"5min\", \"15min\", \"60min\"],\n    window_sizes=[12, 8, 8, 24],       # Lookback per timeframe\n    execute_on=(5, \"Minute\")           # Execute every 5 minutes\n)\n</code></pre>"},{"location":"#futures-trading-with-leverage","title":"Futures Trading with Leverage","text":"<p>Trade with leverage and manage margin:</p> <pre><code>config = SeqFuturesEnvConfig(\n    leverage=10,                       # 10x leverage\n    initial_cash=10000,\n    margin_call_threshold=0.2,         # 20% margin ratio triggers liquidation\n)\n</code></pre>"},{"location":"#stop-loss-take-profit-bracket-orders","title":"Stop-Loss / Take-Profit Bracket Orders","text":"<p>Risk management with combinatorial action spaces:</p> <pre><code>config = SeqLongOnlySLTPEnvConfig(\n    stoploss_levels=[-0.02, -0.05],    # -2%, -5%\n    takeprofit_levels=[0.05, 0.10],    # +5%, +10%\n    include_hold_action=True,          # Optional: set False to remove HOLD\n)\n# Action space: HOLD + (2 SL \u00d7 2 TP) = 5 actions (or 4 without HOLD)\n</code></pre>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>Raw OHLCV Data (1-minute bars)\n    \u2193\nMarketDataObservationSampler\n    \u251c\u2500\u2500 Resample to multiple timeframes\n    \u251c\u2500\u2500 Apply feature preprocessing\n    \u2514\u2500\u2500 Create sliding windows\n    \u2193\nTensorDict Observations\n    \u251c\u2500\u2500 market_data_* (per timeframe)\n    \u2514\u2500\u2500 account_state (cash, position, PnL)\n    \u2193\nTorchRL Environment (EnvBase)\n    \u251c\u2500\u2500 _reset() - Initialize episode\n    \u251c\u2500\u2500 _step(action) - Execute trade\n    \u251c\u2500\u2500 _calculate_reward() - Compute reward\n    \u2514\u2500\u2500 _check_termination() - Check end\n    \u2193\nLoss Function (PPO/IQL/GRPO/DSAC)\n    \u2514\u2500\u2500 Optimizer \u2192 Policy Update\n</code></pre>"},{"location":"#environment-comparison","title":"Environment Comparison","text":""},{"location":"#offline-environments-backtesting","title":"Offline Environments (Backtesting)","text":"Environment Futures Leverage Bracket Orders One-Step Best For SeqLongOnlyEnv \u274c \u274c \u274c \u274c Beginners, simple strategies SeqLongOnlySLTPEnv \u274c \u274c \u2705 \u274c Risk management research LongOnlyOneStepEnv \u274c \u274c \u2705 \u2705 GRPO, contextual bandits SeqFuturesEnv \u2705 \u2705 \u274c \u274c Advanced futures backtesting SeqFuturesSLTPEnv \u2705 \u2705 \u2705 \u274c Risk-managed futures FuturesOneStepEnv \u2705 \u2705 \u2705 \u2705 Fast futures iteration"},{"location":"#live-environments-exchange-apis","title":"Live Environments (Exchange APIs)","text":"Environment Exchange Futures Leverage Bracket Orders AlpacaTorchTradingEnv Alpaca \u274c \u274c \u274c AlpacaSLTPTorchTradingEnv Alpaca \u274c \u274c \u2705 BinanceFuturesTorchTradingEnv Binance \u2705 \u2705 \u274c BinanceFuturesSLTPTorchTradingEnv Binance \u2705 \u2705 \u2705 BitgetFuturesTorchTradingEnv Bitget \u2705 \u2705 \u274c BitgetFuturesSLTPTorchTradingEnv Bitget \u2705 \u2705 \u2705"},{"location":"#next-steps","title":"Next Steps","text":"<p>Ready to get started? Head to the Getting Started Guide to install TorchTrade and run your first environment!</p> <p>Already familiar with the basics? Check out:</p> <ul> <li>Offline Environments - Deep dive into backtesting environments</li> <li>Reward Functions - Design better reward signals</li> <li>Building Custom Environments - Extend the framework</li> </ul>"},{"location":"#support","title":"Support","text":"<ul> <li>\ud83d\udcac Questions: GitHub Discussions</li> <li>\ud83d\udc1b Bug Reports: GitHub Issues</li> <li>\ud83d\udce7 Email: torchtradecontact@gmail.com</li> </ul> <p>Built with TorchRL \u2022 Designed for Algorithmic Trading \u2022 Open Source</p>"},{"location":"examples/","title":"Examples","text":"<p>TorchTrade provides a collection of example training scripts to help you get started. These examples are designed for inspiration and learning - use them as starting points to build your own custom training pipelines.</p>"},{"location":"examples/#design-philosophy","title":"Design Philosophy","text":"<p>TorchTrade examples closely follow the structure of TorchRL's SOTA implementations, enabling near plug-and-play compatibility with any TorchRL algorithm. This means:</p> <ul> <li>Familiar structure if you've used TorchRL before</li> <li>Easy adaptation of TorchRL algorithms to trading environments</li> <li>Minimal boilerplate - focus on what's unique to your strategy</li> <li>Hydra configuration for easy experimentation</li> </ul>"},{"location":"examples/#direct-compatibility-with-torchrl-sota-implementations","title":"Direct Compatibility with TorchRL SOTA Implementations","text":"<p>To demonstrate the closeness to TorchRL's SOTA implementations, here is a direct comparison:</p> <p>TorchRL's A2C Example: <pre><code>from torchrl.envs import GymEnv\n\n# Environment setup\nenv = GymEnv(\"CartPole-v1\")\n\n# Everything else stays the same\ncollector = SyncDataCollector(env, policy, ...)\nloss_module = A2CLoss(actor, critic, ...)\noptimizer = torch.optim.Adam(loss_module.parameters(), lr=1e-3)\n\nfor batch in collector:\n    loss_values = loss_module(batch)\n    loss_values[\"loss\"].backward()\n    optimizer.step()\n</code></pre></p> <p>TorchTrade Adaptation (Only Environment Changes): <pre><code>from torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\n\n# Environment setup - ONLY CHANGE\nenv = SeqLongOnlyEnv(df, SeqLongOnlyEnvConfig(...))\n\n# Everything else stays EXACTLY the same\ncollector = SyncDataCollector(env, policy, ...)\nloss_module = A2CLoss(actor, critic, ...)\noptimizer = torch.optim.Adam(loss_module.parameters(), lr=1e-3)\n\nfor batch in collector:\n    loss_values = loss_module(batch)\n    loss_values[\"loss\"].backward()\n    optimizer.step()\n</code></pre></p> <p>That's it! The collector, loss function, optimizer, and training loop remain identical. This doesn't only allow the use of any TorchRL algorithm, but any of the other useful components of TorchRL - replay buffers, transforms, modules, data structures - and provides seamless integration into the entire TorchRL ecosystem.</p>"},{"location":"examples/#example-file-structure","title":"Example File Structure","text":"<p>Each example in TorchTrade follows a consistent file organization pattern that separates configuration, utilities, and training logic:</p> <pre><code>examples/online/&lt;algorithm&gt;/\n\u251c\u2500\u2500 config.yaml           # Main configuration file\n\u251c\u2500\u2500 utils.py             # Helper functions (env creation, network setup, etc.)\n\u2514\u2500\u2500 train.py            # Training script and main loop\n</code></pre>"},{"location":"examples/#what-each-component-does","title":"What Each Component Does","text":"<p><code>config.yaml</code> - Configuration Management</p> <p>The configuration file uses Hydra to manage all hyperparameters and settings. This includes:</p> <ul> <li>Environment settings: Symbol, timeframes, initial cash, transaction fees, window sizes</li> <li>Network architecture: Hidden dimensions, activation functions, layer configurations</li> <li>Training hyperparameters: Learning rate, batch size, discount factor (gamma), entropy coefficient</li> <li>Collector settings: Frames per batch, number of parallel environments</li> <li>Logging: Wandb project name, experiment tracking settings</li> </ul> <p>By centralizing all parameters in YAML, you can easily experiment with different configurations without modifying code. Hydra also allows you to override any parameter from the command line:</p> <pre><code># Override multiple parameters\npython train.py env.symbol=\"ETH/USD\" optim.lr=1e-4 loss.gamma=0.95\n</code></pre> <p>Example config.yaml: <pre><code>env:\n  name: SeqLongOnlyEnv\n  symbol: \"BTC/USD\"\n  time_frames: [\"5Min\", \"15Min\"]\n  window_sizes: [10, 10]\n  execute_on: \"15Min\"\n  initial_cash: [1000, 5000]\n  transaction_fee: 0.0025\n  train_envs: 10\n  eval_envs: 2\n\ncollector:\n  frames_per_batch: 100000\n  total_frames: 100_000_000\n\noptim:\n  lr: 2.5e-4\n  max_grad_norm: 0.5\n  anneal_lr: True\n\nloss:\n  gamma: 0.9\n  mini_batch_size: 33333\n  ppo_epochs: 3\n  clip_epsilon: 0.1\n  entropy_coef: 0.01\n\nlogger:\n  backend: wandb\n  project_name: TorchTrade-Online\n  exp_name: ppo\n</code></pre></p> <p><code>utils.py</code> - Helper Functions</p> <p>This file contains modular helper functions that handle setup tasks:</p> <ul> <li><code>make_env()</code>: Creates and configures the trading environment (offline or online)</li> <li><code>make_actor()</code>: Builds the policy network architecture (deterministic or stochastic)</li> <li><code>make_critic()</code>: Creates the value function network (if needed for the algorithm)</li> <li><code>make_loss()</code>: Initializes the loss module (PPO, SAC, IQL, etc.)</li> <li><code>make_collector()</code>: Sets up the data collection pipeline</li> </ul> <p>These utility functions keep the main training script clean and make it easy to swap components (e.g., changing from SeqLongOnlyEnv to SeqFuturesEnv requires only modifying <code>make_env()</code>).</p> <p><code>train.py</code> - Training Loop</p> <p>The main training script orchestrates everything:</p> <ol> <li>Hydra initialization: Loads configuration from <code>config.yaml</code></li> <li>Component creation: Uses <code>utils.py</code> functions to create env, actor, loss, optimizer</li> <li>Training loop: Collects data, computes losses, updates policy, logs metrics</li> <li>Evaluation: Periodically evaluates the policy on test environments</li> <li>Checkpointing: Saves model weights and training state</li> </ol> <p>This structure mirrors TorchRL's SOTA implementations, making it familiar to TorchRL users and easy to adapt existing algorithms.</p>"},{"location":"examples/#available-examples","title":"Available Examples","text":"<p>The following examples demonstrate the flexibility of TorchTrade across different algorithms, environments, and use cases. These examples are meant to be starting points for further experimentation and adaptation - customize them according to your needs, ideas, and environments.</p>"},{"location":"examples/#online-rl-offline-backtesting-environments","title":"Online RL (Offline Backtesting Environments)","text":"<p>These examples use online RL algorithms (learning from interaction as it happens) with historical market data for backtesting. This allows you to train policies on past data before deploying them to live trading environments. We typically split the training data into training and test environments to evaluate the generalization performance of learned policies on unseen market conditions.</p> <p>Located in <code>examples/online/</code>:</p> Example Algorithm Environment Key Features ppo/ PPO SeqLongOnlyEnv Standard policy gradient with multi-timeframe observations ppo_futures/ PPO SeqFuturesEnv Futures trading with leverage and margin management ppo_futures_sltp/ PPO SeqFuturesSLTPEnv Futures with bracket orders (stop-loss/take-profit) ppo_futures_onestep/ PPO FuturesOneStepEnv One-step futures for episodic optimization ppo_chronos/ PPO SeqLongOnlyEnv + ChronosEmbedding Time series embedding with Chronos T5 models iql/ IQL (Implicit Q-Learning) SeqLongOnlyEnv Offline RL algorithm for sequential trading dsac/ DSAC (Distributional SAC) SeqLongOnlyEnv Soft actor-critic with distributional value functions grpo_futures_onestep/ GRPO FuturesOneStepEnv Group relative policy optimization for one-step RL long_onestep_env/ GRPO LongOnlyOneStepEnv One-step long-only with SLTP bracket orders"},{"location":"examples/#offline-rl","title":"Offline RL","text":"<p>These examples use offline RL algorithms that learn from pre-collected datasets without requiring live environment interaction during training. The data can be collected from interactions with offline backtesting environments or from real online live trading sessions. We provide simple example offline datasets at HuggingFace/Torch-Trade.</p> <p>Located in <code>examples/offline/</code>:</p> Example Algorithm Environment Key Features iql/ IQL SeqLongOnlyEnv Offline RL from pre-collected trajectories <p>Note: Thanks to the compatibility with TorchRL, you can easily add other offline RL methods like CQL, TD3+BC, and Decision Transformers from TorchRL to do offline RL with TorchTrade environments.</p>"},{"location":"examples/#llm-actors","title":"LLM Actors","text":"<p>TorchTrade implements LLMActor that allows the integration of local LLMs for trading decision-making. You can use models directly from HuggingFace Models or quantized models from Unsloth for memory-efficient and fast interactions. This approach leverages the reasoning capabilities of LLMs to make trading decisions and provides fertile soil for new ideas and adaptations for trading with LLMs thanks to TorchTrade.</p> <p>Located in <code>examples/llm/</code>:</p> Example Type Description local/ LocalLLMActor Trading with local LLMs (vLLM/transformers backend) <p>Future Work: We plan to provide example scripts for fine-tuning LLMs on reasoning traces from frontier models for trading. Additionally, we are working on integrating Vision-Language Models (VLMs) to process trading chart plots for decision-making.</p>"},{"location":"examples/#rule-based-actors","title":"Rule-Based Actors","text":"<p>TorchTrade provides actor classes that allow easy creation of rule-based trading strategies using technical indicators and market signals, for example mean reversion, breakout, and more. These rule-based actors integrate seamlessly with TorchTrade environments for both backtesting and live trading, serving as baselines or components in hybrid approaches. This is especially interesting with our custom feature preprocessing, which allows you to add technical indicators and derived features to enhance rule-based strategies.</p> <p>Located in <code>examples/online/rulebased/</code>:</p> Example Actor Type Environment Description rulebased/ MeanReversionActor SeqLongOnlyEnv Mean reversion strategy using Bollinger Bands and Stochastic RSI <p>Future Work: We plan to provide examples of hybrid approaches that combine rule-based policies with neural network policies as actors, leveraging the strengths of both deterministic strategies and learned behaviors.</p>"},{"location":"examples/#live-trading","title":"Live Trading","text":"<p>These examples demonstrate deploying trained policies to real exchange APIs for live trading. We strongly recommend starting with paper trading to validate your strategies risk-free before transitioning to live capital deployment.</p> <p>Located in <code>examples/live/</code>:</p> Example Exchange Description alpaca/ Alpaca Live paper trading with Alpaca API"},{"location":"examples/#transforms","title":"Transforms","text":"<p>Inspired by work such as R3M and VIP that utilize large pretrained models for representation learning, we created the ChronosEmbeddingTransform using Chronos forecasting models to embed historical trading data. This demonstrates the flexibility and adaptability of TorchTrade for integrating pretrained models as transforms for enhanced feature representations.</p> <p>Located in <code>examples/transforms/</code>:</p> Example Transform Description chronos_embedding_example.py ChronosEmbeddingTransform Time series embedding with Chronos T5 models <p>Note: If you would like us to add additional transforms for other pretrained models (similar to ChronosEmbedding), we welcome GitHub issues with your requests. We're happy to implement these given the availability of model weights and resources.</p>"},{"location":"examples/#running-examples","title":"Running Examples","text":"<p>All examples use Hydra for configuration management:</p> <pre><code># Run with default configuration\nuv run python examples/online/ppo/train.py\n\n# Override config parameters\nuv run python examples/online/ppo/train.py \\\n    env.symbol=\"BTC/USD\" \\\n    optim.lr=1e-4 \\\n    collector.frames_per_batch=1000 \\\n    loss.gamma=0.95\n</code></pre>"},{"location":"examples/#common-hydra-overrides","title":"Common Hydra Overrides","text":"Parameter Example Description <code>env.symbol</code> <code>\"BTC/USD\"</code> Trading pair/symbol <code>env.initial_cash</code> <code>10000</code> Starting capital <code>env.time_frames</code> <code>'[\"1min\",\"5min\"]'</code> Multi-timeframe observations <code>optim.lr</code> <code>1e-4</code> Learning rate <code>loss.gamma</code> <code>0.99</code> Discount factor <code>collector.frames_per_batch</code> <code>2000</code> Frames collected per iteration <code>total_frames</code> <code>100000</code> Total training frames"},{"location":"examples/#example-structure","title":"Example Structure","text":"<p>Each training example follows this pattern:</p> <pre><code># 1. Configuration (via Hydra)\n@hydra.main(config_path=\"config\", config_name=\"config\")\ndef main(cfg):\n\n    # 2. Create environment\n    env = make_env(cfg)\n\n    # 3. Build policy network\n    actor = make_actor(cfg, env)\n\n    # 4. Create collector\n    collector = SyncDataCollector(\n        env,\n        actor,\n        frames_per_batch=cfg.collector.frames_per_batch,\n    )\n\n    # 5. Loss function\n    loss_module = make_loss(cfg, actor)\n\n    # 6. Optimizer\n    optimizer = torch.optim.Adam(\n        loss_module.parameters(),\n        lr=cfg.optim.lr\n    )\n\n    # 7. Training loop\n    for batch in collector:\n        loss_values = loss_module(batch)\n        loss = loss_values[\"loss_objective\"] + loss_values[\"loss_critic\"]\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre> <p>This structure mirrors TorchRL's SOTA implementations, making it easy to adapt other algorithms.</p>"},{"location":"examples/#configuration-files","title":"Configuration Files","text":"<p>Each example includes a <code>config/</code> directory with Hydra configs:</p> <pre><code>examples/online/ppo/\n\u251c\u2500\u2500 train.py              # Training script\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 config.yaml       # Main config\n\u2502   \u251c\u2500\u2500 env/\n\u2502   \u2502   \u2514\u2500\u2500 default.yaml  # Environment config\n\u2502   \u251c\u2500\u2500 collector/\n\u2502   \u2502   \u2514\u2500\u2500 default.yaml  # Data collection config\n\u2502   \u2514\u2500\u2500 loss/\n\u2502       \u2514\u2500\u2500 default.yaml  # Loss function config\n</code></pre> <p>This allows clean separation of concerns and easy experimentation.</p>"},{"location":"examples/#creating-your-own-examples","title":"Creating Your Own Examples","text":"<p>To create a new training script:</p> <ol> <li>Copy an existing example that's closest to your use case</li> <li>Modify the environment - change to your preferred env and config</li> <li>Customize the policy - adjust network architecture if needed</li> <li>Tune hyperparameters - update config files</li> <li>Add custom logic - rewards, features, transforms</li> </ol> <p>Tips: - Start with <code>ppo/</code> for standard RL - Start with <code>grpo_futures_onestep/</code> for one-step RL - Start with <code>ppo_chronos/</code> for time series embeddings - Start with <code>rulebased/</code> for non-RL baselines</p>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ul> <li>Offline Environments - Understand environment mechanics</li> <li>Reward Functions - Design better reward signals</li> <li>Feature Engineering - Add technical indicators</li> <li>Transforms - Use ChronosEmbedding and other transforms</li> <li>TorchRL SOTA Implementations - Explore more TorchRL algorithms</li> </ul>"},{"location":"examples/#support","title":"Support","text":"<ul> <li>\ud83d\udcac Questions: GitHub Discussions</li> <li>\ud83d\udc1b Bug Reports: GitHub Issues</li> <li>\ud83d\udce7 Email: torchtradecontact@gmail.com</li> </ul>"},{"location":"getting-started/","title":"Getting Started with TorchTrade","text":"<p>This guide will help you install TorchTrade and run your first trading environment.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>CUDA (optional, for GPU acceleration)</li> <li>UV - Fast Python package installer</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#1-install-uv","title":"1. Install UV","text":"<p>UV is a fast Python package installer and environment manager:</p> <pre><code># On Unix/macOS\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# On Windows (PowerShell)\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre>"},{"location":"getting-started/#2-clone-the-repository","title":"2. Clone the Repository","text":"<pre><code>git clone https://github.com/TorchTrade/torchtrade.git\ncd torchtrade\n</code></pre>"},{"location":"getting-started/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code># Install TorchTrade and all dependencies\nuv sync\n\n# Activate the virtual environment\nsource .venv/bin/activate  # On Unix/macOS\n# or\n.venv\\Scripts\\activate  # On Windows\n</code></pre>"},{"location":"getting-started/#4-verify-installation","title":"4. Verify Installation","text":"<pre><code># Run tests to verify everything works\nuv run pytest tests/ -v\n</code></pre>"},{"location":"getting-started/#your-first-environment","title":"Your First Environment","text":"<p>Let's create a simple trading environment using historical OHLCV data.</p>"},{"location":"getting-started/#step-1-prepare-your-data","title":"Step 1: Prepare Your Data","text":"<p>TorchTrade expects OHLCV data with the following columns:</p> <pre><code>import pandas as pd\n\n# Your data should have these columns\ndf = pd.DataFrame({\n    'timestamp': [...],  # datetime or parseable strings\n    'open': [...],       # opening prices\n    'high': [...],       # high prices\n    'low': [...],        # low prices\n    'close': [...],      # closing prices\n    'volume': [...]      # trading volume\n})\n</code></pre> <p>Note: You can also use our pre-processed datasets from HuggingFace/Torch-Trade which include various cryptocurrency pairs with 1-minute OHLCV data.</p>"},{"location":"getting-started/#step-2-create-an-environment","title":"Step 2: Create an Environment","text":"<pre><code>from torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\nimport pandas as pd\n\n# Load your data\ndf = pd.read_csv(\"btcusdt_1m.csv\")\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n\n# Configure environment\nconfig = SeqLongOnlyEnvConfig(\n    time_frames=[\"1min\", \"5min\", \"15min\"],        # 1m, 5m, 15m bars\n    window_sizes=[12, 8, 8],       # Lookback windows\n    execute_on=(5, \"Minute\"),      # Execute every 5 minutes\n    initial_cash=1000,             # Starting capital\n    transaction_fee=0.0025,        # 0.25% fee\n    slippage=0.001                 # 0.1% slippage\n)\n\n# Create environment\nenv = SeqLongOnlyEnv(df, config)\n</code></pre>"},{"location":"getting-started/#step-3-run-the-environment","title":"Step 3: Run the Environment","text":"<pre><code># Reset environment\ntensordict = env.reset()\n\nprint(\"Initial observation keys:\", tensordict.keys())\nprint(\"Market data shape:\", tensordict[\"market_data_5Minute\"].shape)\nprint(\"Account state:\", tensordict[\"account_state\"])\n\n# Take an action\ntensordict[\"action\"] = torch.tensor([2])  # BUY action\ntensordict = env.step(tensordict)\n\nprint(f\"Reward: {tensordict['reward'].item()}\")\nprint(f\"Done: {tensordict['done'].item()}\")\n</code></pre> <p>Note: TorchTrade uses a default log-return reward function, but you can customize it to shape agent behavior. See Reward Functions for examples including transaction cost penalties, Sharpe ratio rewards, and more.</p>"},{"location":"getting-started/#training-your-first-policy","title":"Training Your First Policy","text":"<p>Let's train a PPO policy on the long-only environment.</p>"},{"location":"getting-started/#quick-training-run","title":"Quick Training Run","text":"<pre><code># Train PPO with default config\nuv run python examples/online/ppo/train.py\n\n# Customize with Hydra overrides\nuv run python examples/online/ppo/train.py \\\n    env.symbol=\"BTC/USD\" \\\n    optim.lr=1e-4 \\\n    loss.gamma=0.95\n</code></pre> <p>Note: TorchTrade provides several example training scripts (PPO, IQL, DSAC, GRPO, etc.) designed for inspiration and learning. These examples follow the structure of TorchRL's SOTA implementations, enabling near plug-and-play compatibility with any TorchRL algorithm. See the Examples page for a complete list and usage guide.</p>"},{"location":"getting-started/#understanding-the-training-script","title":"Understanding the Training Script","text":"<p>The training script structure:</p> <pre><code>from torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\nfrom torchrl.collectors import SyncDataCollector\nfrom torchrl.modules import TanhNormal, ProbabilisticActor\n\n# 1. Create environment\nconfig = SeqLongOnlyEnvConfig(...)\nenv = SeqLongOnlyEnv(df, config)\n\n# 2. Create your custom policy architecture\n# TorchTrade provides simple default networks in the examples\n# See torchtrade/models/simple_encoders.py for reference implementations\n# Check the examples/ directory for more details on network architectures\nactor_net = YourCustomNetwork(...)\npolicy = ProbabilisticActor(...)\n\n# 3. Create data collector\ncollector = SyncDataCollector(\n    env,\n    policy,\n    frames_per_batch=10000,\n    total_frames=1_000_000\n)\n\n# 4. Training loop\nfor batch in collector:\n    loss = loss_module(batch)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"getting-started/#common-use-cases","title":"Common Use Cases","text":""},{"location":"getting-started/#loading-historical-data-from-huggingface","title":"Loading Historical Data from HuggingFace","text":"<pre><code>import datasets\n\n# Load pre-processed Bitcoin data\nds = datasets.load_dataset(\"Torch-Trade/btcusdt_spot_1m_01_2020_to_12_2025\")\ndf = ds[\"train\"].to_pandas()\ndf['0'] = pd.to_datetime(df['0'])  # First column is timestamp\ndf.columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n\n# Create environment\nenv = SeqLongOnlyEnv(df, config)\n</code></pre>"},{"location":"getting-started/#multi-timeframe-configuration","title":"Multi-Timeframe Configuration","text":"<pre><code>config = SeqLongOnlyEnvConfig(\n    time_frames=[\"1min\", \"5min\", \"15min\", \"60min\"],        # 1m, 5m, 15m, 1h\n    window_sizes=[12, 8, 8, 24],       # Lookback per timeframe\n    execute_on=(5, \"Minute\"),          # Execute every 5 minutes\n    initial_cash=[1000, 5000],         # Domain randomization\n)\n</code></pre> <p>The environment will provide observations: - <code>market_data_1Minute</code>: [12, num_features] - Last 12 one-minute bars - <code>market_data_5Minute</code>: [8, num_features] - Last 40 minutes - <code>market_data_15Minute</code>: [8, num_features] - Last 120 minutes - <code>market_data_60Minute</code>: [24, num_features] - Last 24 hours</p>"},{"location":"getting-started/#using-stop-loss-take-profit","title":"Using Stop-Loss / Take-Profit","text":"<pre><code>from torchtrade.envs.offline import SeqLongOnlySLTPEnv, SeqLongOnlySLTPEnvConfig\n\nconfig = SeqLongOnlySLTPEnvConfig(\n    stoploss_levels=[-0.02, -0.05],     # -2%, -5%\n    takeprofit_levels=[0.05, 0.10],     # +5%, +10%\n    time_frames=[\"1min\", \"5min\", \"15min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n    initial_cash=1000\n)\n\nenv = SeqLongOnlySLTPEnv(df, config)\n\n# Action space: 1 (HOLD) + 2\u00d72 (SL/TP combinations) = 5 actions\n# Action 0: HOLD\n# Action 1: BUY with SL=-2%, TP=+5%\n# Action 2: BUY with SL=-2%, TP=+10%\n# Action 3: BUY with SL=-5%, TP=+5%\n# Action 4: BUY with SL=-5%, TP=+10%\n</code></pre>"},{"location":"getting-started/#live-trading-setup","title":"Live Trading Setup","text":"<p>For live trading with real exchanges, you'll need API credentials.</p>"},{"location":"getting-started/#alpaca-us-stocks-crypto","title":"Alpaca (US Stocks &amp; Crypto)","text":"<p>Alpaca offers commission-free paper trading for testing strategies without risk. See Alpaca Paper Trading Docs for API credentials setup.</p> <pre><code># Create .env file\ncat &gt; .env &lt;&lt; EOF\nAPI_KEY=your_alpaca_api_key\nSECRET_KEY=your_alpaca_secret_key\nEOF\n</code></pre> <pre><code>from torchtrade.envs.alpaca import AlpacaTorchTradingEnv, AlpacaTradingEnvConfig\nfrom alpaca.data.timeframe import TimeFrame, TimeFrameUnit\n\nconfig = AlpacaTradingEnvConfig(\n    symbol=\"BTC/USD\",\n    time_frames=[\n        TimeFrame(1, TimeFrameUnit.Minute),\n        TimeFrame(5, TimeFrameUnit.Minute),\n    ],\n    window_sizes=[12, 8],\n    execute_on=TimeFrame(5, TimeFrameUnit.Minute),\n    paper=True  # Start with paper trading!\n)\n\nenv = AlpacaTorchTradingEnv(config)\n</code></pre>"},{"location":"getting-started/#binance-crypto-futures","title":"Binance (Crypto Futures)","text":"<p>If you want to trade on Binance, register here in case you have no account. Binance also allows for demo trading, see here.</p> <pre><code># Add to .env\nBINANCE_API_KEY=your_binance_api_key\nBINANCE_SECRET_KEY=your_binance_secret_key\n</code></pre> <pre><code>from torchtrade.envs.binance import (\n    BinanceFuturesTorchTradingEnv,\n    BinanceFuturesTradingEnvConfig\n)\n\nconfig = BinanceFuturesTradingEnvConfig(\n    symbol=\"BTCUSDT\",\n    intervals=[\"1m\", \"5m\"],\n    window_sizes=[12, 8],\n    execute_on=\"1m\",\n    leverage=5,                        # 5x leverage\n    quantity_per_trade=0.01,\n    demo=True,                         # Use testnet\n)\n\nenv = BinanceFuturesTorchTradingEnv(config)\n</code></pre> <p>Note: Alpaca and Binance are just two examples of live environments/brokers that TorchTrade supports. For more details on all available exchanges and configurations, see Online Environments. We're always open to including additional brokers - if you'd like to request support for a new exchange, please create an issue or contact us directly at torchtradecontact@gmail.com.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have the basics, explore these topics:</p> <ul> <li>Offline Environments - Deep dive into backtesting environments</li> <li>Online Environments - Live trading with exchange APIs</li> <li>Feature Engineering - Add technical indicators</li> <li>Reward Functions - Design better reward signals</li> <li>Performance Metrics - Evaluate agent performance with trading metrics</li> <li>Understanding the Sampler - How multi-timeframe sampling works</li> <li>Building Custom Environments - Extend TorchTrade</li> </ul>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#common-issues","title":"Common Issues","text":"<p>Issue: \"No module named 'torchtrade'\"</p> <p>Solution: Make sure you've activated the virtual environment: <pre><code>source .venv/bin/activate  # Unix/macOS\n.venv\\Scripts\\activate  # Windows\n</code></pre></p> <p>Issue: \"CUDA out of memory\"</p> <p>Solution: Reduce batch size or use CPU: <pre><code># In training config\ncollector:\n  frames_per_batch: 50000  # Reduce from 100000\n  device: \"cpu\"  # Use CPU instead of CUDA\n</code></pre></p> <p>Issue: \"Columns do not match required format\"</p> <p>Solution: Ensure your DataFrame has the exact column names: <pre><code>df.columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n</code></pre></p> <p>Issue: \"Environment returns NaN rewards\"</p> <p>Solution: Check for invalid price data: <pre><code># Remove NaN values\ndf = df.dropna()\n\n# Check for zero/negative prices\nassert (df[['open', 'high', 'low', 'close']] &gt; 0).all().all()\n</code></pre></p>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcac Questions: GitHub Discussions</li> <li>\ud83d\udc1b Bug Reports: GitHub Issues</li> <li>\ud83d\udce7 Email: torchtradecontact@gmail.com</li> </ul>"},{"location":"components/actors/","title":"Trading Actors","text":"<p>Trading actors implement the policy interface for TorchTrade environments. Beyond standard neural network policies, TorchTrade provides rule-based strategies, LLM-powered agents, and human-in-the-loop interfaces.</p>"},{"location":"components/actors/#available-actors","title":"Available Actors","text":"Actor Type Use Case Key Features RuleBasedActor Deterministic Strategy Baselines, debugging, research benchmarks Technical indicators, no learning required MeanReversionActor Rule-Based (Bollinger + Stoch RSI) Ranging markets, baseline comparisons ~0.3-0.8 Sharpe on sideways markets LLMActor LLM (OpenAI API) Research, rapid prototyping GPT-4/5 for decision-making LocalLLMActor LLM (Local inference) Production, privacy, cost efficiency vLLM or transformers backend, quantization support HumanActor Human-in-the-loop Expert demonstrations, data collection Interactive trading with visualization"},{"location":"components/actors/#rulebasedactor-base-class","title":"RuleBasedActor (Base Class)","text":"<p>Abstract base class for implementing deterministic trading strategies using technical indicators. Rule-based actors provide strong baselines and help validate environment design without RL complexity.</p>"},{"location":"components/actors/#key-features","title":"Key Features","text":"<ul> <li>Deterministic: No randomness, reproducible results</li> <li>Preprocessing pattern: Compute indicators on full dataset upfront</li> <li>Feature extraction: Helper methods to access preprocessed indicators</li> <li>Debugging support: Built-in debug mode for step-by-step analysis</li> </ul>"},{"location":"components/actors/#when-to-use","title":"When to Use","text":"<ul> <li>Baselines: Compare RL agents against known strategies</li> <li>Environment validation: Verify reward functions and environment mechanics</li> <li>Research benchmarks: Establish performance floors</li> <li>Strategy prototyping: Test ideas before implementing in RL</li> </ul>"},{"location":"components/actors/#implementation-pattern","title":"Implementation Pattern","text":"<p>Rule-based actors follow a two-phase pattern:</p> <ol> <li>Preprocessing: Compute all technical indicators on full dataset</li> <li>Decision-making: Extract features and apply rules at each step</li> </ol> <pre><code>from torchtrade.actor.rulebased.base import RuleBasedActor\n\nclass MyStrategy(RuleBasedActor):\n    def get_preprocessing_fn(self):\n        \"\"\"Return function that computes indicators on full dataset.\"\"\"\n        def preprocess(df):\n            # Compute indicators (e.g., moving averages, RSI)\n            df[\"features_sma_20\"] = df[\"close\"].rolling(20).mean()\n            df[\"features_rsi_14\"] = compute_rsi(df[\"close\"], 14)\n            return df\n        return preprocess\n\n    def select_action(self, observation):\n        \"\"\"Apply trading rules based on current observation.\"\"\"\n        data = self.extract_market_data(observation)\n        sma = self.get_feature(data, \"features_sma_20\")[-1]\n        rsi = self.get_feature(data, \"features_rsi_14\")[-1]\n\n        # Trading logic\n        if rsi &lt; 30 and price &lt; sma:\n            return 2  # Buy\n        elif rsi &gt; 70 and price &gt; sma:\n            return 0  # Sell\n        return 1  # Hold\n</code></pre>"},{"location":"components/actors/#meanreversionactor","title":"MeanReversionActor","text":"<p>Concrete implementation of RuleBasedActor using Bollinger Bands and Stochastic RSI for mean reversion trading.</p>"},{"location":"components/actors/#strategy-logic","title":"Strategy Logic","text":"<ul> <li>Buy signal: Price below lower Bollinger Band AND Stoch RSI bullish crossover from oversold (&lt;20) AND volume confirmation</li> <li>Sell signal: Price above upper Bollinger Band AND Stoch RSI bearish crossover from overbought (&gt;80) AND volume confirmation</li> <li>Hold: Otherwise</li> </ul>"},{"location":"components/actors/#expected-performance","title":"Expected Performance","text":"Metric Value Notes Sharpe Ratio 0.3 - 0.8 Varies by market regime Action Distribution 30% long, 30% short, 40% hold Balanced strategy Best Markets Ranging/sideways Mean reversion works when prices oscillate Worst Markets Strong trends Gets caught in persistent directional moves"},{"location":"components/actors/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Default Description <code>bb_window</code> int 20 Bollinger Bands period <code>bb_std</code> float 2.0 Bollinger Bands standard deviations <code>stoch_rsi_window</code> int 14 Stochastic RSI period <code>stoch_k_window</code> int 3 Stochastic %K smoothing window <code>stoch_d_window</code> int 3 Stochastic %D smoothing window <code>oversold_threshold</code> float 20.0 Stoch RSI oversold level <code>overbought_threshold</code> float 80.0 Stoch RSI overbought level <code>execute_timeframe</code> TimeFrame 5Minute Timeframe for feature extraction"},{"location":"components/actors/#technical-indicator-references","title":"Technical Indicator References","text":"<ul> <li>Bollinger Bands: Bollinger, J. (2001). \"Bollinger on Bollinger Bands\". McGraw-Hill Education</li> <li>Stochastic RSI: Introduced by Stanley Kroll and Tushar Chande (1994) in \"The New Technical Trader\"</li> </ul>"},{"location":"components/actors/#usage-example","title":"Usage Example","text":"<pre><code>from torchtrade.actor.rulebased.meanreversion import MeanReversionActor\nfrom torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\n\n# Create actor\nactor = MeanReversionActor(\n    market_data_keys=[\"market_data_1Minute_12\", \"market_data_5Minute_8\"],\n    features_order=[\"open\", \"high\", \"low\", \"close\", \"volume\",\n                    \"features_bb_middle\", \"features_bb_std\", \"features_bb_upper\",\n                    \"features_bb_lower\", \"features_bb_position\",\n                    \"features_stoch_rsi_k\", \"features_stoch_rsi_d\"],\n    bb_window=20,\n    oversold_threshold=20,\n    debug=False,\n)\n\n# Get preprocessing function\npreprocessing_fn = actor.get_preprocessing_fn()\n\n# Create environment with preprocessing\nconfig = SeqLongOnlyEnvConfig(\n    df=train_df,\n    preprocessing_fn=preprocessing_fn,  # Compute indicators upfront\n    # ... other config ...\n)\nenv = SeqLongOnlyEnv(config)\n\n# Use actor to trade\nobservation = env.reset()\nwhile not done:\n    action = actor.select_action(observation)\n    observation, reward, done, info = env.step(action)\n</code></pre> <p>Code Reference: <code>torchtrade/actor/rulebased/meanreversion/actor.py</code></p>"},{"location":"components/actors/#llmactor","title":"LLMActor","text":"<p>LLM-based trading actor using OpenAI API (GPT-4, GPT-5) for decision-making. The actor constructs prompts from market data and account state, queries the LLM, and parses actions from structured responses.</p>"},{"location":"components/actors/#key-features_1","title":"Key Features","text":"<ul> <li>Natural language reasoning: LLM explains decisions in <code>&lt;think&gt;</code> tags</li> <li>Flexible action spaces: Supports standard (buy/sell/hold) and custom actions</li> <li>Structured prompts: Market data formatted as tables for readability</li> <li>OpenAI API: Uses latest GPT models</li> </ul>"},{"location":"components/actors/#when-to-use_1","title":"When to Use","text":"<ul> <li>Research: Explore LLM capabilities for trading</li> <li>Rapid prototyping: Test strategies without training neural networks</li> <li>Interpretability: Get explanations for each decision</li> <li>Multi-modal agents: Combine with other data sources (news, sentiment)</li> </ul>"},{"location":"components/actors/#configuration-parameters_1","title":"Configuration Parameters","text":"Parameter Type Default Description <code>market_data_keys</code> list[str] Required TensorDict keys for market data (e.g., <code>[\"market_data_1Minute_12\"]</code>) <code>account_state</code> list[str] Required Account state field names (e.g., <code>[\"cash\", \"position_size\", ...]</code>) <code>model</code> str \"gpt-5-nano\" OpenAI model identifier <code>symbol</code> str \"BTC/USD\" Trading symbol for prompt context <code>execute_on</code> str \"5Minute\" Timeframe for prompt context <code>action_dict</code> dict <code>{\"buy\": 2, \"sell\": 0, \"hold\": 1}</code> Action name to index mapping <code>debug</code> bool False Print prompts and responses"},{"location":"components/actors/#usage-example_1","title":"Usage Example","text":"<pre><code>from torchtrade.actor import LLMActor\nfrom torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\n\n# Create environment\nconfig = SeqLongOnlyEnvConfig(...)\nenv = SeqLongOnlyEnv(config)\n\n# Create LLM actor (uses environment attributes)\nactor = LLMActor(\n    market_data_keys=env.market_data_keys,\n    account_state=env.account_state,\n    symbol=config.symbol,\n    execute_on=config.execute_on,\n    model=\"gpt-4-turbo\",\n    debug=True,  # Print prompts/responses\n)\n\n# Trade\nobservation = env.reset()\noutput = actor(observation)  # Returns tensordict with \"action\" and \"thinking\"\naction = output[\"action\"]\nthinking = output.get(\"thinking\", \"\")  # LLM's reasoning\n</code></pre>"},{"location":"components/actors/#prompt-format","title":"Prompt Format","text":"<p>LLMActor constructs prompts with:</p> <p>System Prompt: <pre><code>You are a disciplined trading agent for BTC/USD on the 5Minute timeframe.\nAt each step, you receive the latest account state and market data.\nYou must choose exactly one action from: buy, sell, hold.\n\n- Base your decision on the provided data.\n- Think step-by-step inside &lt;think&gt;&lt;/think&gt;.\n- Output your final action: &lt;answer&gt;buy&lt;/answer&gt;, &lt;answer&gt;sell&lt;/answer&gt;, or &lt;answer&gt;hold&lt;/answer&gt;.\n</code></pre></p> <p>User Prompt (example): <pre><code>Current account state:\ncash: 10000.0\nposition_size: 0.0\nposition_value: 0.0\nentry_price: 0.0\ncurrent_price: 50000.0\nunrealized_pnlpct: 0.0\nholding_time: 0.0\n\n---\nCurrent market data:\n\nmarket_data_5Minute_8:\n\n   close |     open |     high |      low |   volume\n\n50123.4 | 50100.2 | 50150.0 | 50050.0 |   125.3\n50156.1 | 50123.4 | 50200.5 | 50100.0 |   142.7\n...\n</code></pre></p> <p>LLM Response: <pre><code>&lt;think&gt;\nThe price has been trending upward over the last 8 candles, with increasing volume.\nCurrent price is near resistance at 50200. RSI would likely be overbought.\nI should wait for a pullback before entering.\n&lt;/think&gt;\n&lt;answer&gt;hold&lt;/answer&gt;\n</code></pre></p> <p>Requirements: <code>.env</code> file with <code>OPENAI_API_KEY</code></p> <p>Code Reference: <code>torchtrade/actor/llm_actor.py</code></p>"},{"location":"components/actors/#localllmactor","title":"LocalLLMActor","text":"<p>Local LLM-based actor using vLLM or transformers for inference. Similar to LLMActor but runs models locally for privacy, cost efficiency, and production deployment.</p>"},{"location":"components/actors/#key-features_2","title":"Key Features","text":"<ul> <li>Local inference: No API calls, full control over model</li> <li>Multiple backends:<ul> <li>vLLM: High-throughput inference with PagedAttention (recommended)</li> <li>transformers: HuggingFace compatibility, easier setup</li> </ul> </li> <li>Quantization support: 4-bit and 8-bit quantization for memory efficiency</li> <li>Auto-detection: Automatically detects environment type (standard, SLTP, futures)</li> <li>Flexible action spaces: Supports standard 3-action, SLTP, and futures environments</li> </ul>"},{"location":"components/actors/#backend-comparison","title":"Backend Comparison","text":"Backend Speed Setup Use Case vLLM 5-10x faster Requires CUDA Production, high-throughput transformers Baseline CPU/GPU/MPS Development, compatibility"},{"location":"components/actors/#configuration-parameters_2","title":"Configuration Parameters","text":"Parameter Type Default Description <code>model</code> str \"Qwen/Qwen2.5-0.5B-Instruct\" HuggingFace model ID <code>backend</code> str \"vllm\" Inference backend (\"vllm\" or \"transformers\") <code>device</code> str Auto-detect Device for inference (\"cuda\", \"cpu\", \"mps\") <code>quantization</code> str None Quantization mode (None, \"4bit\", \"8bit\") <code>max_tokens</code> int 512 Maximum tokens to generate <code>temperature</code> float 0.7 Sampling temperature <code>gpu_memory_utilization</code> float 0.9 Fraction of GPU memory for vLLM (0.0-1.0) <code>action_space_type</code> str \"standard\" Action space (\"standard\", \"sltp\", \"futures_sltp\") <code>action_map</code> dict None Required for SLTP environments <code>debug</code> bool False Print prompts and responses"},{"location":"components/actors/#usage-example_2","title":"Usage Example","text":"<p>Standard 3-Action Environment: <pre><code>from torchtrade.actor import LocalLLMActor\n\n# Create local LLM actor\nactor = LocalLLMActor(\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    backend=\"vllm\",\n    quantization=\"4bit\",  # Use 4-bit quantization for memory\n    temperature=0.7,\n    debug=False,\n)\n\n# Use like any actor\nobservation = env.reset()\noutput = actor(observation)\naction = output[\"action\"]\n</code></pre></p> <p>SLTP Environment (Bracket Orders): <pre><code>from torchtrade.envs.offline import SeqLongOnlySLTPEnv\n\n# Get action map from environment\nenv = SeqLongOnlySLTPEnv(config)\naction_map = env.action_map  # Dict mapping action indices to (side, sl, tp)\n\n# Create actor for SLTP\nactor = LocalLLMActor(\n    model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n    action_space_type=\"sltp\",\n    action_map=action_map,\n    backend=\"transformers\",  # Fallback to transformers\n)\n</code></pre></p> <p>Futures Environment with Leverage: <pre><code>from torchtrade.envs.offline import SeqFuturesSLTPEnv\n\nenv = SeqFuturesSLTPEnv(config)\n\nactor = LocalLLMActor(\n    model=\"Qwen/Qwen2.5-3B-Instruct\",\n    action_space_type=\"futures_sltp\",\n    action_map=env.action_map,\n    backend=\"vllm\",\n    gpu_memory_utilization=0.8,\n)\n</code></pre></p>"},{"location":"components/actors/#quantization-benefits","title":"Quantization Benefits","text":"Quantization Memory Reduction Speed Quality Loss None (fp16) Baseline Baseline None 8-bit ~2x ~1.5x faster Minimal 4-bit ~4x ~2x faster Small (~2-3% accuracy) <p>Recommendation: Use 4-bit for models &gt;1B parameters to fit in consumer GPUs.</p>"},{"location":"components/actors/#model-recommendations","title":"Model Recommendations","text":"Model Size Memory (4-bit) Use Case Qwen/Qwen2.5-0.5B-Instruct 0.5B ~500MB Quick testing Qwen/Qwen2.5-1.5B-Instruct 1.5B ~1.5GB Lightweight production Qwen/Qwen2.5-3B-Instruct 3B ~2.5GB Balanced performance Qwen/Qwen2.5-7B-Instruct 7B ~5GB Best quality <p>Installation: <pre><code># For vLLM backend (recommended)\npip install vllm\n\n# For transformers backend\npip install transformers accelerate bitsandbytes\n</code></pre></p> <p>Code Reference: <code>torchtrade/actor/local_llm_actor.py</code></p>"},{"location":"components/actors/#humanactor","title":"HumanActor","text":"<p>Interactive debugging and visualization tool that allows humans to step through environments and understand what the agent \"sees\" at each timestep. Primarily designed for environment validation and debugging, it can also be used to collect human demonstrations for imitation learning.</p>"},{"location":"components/actors/#key-features_3","title":"Key Features","text":"<ul> <li>Interactive visualization: Plotly dashboards show market data across all timeframes</li> <li>Observation inspection: View complete account state and market data at each step</li> <li>Step-by-step debugging: Manually step through environment to validate dynamics</li> <li>Agent perspective: See exactly what your RL agent observes at each timestep</li> <li>Optional demonstrations: Can collect human trajectories for imitation learning</li> </ul>"},{"location":"components/actors/#when-to-use_2","title":"When to Use","text":"<ul> <li>Environment debugging: Step through episodes to verify environment correctness and understand dynamics</li> <li>Observation inspection: See exactly what data the agent receives at each timestep</li> <li>Reward validation: Manually verify reward signals make sense for trading decisions</li> <li>Algorithm understanding: Understand what information is available to your RL agent</li> <li>Expert demonstrations: Optionally collect human trajectories for imitation learning</li> </ul>"},{"location":"components/actors/#configuration-parameters_3","title":"Configuration Parameters","text":"Parameter Type Description <code>symbol</code> str Trading symbol for display <code>features</code> list[str] Feature names (OHLCV columns) <code>market_data_keys</code> list[str] Market data keys to visualize <code>account_state_key</code> str TensorDict key for account state <code>action_spec</code> Spec Action space specification"},{"location":"components/actors/#usage-example_3","title":"Usage Example","text":"<pre><code>from torchtrade.actor import HumanActor\nfrom torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\n\n# Create environment\nconfig = SeqLongOnlyEnvConfig(...)\nenv = SeqLongOnlyEnv(config)\n\n# Create human actor\nactor = HumanActor(\n    symbol=config.symbol,\n    features=[\"close\", \"open\", \"high\", \"low\", \"volume\"],\n    market_data_keys=env.market_data_keys,\n    account_state_key=\"account_state\",\n    action_spec=env.action_spec,\n)\n\n# Interactive trading loop\nobservation = env.reset()\ndone = False\nwhile not done:\n    # Shows plotly dashboard + account state\n    # Prompts: \"What action do you want to take? (buy:b, sell:s, hold:h)\"\n    output = actor(observation)\n    action = output[\"action\"]\n    observation, reward, done, info = env.step(action)\n\nprint(f\"Episode return: {env.episode_return}\")\n</code></pre> <p>Interactive Prompt: <pre><code>Current account state:\ncash: 9500.0\nposition_size: 0.5\nposition_value: 25000.0\nentry_price: 50000.0\ncurrent_price: 50500.0\nunrealized_pnlpct: 0.01\nholding_time: 12.0\n\n---\n[Plotly dashboard displays with multi-timeframe candlestick charts]\n\nWhat action do you want to take? (buy:b, sell:s, hold:h)\n&gt; s\n</code></pre></p> <p>Use Case - Collect Expert Demonstrations: <pre><code># Collect 100 expert trajectories\ndemonstrations = []\nfor episode in range(100):\n    trajectory = []\n    obs = env.reset()\n    done = False\n    while not done:\n        action_td = actor(obs)  # Human provides action\n        trajectory.append((obs, action_td[\"action\"]))\n        obs, reward, done, info = env.step(action_td[\"action\"])\n    demonstrations.append(trajectory)\n\n# Use for imitation learning\ntrain_behavioral_cloning(demonstrations)\n</code></pre></p> <p>Code Reference: <code>torchtrade/actor/human.py</code></p>"},{"location":"components/actors/#quick-reference","title":"Quick Reference","text":""},{"location":"components/actors/#actor-selection-guide","title":"Actor Selection Guide","text":"Scenario Recommended Actor Notes Baseline comparison MeanReversionActor Known strategy, deterministic RL training Neural network policy Standard TorchRL actor-critic Research prototyping LLMActor Fast iteration, interpretable Production LLM LocalLLMActor Local inference, cost-efficient Environment debugging HumanActor Visualize observations, validate rewards Data collection HumanActor Expert demonstrations (secondary use)"},{"location":"components/actors/#common-patterns","title":"Common Patterns","text":"<p>Pattern 1: Rule-Based Baseline <pre><code># Test environment with known strategy first\nactor = MeanReversionActor(...)\npreprocessing_fn = actor.get_preprocessing_fn()\nenv = SeqLongOnlyEnv(config, preprocessing_fn=preprocessing_fn)\n# ... run episodes ...\n</code></pre></p> <p>Pattern 2: LLM Exploration <pre><code># Quickly test ideas without training\nactor = LocalLLMActor(model=\"Qwen/Qwen2.5-1.5B\", quantization=\"4bit\")\n# ... trade ...\n</code></pre></p> <p>Pattern 3: Environment Debugging &amp; Validation <pre><code># Debug environment and understand agent observations\nhuman_actor = HumanActor(...)\nobs = env.reset()\n\n# Step through manually to verify environment behavior\nfor step in range(100):\n    # Visualize what the agent sees\n    action_td = human_actor(obs)  # Shows plotly dashboard + prompts for action\n    obs = env.step(action_td[\"action\"])\n    # Verify rewards, transitions, and observations are correct\n</code></pre></p> <p>Pattern 4: Optional - Human Demonstrations for Imitation Learning <pre><code># Collect expert trajectories (secondary use case)\ndemos = collect_demonstrations(env, human_actor, num_episodes=100)\nbc_loss = behavioral_cloning_loss(policy, demos)\n# ... train ...\n</code></pre></p>"},{"location":"components/actors/#implementation-notes","title":"Implementation Notes","text":""},{"location":"components/actors/#rulebasedactor-preprocessing","title":"RuleBasedActor Preprocessing","text":"<p>Rule-based actors compute all indicators upfront on the full dataset:</p> <pre><code>preprocessing_fn = actor.get_preprocessing_fn()\ndf = preprocessing_fn(df)  # Compute features once\n# Features stored in DataFrame, accessed during episodes\n</code></pre> <p>This is efficient because indicators like Bollinger Bands require historical data, so computing them once is faster than recalculating at each step.</p>"},{"location":"components/actors/#llm-prompt-engineering","title":"LLM Prompt Engineering","text":"<p>Both LLMActor and LocalLLMActor use structured prompts:</p> <ul> <li>System prompt: Defines role, action space, output format</li> <li>User prompt: Current state (account + market data)</li> <li>Response format: <code>&lt;think&gt;reasoning&lt;/think&gt;&lt;answer&gt;action&lt;/answer&gt;</code></li> </ul> <p>The <code>&lt;think&gt;</code> tags encourage chain-of-thought reasoning, improving decision quality.</p>"},{"location":"components/actors/#localllmactor-backend-selection","title":"LocalLLMActor Backend Selection","text":"<p>vLLM is faster but requires: - CUDA GPU - Linux (Windows via WSL) - <code>pip install vllm</code></p> <p>transformers works everywhere: - CPU/GPU/MPS (Apple Silicon) - Cross-platform - <code>pip install transformers</code></p> <p>If vLLM import fails, LocalLLMActor automatically falls back to transformers.</p>"},{"location":"components/actors/#see-also","title":"See Also","text":"<ul> <li>Environments Guide - Compatible environment types</li> <li>Reward Functions - Reward engineering for RL actors</li> <li>Loss Functions - Training objectives for neural network actors</li> <li>TorchRL Actors - Building neural network policies</li> </ul>"},{"location":"components/losses/","title":"Loss Functions","text":"<p>TorchTrade provides specialized loss functions for training RL trading agents, including one-step policy gradient methods (GRPO) and representation learning objectives (CTRL).</p>"},{"location":"components/losses/#available-loss-functions","title":"Available Loss Functions","text":"Loss Function Type Use Case Key Features GRPOLoss Policy Gradient One-step RL with SLTP environments Group Relative Policy Optimization, entropy regularization, advantage normalization CTRLLoss Representation Learning Self-supervised encoder training Cross-trajectory representation learning, prototype-based contrastive learning CTRLPPOLoss Combined Joint policy + representation learning Combines ClipPPOLoss with CTRLLoss for end-to-end training"},{"location":"components/losses/#grpoloss","title":"GRPOLoss","text":"<p>Group Relative Policy Optimization (GRPO) loss for one-step reinforcement learning. This loss is designed for environments where actions have immediate consequences, such as SLTP (Stop-Loss/Take-Profit) bracket order environments.</p>"},{"location":"components/losses/#key-features","title":"Key Features","text":"<ul> <li>One-step optimization: No multi-step rollouts or value estimation required</li> <li>Advantage normalization: Standardizes advantages within each batch for stable training</li> <li>Clipped policy updates: Similar to PPO, prevents destructive policy updates</li> <li>Entropy regularization: Encourages exploration through entropy bonus</li> </ul>"},{"location":"components/losses/#when-to-use","title":"When to Use","text":"<ul> <li>FuturesOneStepEnv or LongOnlyOneStepEnv - One-step environments where episodes reset after each action</li> <li>SLTP bracket orders - Trading with predefined stop-loss and take-profit levels</li> <li>Contextual bandit - Single-step decision problems</li> </ul>"},{"location":"components/losses/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Default Description <code>actor_network</code> ProbabilisticTensorDictSequential Required Policy network that outputs action distributions <code>entropy_coeff</code> float or dict 0.01 Entropy regularization coefficient (supports per-head coefficients for composite actions) <code>epsilon_low</code> float 0.2 Lower clipping bound for policy ratio <code>epsilon_high</code> float 0.2 Upper clipping bound for policy ratio <code>samples_mc_entropy</code> int 1 Number of Monte Carlo samples for entropy estimation <code>reduction</code> str \"mean\" Reduction method for loss aggregation (\"mean\", \"sum\", \"none\")"},{"location":"components/losses/#usage-example","title":"Usage Example","text":"<pre><code>from torchtrade.losses import GRPOLoss\nfrom torchrl.modules import ProbabilisticActor\n\n# Create policy network\nactor = ProbabilisticActor(\n    module=policy_network,\n    in_keys=[\"observation\"],\n    out_keys=[\"action\"],\n    distribution_class=Categorical,\n)\n\n# Create GRPO loss\nloss_module = GRPOLoss(\n    actor_network=actor,\n    entropy_coeff=0.01,\n)\n\n# Training loop\nfor batch in collector:\n    loss_td = loss_module(batch)\n    loss = loss_td[\"loss_objective\"] + loss_td[\"loss_entropy\"]\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>Paper Reference: \"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\" (arXiv:2402.03300) - Section 2.2 introduces GRPO (Group Relative Policy Optimization)</p> <p>Code Reference: <code>torchtrade/losses/grpo_loss.py</code></p>"},{"location":"components/losses/#ctrlloss","title":"CTRLLoss","text":"<p>Cross-Trajectory Representation Learning (CTRL) loss for self-supervised encoder training. CTRL improves zero-shot generalization by training encoders to recognize behavioral similarity across trajectories without using rewards.</p>"},{"location":"components/losses/#key-features_1","title":"Key Features","text":"<ul> <li>Self-supervised learning: No reward labels required</li> <li>Prototype-based clustering: Uses Sinkhorn algorithm for soft cluster assignments</li> <li>MYOW loss: Encourages cross-trajectory consistency for similar behaviors</li> <li>Zero-shot transfer: Representations generalize across market conditions</li> </ul>"},{"location":"components/losses/#when-to-use_1","title":"When to Use","text":"<ul> <li>Encoder pre-training - Train shared encoder before policy learning</li> <li>Multi-environment transfer - Learn representations that work across different assets</li> <li>Data-efficient RL - Leverage unlabeled trajectory data for better representations</li> </ul>"},{"location":"components/losses/#configuration-parameters_1","title":"Configuration Parameters","text":"Parameter Type Default Description <code>encoder_network</code> TensorDictModule Required Encoder network that produces embeddings <code>embedding_dim</code> int Required Dimension of encoder output embeddings <code>projection_dim</code> int 128 Dimension of projection space for prototypes <code>num_prototypes</code> int 512 Number of learnable prototype vectors <code>sinkhorn_iters</code> int 3 Iterations for Sinkhorn-Knopp algorithm <code>temperature</code> float 0.1 Temperature for softmax (lower = more peaked assignments) <code>window_len</code> int 4 Length of sliding window for trajectory segments <code>myow_k</code> int 5 Number of nearest prototypes for MYOW loss <code>myow_coeff</code> float 1.0 Coefficient for MYOW loss term <code>reduction</code> str \"mean\" Reduction method for loss aggregation"},{"location":"components/losses/#usage-example_1","title":"Usage Example","text":"<pre><code>from torchtrade.losses import CTRLLoss\nfrom tensordict.nn import TensorDictModule\n\n# Create encoder network\nencoder = TensorDictModule(\n    nn.Sequential(\n        nn.Linear(observation_dim, 256),\n        nn.ReLU(),\n        nn.Linear(256, 128)\n    ),\n    in_keys=[\"observation\"],\n    out_keys=[\"embedding\"]\n)\n\n# Create CTRL loss\nctrl_loss = CTRLLoss(\n    encoder_network=encoder,\n    embedding_dim=128,\n    num_prototypes=512,\n    myow_coeff=1.0,\n)\n\n# Pre-training loop\nfor batch in collector:\n    loss_td = ctrl_loss(batch)\n    loss = loss_td[\"loss_ctrl\"]\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>Paper Reference: \"Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL\" (arXiv:2106.02193)</p> <p>Code Reference: <code>torchtrade/losses/ctrl.py</code></p>"},{"location":"components/losses/#ctrlppoloss","title":"CTRLPPOLoss","text":"<p>Combined loss module that jointly trains policy (via PPO) and encoder representations (via CTRL). This enables end-to-end training where the encoder learns useful representations while the policy learns to act.</p>"},{"location":"components/losses/#key-features_2","title":"Key Features","text":"<ul> <li>Joint optimization: Trains policy and encoder simultaneously</li> <li>Shared encoder: Encoder representations used by both actor and critic</li> <li>Weighted combination: Control relative importance of PPO and CTRL objectives</li> </ul>"},{"location":"components/losses/#when-to-use_2","title":"When to Use","text":"<ul> <li>End-to-end training - Train policy and encoder together from scratch</li> <li>Representation + policy learning - When both components need to adapt jointly</li> <li>Multi-task RL - Learn representations that support multiple downstream tasks</li> </ul>"},{"location":"components/losses/#configuration-parameters_2","title":"Configuration Parameters","text":"Parameter Type Default Description <code>ppo_loss</code> LossModule Required PPO loss module (e.g., ClipPPOLoss) <code>ctrl_loss</code> CTRLLoss Required CTRL loss module for representation learning <code>ctrl_coeff</code> float 1.0 Coefficient for CTRL loss term (balances with PPO loss)"},{"location":"components/losses/#usage-example_2","title":"Usage Example","text":"<pre><code>from torchtrade.losses import CTRLLoss, CTRLPPOLoss\nfrom torchrl.objectives import ClipPPOLoss\n\n# Create PPO loss\nppo_loss = ClipPPOLoss(\n    actor_network=actor,\n    critic_network=critic,\n)\n\n# Create CTRL loss for encoder\nctrl_loss = CTRLLoss(\n    encoder_network=encoder,\n    embedding_dim=128,\n)\n\n# Combine into joint loss\ncombined_loss = CTRLPPOLoss(\n    ppo_loss=ppo_loss,\n    ctrl_loss=ctrl_loss,\n    ctrl_coeff=0.5,  # Weight CTRL loss at 50%\n)\n\n# Training loop\nfor batch in collector:\n    loss_td = combined_loss(batch)\n    # loss_td contains both PPO keys (loss_objective, loss_critic, etc.)\n    # and CTRL keys (loss_ctrl, loss_proto, loss_myow)\n    total_loss = (\n        loss_td[\"loss_objective\"] +\n        loss_td[\"loss_critic\"] +\n        loss_td[\"loss_ctrl\"]\n    )\n    total_loss.backward()\n    optimizer.step()\n</code></pre> <p>Code Reference: <code>torchtrade/losses/ctrl.py</code> (defined in same file as CTRLLoss)</p>"},{"location":"components/losses/#quick-reference","title":"Quick Reference","text":""},{"location":"components/losses/#loss-selection-guide","title":"Loss Selection Guide","text":"Scenario Recommended Loss Notes One-step SLTP trading GRPOLoss No value estimation needed Encoder pre-training CTRLLoss Self-supervised, no rewards Joint policy + encoder CTRLPPOLoss End-to-end training Multi-step sequential ClipPPOLoss (TorchRL) Use TorchRL's built-in losses"},{"location":"components/losses/#common-patterns","title":"Common Patterns","text":"<p>Pattern 1: GRPO for One-Step Environments <pre><code>loss = GRPOLoss(actor, entropy_coeff=0.01)\n# Train with: loss_objective + loss_entropy\n</code></pre></p> <p>Pattern 2: CTRL Pre-Training + Fine-Tuning <pre><code># Phase 1: Pre-train encoder with CTRL\nctrl_loss = CTRLLoss(encoder, embedding_dim=128)\n# ... train encoder ...\n\n# Phase 2: Freeze encoder, train policy with PPO\nppo_loss = ClipPPOLoss(actor, critic)\nencoder.requires_grad_(False)\n# ... train policy ...\n</code></pre></p> <p>Pattern 3: Joint Training <pre><code>combined = CTRLPPOLoss(ppo_loss, ctrl_loss, ctrl_coeff=0.3)\n# Single optimizer updates both policy and encoder\n</code></pre></p>"},{"location":"components/losses/#implementation-details","title":"Implementation Details","text":""},{"location":"components/losses/#grpoloss-advantage-computation","title":"GRPOLoss Advantage Computation","text":"<p>GRPO normalizes advantages within each batch using the immediate reward:</p> <pre><code>advantage = (reward - reward.mean()) / (reward.std() + 1e-8)\n</code></pre> <p>This per-batch normalization makes training stable even when reward scales vary significantly.</p>"},{"location":"components/losses/#ctrl-loss-components","title":"CTRL Loss Components","text":"<p>The total CTRL loss combines two terms:</p> <pre><code>loss_ctrl = loss_proto + myow_coeff * loss_myow\n</code></pre> <ul> <li>loss_proto: Prototype contrastive loss (cross-entropy between predictions and Sinkhorn targets)</li> <li>loss_myow: MYOW loss (cosine similarity for trajectories with shared prototypes)</li> </ul>"},{"location":"components/losses/#see-also","title":"See Also","text":"<ul> <li>Environments Guide - Compatible environment types</li> <li>Transforms Guide - Data augmentation and preprocessing</li> <li>Reward Functions - Reward engineering patterns</li> </ul>"},{"location":"components/transforms/","title":"Transforms","text":"<p>TorchRL transforms are composable modules that modify environment observations, rewards, or actions. TorchTrade extends TorchRL's transform system with domain-specific transforms for trading environments.</p>"},{"location":"components/transforms/#available-transforms","title":"Available Transforms","text":"Transform Type Purpose Use Case CoverageTracker Monitoring Track dataset coverage during training Monitor exploration, detect overfitting ChronosEmbeddingTransform Feature Extraction Embed time series with Chronos T5 models Replace raw OHLCV with learned representations"},{"location":"components/transforms/#coveragetracker","title":"CoverageTracker","text":"<p>Transform that tracks which portions of the dataset are visited during training with random episode resets. Monitors both episode start diversity and full trajectory coverage.</p>"},{"location":"components/transforms/#key-features","title":"Key Features","text":"<ul> <li>Dual coverage tracking:<ul> <li>Reset coverage: Which starting positions are used for episode starts</li> <li>State coverage: All timesteps visited during episodes</li> </ul> </li> <li>Entropy metrics: Measures uniformity of visit distribution</li> <li>Zero overhead: Tracking happens in postproc, outside critical environment path</li> <li>Auto-detection: Only activates for training environments with <code>random_start=True</code></li> </ul>"},{"location":"components/transforms/#when-to-use","title":"When to Use","text":"<ul> <li>Dataset exploration analysis - Ensure comprehensive coverage of training data</li> <li>Overfitting detection - Identify if agent concentrates on specific market conditions</li> <li>Curriculum learning - Track coverage progression across training phases</li> <li>Data efficiency - Verify all collected data is being utilized</li> </ul>"},{"location":"components/transforms/#key-metrics","title":"Key Metrics","text":"Metric Range Interpretation <code>reset_coverage</code> [0.0, 1.0] Fraction of positions used as episode starts (0.8+ is good) <code>state_coverage</code> [0.0, 1.0] Fraction of all states visited (should be &gt;&gt; reset_coverage) <code>reset_entropy</code> [0, log(N)] Uniformity of episode start distribution (higher = more uniform) <code>state_entropy</code> [0, log(N)] Uniformity of state visit distribution (higher = better exploration) <p>Interpretation Guide:</p> <ul> <li>Good pattern: <code>reset_coverage=0.3</code>, <code>state_coverage=0.9</code> \u2192 Starting from 30% of positions but exploring 90% of dataset through trajectories</li> <li>Warning: <code>reset_coverage=0.5</code>, <code>state_coverage=0.5</code> \u2192 Only seeing states near episode starts, not exploring forward in time</li> <li>Ideal: Both high coverage (&gt;0.8) and high entropy (close to log(N))</li> </ul>"},{"location":"components/transforms/#usage-example","title":"Usage Example","text":"<pre><code>from torchrl.collectors import SyncDataCollector\nfrom torchrl.envs import TransformedEnv, Compose, InitTracker, DoubleToFloat, RewardSum\nfrom torchtrade.envs.transforms import CoverageTracker\n\n# Create environment with standard transforms\nenv = TransformedEnv(\n    base_env,\n    Compose(\n        InitTracker(),\n        DoubleToFloat(),\n        RewardSum(),\n    )\n)\n\n# Create coverage tracker for postproc (NOT in environment transform chain)\ncoverage_tracker = CoverageTracker()\n\n# Use as postproc in collector\ncollector = SyncDataCollector(\n    env,\n    policy,\n    frames_per_batch=1000,\n    total_frames=100000,\n    postproc=coverage_tracker,  # Process batches after collection\n)\n\n# During training loop\nfor batch in collector:\n    # ... train on batch ...\n\n    # Log coverage metrics periodically\n    stats = coverage_tracker.get_coverage_stats()\n    if stats[\"enabled\"]:\n        logger.log({\n            \"train/reset_coverage\": stats[\"reset_coverage\"],\n            \"train/reset_entropy\": stats[\"reset_entropy\"],\n            \"train/state_coverage\": stats[\"state_coverage\"],\n            \"train/state_entropy\": stats[\"state_entropy\"],\n        })\n</code></pre>"},{"location":"components/transforms/#configuration","title":"Configuration","text":"<p>CoverageTracker requires no configuration - it auto-detects environment settings:</p> <ul> <li>Initialization: Automatically detects dataset size from environment sampler</li> <li>Activation: Only tracks when <code>env.random_start=True</code> (training mode)</li> <li>Deactivation: Silently disabled for sequential/eval environments</li> </ul>"},{"location":"components/transforms/#coverage-statistics","title":"Coverage Statistics","text":"<p>Full statistics returned by <code>get_coverage_stats()</code>:</p> <pre><code>{\n    \"enabled\": True,\n    \"total_positions\": 100000,  # Dataset size\n\n    # Reset coverage (episode start diversity)\n    \"reset_visited\": 25000,\n    \"reset_coverage\": 0.25,\n    \"total_resets\": 50000,\n    \"reset_mean_visits\": 2.0,\n    \"reset_max_visits\": 150,\n    \"reset_min_visits\": 0,\n    \"reset_std_visits\": 5.2,\n    \"reset_entropy\": 4.3,\n\n    # State coverage (full trajectory coverage)\n    \"state_visited\": 85000,\n    \"state_coverage\": 0.85,\n    \"total_states\": 500000,\n    \"state_mean_visits\": 5.8,\n    \"state_max_visits\": 500,\n    \"state_min_visits\": 0,\n    \"state_std_visits\": 12.1,\n    \"state_entropy\": 6.1,\n}\n</code></pre> <p>Reference: <code>torchtrade/envs/transforms/coverage_tracker.py</code></p>"},{"location":"components/transforms/#chronosembeddingtransform","title":"ChronosEmbeddingTransform","text":"<p>Transform that embeds time series observations using pretrained Chronos T5 forecasting models. Replaces raw OHLCV data with learned representations, similar to how VC1Transform works for vision.</p>"},{"location":"components/transforms/#key-features_1","title":"Key Features","text":"<ul> <li>Pretrained representations: Leverages Amazon's Chronos T5 models trained on diverse time series</li> <li>Lazy loading: Model loaded on first use, not during initialization</li> <li>Multiple model sizes: From tiny (8M params) to large (710M params)</li> <li>Flexible aggregation: Mean, max, or concat for multi-feature embeddings</li> <li>GPU acceleration: Automatic device detection with bfloat16 precision</li> </ul>"},{"location":"components/transforms/#when-to-use_1","title":"When to Use","text":"<ul> <li>Limited training data - Pretrained representations generalize better than learning from scratch</li> <li>Transfer learning - Embeddings trained on diverse time series work across assets</li> <li>Feature engineering alternative - Replace manual technical indicators with learned features</li> <li>Model compression - Reduce observation dimension while preserving information</li> </ul>"},{"location":"components/transforms/#available-models","title":"Available Models","text":"Model Parameters Embedding Dim Memory Use Case chronos-t5-tiny 8M 512 ~1GB Testing, CI chronos-t5-mini 20M 512 ~2GB Small deployments chronos-t5-small 46M 768 ~3GB Balanced chronos-t5-base 200M 768 ~5GB Standard (recommended) chronos-t5-large 710M 1024 ~12GB Best performance"},{"location":"components/transforms/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Default Description <code>in_keys</code> list[str] Required Market data keys to embed (e.g., <code>[\"market_data_1Minute_12\"]</code>) <code>out_keys</code> list[str] Required Output embedding keys (e.g., <code>[\"chronos_embedding\"]</code>) <code>model_name</code> str \"amazon/chronos-t5-large\" HuggingFace model identifier <code>aggregation</code> str \"mean\" How to aggregate multi-feature embeddings (\"mean\", \"max\", \"concat\") <code>del_keys</code> bool True Remove input keys after transformation <code>device</code> str Auto-detect Device for inference (\"cuda\", \"cpu\", \"mps\") <code>torch_dtype</code> dtype torch.bfloat16 Model precision (bfloat16 recommended for memory)"},{"location":"components/transforms/#usage-example_1","title":"Usage Example","text":"<pre><code>from torchrl.envs import TransformedEnv, Compose\nfrom torchtrade.envs.transforms import ChronosEmbeddingTransform\n\n# Create environment with Chronos embedding\nenv = TransformedEnv(\n    base_env,\n    Compose(\n        ChronosEmbeddingTransform(\n            in_keys=[\"market_data_1Minute_12\"],\n            out_keys=[\"chronos_embedding\"],\n            model_name=\"amazon/chronos-t5-large\",\n            aggregation=\"mean\",  # Average across OHLCV features\n        ),\n        # ... other transforms ...\n    )\n)\n\n# Observation spec automatically updated\n# Before: {\"market_data_1Minute_12\": (12, 5)}\n# After:  {\"chronos_embedding\": (1024,)}  # For chronos-t5-large\n</code></pre>"},{"location":"components/transforms/#aggregation-strategies","title":"Aggregation Strategies","text":"<p>When input has multiple features (e.g., OHLCV has 5 features), choose aggregation method:</p> <p>Mean (default): <pre><code>aggregation=\"mean\"  # Output: (embedding_dim,)\n# Averages embeddings across features\n# Best for: Reducing dimensionality, treating features equally\n</code></pre></p> <p>Max: <pre><code>aggregation=\"max\"  # Output: (embedding_dim,)\n# Takes element-wise maximum across feature embeddings\n# Best for: Highlighting strongest signals\n</code></pre></p> <p>Concat: <pre><code>aggregation=\"concat\"  # Output: (num_features * embedding_dim,)\n# Concatenates all feature embeddings\n# Best for: Preserving feature-specific information, larger models\n</code></pre></p>"},{"location":"components/transforms/#integration-with-append_transform","title":"Integration with append_transform","text":"<p>For environments already created, use <code>append_transform</code>:</p> <pre><code>from torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\n\n# Create environment\nconfig = SeqLongOnlyEnvConfig(...)\nenv = SeqLongOnlyEnv(config)\n\n# Append Chronos transform\nenv = env.append_transform(\n    ChronosEmbeddingTransform(\n        in_keys=env.market_data_keys,  # Use environment's market data keys\n        out_keys=[\"chronos_embedding\"],\n        model_name=\"amazon/chronos-t5-base\",\n    )\n)\n</code></pre>"},{"location":"components/transforms/#performance-considerations","title":"Performance Considerations","text":"<p>Memory: - Use <code>torch_dtype=torch.bfloat16</code> to reduce memory footprint - Smaller models (mini/small) for resource-constrained environments - Embeddings are computed on CPU then moved to GPU to avoid OOM errors</p> <p>Speed: - First forward pass loads model (1-5 seconds depending on size) - Subsequent passes are fast (~10-50ms per batch depending on model) - Use vLLM backend for production deployment (not yet supported)</p> <p>Example Installation: <pre><code>pip install git+https://github.com/amazon-science/chronos-forecasting.git\n</code></pre></p> <p>Paper Reference: \"Chronos: Learning the Language of Time Series\" (arXiv:2403.07815)</p> <p>Model Hub: amazon-science/chronos-forecasting</p> <p>Code Reference: <code>torchtrade/envs/transforms/chronos_embedding.py</code></p>"},{"location":"components/transforms/#quick-reference","title":"Quick Reference","text":""},{"location":"components/transforms/#transform-selection-guide","title":"Transform Selection Guide","text":"Scenario Recommended Transform Notes Monitor data coverage CoverageTracker Use as collector postproc Pretrained features ChronosEmbeddingTransform Replace raw OHLCV with embeddings Standard preprocessing TorchRL built-ins DoubleToFloat, RewardSum, etc."},{"location":"components/transforms/#common-patterns","title":"Common Patterns","text":"<p>Pattern 1: Coverage Monitoring (Training) <pre><code># Create postproc transform (NOT in env transform chain)\ncoverage = CoverageTracker()\n\n# Use in collector\ncollector = SyncDataCollector(\n    env, policy,\n    postproc=coverage,\n)\n\n# Log periodically\nstats = coverage.get_coverage_stats()\n</code></pre></p> <p>Pattern 2: Chronos Embedding (Feature Engineering) <pre><code># Replace raw OHLCV with learned embeddings\nenv = TransformedEnv(\n    base_env,\n    Compose(\n        ChronosEmbeddingTransform(\n            in_keys=[\"market_data_1Minute_12\"],\n            out_keys=[\"embedding\"],\n            model_name=\"amazon/chronos-t5-base\",\n        ),\n        DoubleToFloat(),\n        RewardSum(),\n    )\n)\n</code></pre></p> <p>Pattern 3: Multi-Timeframe Embedding <pre><code># Embed multiple timeframes separately\nChronosEmbeddingTransform(\n    in_keys=[\"market_data_1Minute_12\", \"market_data_5Minute_8\"],\n    out_keys=[\"embed_1min\", \"embed_5min\"],\n    aggregation=\"concat\",  # Preserve all feature info\n)\n# Policy network concatenates embed_1min + embed_5min\n</code></pre></p>"},{"location":"components/transforms/#implementation-notes","title":"Implementation Notes","text":""},{"location":"components/transforms/#coveragetracker-design","title":"CoverageTracker Design","text":"<ul> <li>Postproc usage: Designed to run outside environment step loop for zero overhead</li> <li>Dual tracking: Separates episode start diversity from full trajectory coverage</li> <li>Batch aggregation: Uses <code>torch.unique</code> for efficient batch processing</li> <li>Auto-initialization: Detects dataset size from environment or first batch</li> </ul>"},{"location":"components/transforms/#chronosembeddingtransform-design","title":"ChronosEmbeddingTransform Design","text":"<ul> <li>Lazy loading: Model loaded on first forward pass, not init</li> <li>CPU embedding: Chronos requires CPU for embedding extraction (library limitation)</li> <li>Observation spec update: Automatically updates environment specs with embedding dimensions</li> <li>Batch processing: Handles both single and batched observations from parallel environments</li> </ul>"},{"location":"components/transforms/#see-also","title":"See Also","text":"<ul> <li>TorchRL Transforms Documentation - Built-in transforms</li> <li>Feature Engineering - Manual feature engineering patterns</li> <li>Loss Functions - Training objectives that work with transforms</li> <li>Example: Chronos Embedding - Complete usage example</li> </ul>"},{"location":"environments/offline-rl/","title":"Offline RL","text":"<p>TorchTrade supports offline reinforcement learning, enabling agents to learn from pre-collected datasets without requiring live environment interaction during training.</p>"},{"location":"environments/offline-rl/#overview","title":"Overview","text":"<p>TorchTrade provides TensorDict-based datasets that can be loaded and used directly with TorchRL's replay buffer. These datasets are available for download from HuggingFace/Torch-Trade and contain pre-collected trading trajectories for offline RL research.</p> <p>Offline RL can be performed using datasets collected from two sources:</p> <ol> <li>Offline Environment Interactions - Collect trajectories by running policies in backtesting environments (SeqLongOnlyEnv, SeqFuturesEnv, etc.)</li> <li>Real Online Environment Interactions - Record actual trading data from live exchanges (Alpaca, Binance, Bitget)</li> </ol> <p>This approach is particularly valuable for: - Learning from expert demonstrations or historical trading data - Training without market risk or transaction costs - Developing policies when live interaction is expensive or dangerous - Bootstrapping learning before deploying to real markets</p>"},{"location":"environments/offline-rl/#example-iql-implicit-q-learning","title":"Example: IQL (Implicit Q-Learning)","text":"<p>TorchTrade provides an example implementation of offline RL using Implicit Q-Learning (IQL) in <code>examples/offline/iql/</code>.</p> <pre><code># Example: Training IQL on pre-collected dataset\nfrom torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\nfrom torchrl.collectors import SyncDataCollector\nfrom torchrl.data import LazyTensorStorage, TensorDictReplayBuffer\n\n# 1. Create environment (for evaluation only)\nenv = SeqLongOnlyEnv(df, config)\n\n# 2. Load pre-collected dataset\n# Dataset should contain trajectories: (observation, action, reward, next_observation, done)\nreplay_buffer = TensorDictReplayBuffer(\n    storage=LazyTensorStorage(max_size=1_000_000),\n)\n\n# 3. Train IQL from offline data\nfor batch in replay_buffer:\n    loss = iql_loss_module(batch)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>For a complete implementation, see examples/offline/iql/.</p>"},{"location":"environments/offline-rl/#dataset-collection","title":"Dataset Collection","text":""},{"location":"environments/offline-rl/#from-offline-environments","title":"From Offline Environments","text":"<pre><code>from torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\nfrom torchrl.collectors import SyncDataCollector\nfrom torchrl.data import LazyTensorStorage, TensorDictReplayBuffer\n\n# Collect trajectories with any policy (random, rule-based, pre-trained)\ncollector = SyncDataCollector(\n    env,\n    policy,\n    frames_per_batch=10000,\n    total_frames=1_000_000,\n)\n\n# Store in replay buffer\nreplay_buffer = TensorDictReplayBuffer(\n    storage=LazyTensorStorage(max_size=1_000_000),\n)\n\nfor batch in collector:\n    replay_buffer.extend(batch)\n</code></pre>"},{"location":"environments/offline-rl/#from-real-online-environments","title":"From Real Online Environments","text":"<pre><code>from torchtrade.envs.alpaca import AlpacaTorchTradingEnv, AlpacaTradingEnvConfig\n\n# Collect real trading data (paper trading recommended)\nenv = AlpacaTorchTradingEnv(config)\n\n# Record interactions\nfor episode in range(num_episodes):\n    td = env.reset()\n    while not td[\"done\"].item():\n        action = policy(td)\n        td = env.step(td)\n        replay_buffer.add(td)\n</code></pre>"},{"location":"environments/offline-rl/#provided-datasets","title":"Provided Datasets","text":"<p>Coming Soon</p> <p>We plan to provide pre-collected datasets on HuggingFace for offline RL research, including:</p> <ul> <li>Expert demonstrations from rule-based strategies</li> <li>Random policy trajectories for benchmarking</li> <li>Real market interaction data (paper trading)</li> </ul> <p>Stay tuned at HuggingFace/Torch-Trade!</p>"},{"location":"environments/offline-rl/#additional-offline-rl-algorithms","title":"Additional Offline RL Algorithms","text":"<p>TorchTrade's offline RL support is compatible with any offline RL algorithm from TorchRL, including:</p> <ul> <li>CQL (Conservative Q-Learning) - Addresses overestimation in offline Q-learning</li> <li>TD3+BC - Combines TD3 with behavior cloning for offline learning</li> <li>Decision Transformer - Sequence modeling approach to offline RL</li> <li>Any TorchRL algorithm - Use replay buffers with offline data</li> </ul>"},{"location":"environments/offline-rl/#next-steps","title":"Next Steps","text":"<ul> <li>IQL Example - Complete offline RL implementation</li> <li>Offline Environments - Environments for dataset collection</li> <li>Online Environments - Live trading for data collection</li> <li>Examples - Browse all training examples</li> </ul>"},{"location":"environments/offline-rl/#references","title":"References","text":"<ul> <li>IQL Paper - Implicit Q-Learning algorithm</li> <li>TorchRL Replay Buffers - Data storage and sampling</li> <li>Offline RL Guide - Comprehensive offline RL guide</li> </ul>"},{"location":"environments/offline/","title":"Offline Environments","text":"<p>Offline environments are designed for training on historical data (backtesting). These are not \"offline RL\" methods like CQL or IQL, but rather environments that use pre-collected market data instead of live exchange APIs.</p>"},{"location":"environments/offline/#environment-types","title":"Environment Types","text":"<p>TorchTrade currently offers three main categories of offline environments, each designed for different trading strategies and learning paradigms:</p>"},{"location":"environments/offline/#sequential-environments-seq","title":"Sequential Environments (Seq)","text":"<p>Environments: <code>SeqLongOnlyEnv</code>, <code>SeqFuturesEnv</code></p> <p>These are the foundational environments for step-by-step trading with fractional position sizing. The agent can continuously adapt its position at every timestep:</p> <ul> <li>Fractional actions: Action values represent the fraction of capital to deploy (e.g., 0.5 = 50% allocation)</li> <li>Flexible position management: Agent can adjust position size up or down at any time, or go completely market neutral (action = 0)</li> <li>Futures support: <code>SeqFuturesEnv</code> adds leverage (1-125x), margin management, and liquidation mechanics</li> <li>No leverage: <code>SeqLongOnlyEnv</code> uses spot trading without leverage (long-only positions)</li> </ul> <p>Key characteristic: Maximum flexibility - the agent can rebalance or exit positions freely at every decision point.</p>"},{"location":"environments/offline/#sltp-environments-stop-losstake-profit","title":"SL/TP Environments (Stop-Loss/Take-Profit)","text":"<p>Environments: <code>SeqLongOnlySLTPEnv</code>, <code>SeqFuturesSLTPEnv</code></p> <p>These environments extend sequential environments with bracket order risk management:</p> <ul> <li>Bracket orders: Each trade includes configurable stop-loss and take-profit trigger levels</li> <li>Combinatorial action space: Agent selects from combinations of SL/TP levels (e.g., SL=-2% with TP=+5%)</li> <li>Risk mitigation: Automated exit triggers help limit downside and lock in profits</li> <li>Still flexible: Agent can manually exit positions or switch directions before triggers are hit</li> </ul> <p>Key characteristic: Structured risk management with predefined exit conditions, but maintains flexibility for manual intervention.</p>"},{"location":"environments/offline/#onestep-environments-episodic-rollouts","title":"OneStep Environments (Episodic Rollouts)","text":"<p>Environments: <code>LongOnlyOneStepEnv</code>, <code>FuturesOneStepEnv</code></p> <p>These environments are optimized for fast episodic training with algorithms like GRPO:</p> <ul> <li>Single decision point: Agent observes market state, takes one action, and episode completes</li> <li>Internal rollouts: After action is taken, environment internally simulates the position until SL/TP triggers or max rollout length</li> <li>No mid-episode adjustments: Unlike sequential environments, the agent cannot change position during the rollout</li> <li>Terminal rewards: Returns cumulative PnL from the entire rollout as a single terminal reward</li> </ul> <p>Key characteristic: Optimized for fast iteration and contextual bandit-style learning. Policies can be deployed to sequential environments for step-by-step execution.</p> <p>Extensible Framework</p> <p>This is the current base setup of TorchTrade environments. The framework is designed to be extensible:</p> <ul> <li>We plan to add more environment variants based on user needs and research directions</li> <li>Users can create custom environments by inheriting from existing base classes or implementing new ones from scratch</li> <li>See Building Custom Environments for guidance on extending TorchTrade</li> </ul>"},{"location":"environments/offline/#overview","title":"Overview","text":"<p>TorchTrade provides 6 offline environments:</p> Environment Asset Type Futures Leverage Bracket Orders One-Step SeqLongOnlyEnv Crypto/Stocks \u274c \u274c \u274c \u274c SeqLongOnlySLTPEnv Crypto/Stocks \u274c \u274c \u2705 \u274c LongOnlyOneStepEnv Crypto/Stocks \u274c \u274c \u2705 \u2705 SeqFuturesEnv Crypto \u2705 \u2705 \u274c \u274c SeqFuturesSLTPEnv Crypto \u2705 \u2705 \u2705 \u274c FuturesOneStepEnv Crypto \u2705 \u2705 \u2705 \u2705"},{"location":"environments/offline/#action-space-control","title":"Action Space Control","text":"<p>All environments support the <code>include_hold_action</code> parameter (default: <code>True</code>):</p> <ul> <li><code>include_hold_action=True</code> (default): Includes HOLD/no-op action in the action space</li> <li>Standard environments: 3 actions (Sell/Short, Hold, Buy/Long)</li> <li> <p>SLTP environments: HOLD action + SL/TP combinations</p> </li> <li> <p><code>include_hold_action=False</code>: Removes HOLD action, forcing the agent to always take a trading action</p> </li> <li>Standard environments: 2 actions (Sell/Short, Buy/Long)</li> <li>SLTP environments: Only SL/TP combinations (no HOLD)</li> </ul> <p>Use Case: Set to <code>False</code> when you want to ensure the agent is always actively trading rather than holding neutral positions. Useful for testing aggressive strategies or ensuring the agent explores trading actions.</p>"},{"location":"environments/offline/#fractional-position-sizing","title":"Fractional Position Sizing","text":"<p>Non-SLTP environments (<code>SeqLongOnlyEnv</code>, <code>SeqFuturesEnv</code>) support fractional position sizing where action values directly represent the fraction of balance to allocate to positions.</p>"},{"location":"environments/offline/#how-it-works","title":"How It Works","text":"<p>Action Interpretation: - Action values range from -1.0 to 1.0 - Magnitude = fraction of balance to allocate (0.5 = 50%, 1.0 = 100%) - Sign = direction (positive = long, negative = short) - Zero = market neutral (close all positions, stay in cash)</p> <p>Position Sizing Formula: <pre><code># For futures environments with leverage:\nposition_size = (balance \u00d7 |action| \u00d7 leverage) / price\n\n# For long-only environments (no leverage):\nposition_size = (balance \u00d7 action) / price\n</code></pre></p> <p>Examples:</p> Futures (SeqFuturesEnv)Long-Only (SeqLongOnlyEnv) <pre><code>from torchtrade.envs.offline import SeqFuturesEnv, SeqFuturesEnvConfig\n\nconfig = SeqFuturesEnvConfig(\n    leverage=5,  # Fixed 5x leverage\n    action_levels=[-1.0, -0.5, 0.0, 0.5, 1.0],  # 5 discrete actions\n    initial_cash=10000\n)\nenv = SeqFuturesEnv(df, config)\n\n# Action interpretation with $10k balance, 5x leverage, $50k BTC price:\n# action = -1.0  \u2192 100% short: (10k \u00d7 1.0 \u00d7 5) / 50k = -1.0 BTC short\n# action = -0.5  \u2192 50% short:  (10k \u00d7 0.5 \u00d7 5) / 50k = -0.5 BTC short\n# action =  0.0  \u2192 Market neutral: 0 BTC (flat, all cash)\n# action =  0.5  \u2192 50% long:   (10k \u00d7 0.5 \u00d7 5) / 50k = 0.5 BTC long\n# action =  1.0  \u2192 100% long:  (10k \u00d7 1.0 \u00d7 5) / 50k = 1.0 BTC long\n</code></pre> <pre><code>from torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\n\nconfig = SeqLongOnlyEnvConfig(\n    action_levels=[0.0, 0.5, 1.0],  # Default: close, 50%, 100%\n    initial_cash=10000\n)\nenv = SeqLongOnlyEnv(df, config)\n\n# Action interpretation with $10k balance, $50k BTC price:\n# action =  0.0  \u2192 Close position (go to 100% cash)\n# action =  0.5  \u2192 50% invested: 10k \u00d7 0.5 / 50k = 0.1 BTC\n# action =  1.0  \u2192 100% invested: 10k \u00d7 1.0 / 50k = 0.2 BTC\n\n# Note: Negative actions are technically supported for backwards compatibility\n# but are not recommended as they add redundancy (behave same as action=0)\n</code></pre>"},{"location":"environments/offline/#customizing-action-levels","title":"Customizing Action Levels","text":"<p>Action levels are fully customizable. You can specify any list of values in [-1.0, 1.0]:</p> <pre><code># Fine-grained control (9 actions)\naction_levels = [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0]\n\n# More precision near neutral (7 actions) - asymmetric\naction_levels = [-1.0, -0.3, -0.1, 0.0, 0.1, 0.3, 1.0]\n\n# Conservative (no full positions, 5 actions)\naction_levels = [-0.5, -0.25, 0.0, 0.25, 0.5]\n\n# Long-only (buy-focused, 5 actions)\naction_levels = [0.0, 0.25, 0.5, 0.75, 1.0]\n\n# Very coarse (3 actions)\naction_levels = [-1.0, 0.0, 1.0]\n</code></pre> <p>Default Values: - Futures: <code>[-1.0, -0.5, 0.0, 0.5, 1.0]</code> (5 actions: short/neutral/long) - Long-only: <code>[0.0, 0.5, 1.0]</code> (3 actions: close/half/full invested)</p> <p>Note: For long-only, action=0.0 closes the position. Negative actions are technically supported for backwards compatibility but not recommended (they behave identically to action=0.0, adding redundancy to the action space).</p>"},{"location":"environments/offline/#efficient-partial-adjustments","title":"Efficient Partial Adjustments","text":"<p>When adjusting position size (e.g., going from 100% to 50%), the environment only trades the delta:</p> <pre><code># Current position: 1.0 BTC long (100%)\n# New action: 0.5 (target 50% long)\n# Environment sells only 0.5 BTC (the delta)\n# \u2705 Reduces fees by only trading what changed\n# \u2705 Preserves entry price for remaining position\n</code></pre>"},{"location":"environments/offline/#custom-action-levels-example","title":"Custom Action Levels Example","text":"<p>You can customize action levels for finer control over position sizing:</p> <pre><code># Example: More granular position control\nconfig = SeqFuturesEnvConfig(\n    action_levels=[-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n    leverage=5,\n)\n# Each action level controls the fraction of balance to allocate\n# Action values must be in range [-1.0, 1.0]\n# Leverage is set separately and applies to all positions\n</code></pre> <p>Important: Action levels must be in range <code>[-1.0, 1.0]</code>. Values outside this range will fail validation. To adjust position sizes, modify the <code>leverage</code> parameter instead of using values outside the valid range</p>"},{"location":"environments/offline/#leverage-design-futures-environments","title":"Leverage Design (Futures Environments)","text":"<p>For futures environments (<code>SeqFuturesEnv</code>, <code>SeqFuturesSLTPEnv</code>), leverage is a fixed global parameter, not part of the action space.</p> <p>Design Philosophy: - Leverage = \"How much risk am I willing to take?\" (configuration/risk management) - Action = \"How much of my allocation should I deploy?\" (learned policy) - These are fundamentally different questions and should be separated</p> <p>Benefits of Fixed Leverage:</p> <ol> <li>Simpler Learning: Smaller action space \u2192 faster convergence</li> <li>Better Risk Control: Easy to enforce global leverage constraints</li> <li>Easier Tuning: Sweep leverage independently of action granularity</li> <li>Clear Semantics: <code>action=0.5</code> always means \"use 50% of capital\" regardless of leverage</li> <li>Matches Trader Workflows: Most traders choose leverage once, then size positions</li> </ol> <p>Dynamic Leverage (Not Currently Implemented):</p> <p>If you need agents to dynamically choose leverage per trade, this could be implemented as multi-dimensional action spaces:</p> <pre><code># Future extension (not currently available):\naction_space = {\n    \"position_fraction\": Categorical([-1, -0.5, 0, 0.5, 1]),  # Position sizing\n    \"leverage_multiplier\": Categorical([1, 3, 5])              # Leverage choice\n}\n</code></pre> <p>However, fixed leverage is recommended for most use cases. Dynamic leverage: - Increases action space complexity (harder to learn) - Makes risk management harder (no global constraint) - Is rarely used in practice (traders typically fix leverage)</p> <p>Timeframe Format - Critical for Model Compatibility</p> <p>When specifying <code>time_frames</code>, always use canonical forms:</p> <ul> <li>\u2705 Correct: <code>[\"1min\", \"5min\", \"15min\", \"1hour\", \"1day\"]</code></li> <li>\u274c Wrong: <code>[\"1min\", \"5min\", \"15min\", \"60min\"]</code></li> </ul> <p>Why this matters:</p> <ul> <li><code>time_frames=[\"60min\"]</code> creates observation key <code>\"market_data_60Minute\"</code></li> <li><code>time_frames=[\"1hour\"]</code> creates observation key <code>\"market_data_1Hour\"</code></li> <li>These are DIFFERENT keys - your model trained with <code>\"60min\"</code> won't work with config using <code>\"1hour\"</code></li> </ul> <p>The framework will issue a warning if you use non-canonical forms. Use the suggested canonical forms to ensure model compatibility and cleaner observation keys.</p> <p>Common conversions:</p> <ul> <li><code>60min</code> \u2192 use <code>1hour</code></li> <li><code>120min</code> \u2192 use <code>2hours</code></li> <li><code>1440min</code> \u2192 use <code>1day</code></li> <li><code>24hour</code> \u2192 use <code>1day</code></li> </ul>"},{"location":"environments/offline/#seqlongonlyenv","title":"SeqLongOnlyEnv","text":"<p>Simple long-only spot trading environment for sequential RL algorithms. This is the offline backtesting equivalent of the AlpacaTorchTradingEnv live environment, designed for training on historical data before deploying to live stock and crypto markets.</p>"},{"location":"environments/offline/#features","title":"Features","text":"<ul> <li>Long-only trading: Can only buy and hold (no short positions)</li> <li>Sequential episodes: Step-by-step trading simulation</li> <li>3-action discrete space: SELL (0), HOLD (1), BUY (2)</li> <li>Multi-timeframe observations: Observe multiple time scales simultaneously</li> </ul>"},{"location":"environments/offline/#configuration","title":"Configuration","text":"<pre><code>from torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\n\nconfig = SeqLongOnlyEnvConfig(\n    # Multi-timeframe setup\n    time_frames=[\"1min\", \"5min\", \"15min\", \"1hour\"],        # Values: \"1min\", \"5min\", \"15min\", \"1hour\", \"1day\"\n    window_sizes=[12, 8, 8, 24],       # Lookback per timeframe\n    execute_on=(5, \"Minute\"),          # Execute every 5 minutes\n\n    # Trading parameters\n    initial_cash=1000,                 # Starting capital\n    transaction_fee=0.0025,            # 0.25% per trade\n    slippage=0.001,                    # 0.1% slippage\n\n    # Episode configuration\n    max_traj_length=None,              # Full dataset (or set limit)\n    random_start=True,                 # True = random episode starts, False = sequential\n    include_hold_action=True,          # Include HOLD action (default: True)\n\n    # Feature preprocessing (optional)\n    feature_preprocessing_fn=None,      # Custom feature function\n\n    # Reward function (optional)\n    reward_function=None,               # Use default log return\n)\n\n# Create environment\nenv = SeqLongOnlyEnv(df, config)\n</code></pre>"},{"location":"environments/offline/#observation-space","title":"Observation Space","text":"<pre><code>observation = {\n    \"market_data_1Minute\": Tensor([12, num_features]),    # 1m window\n    \"market_data_5Minute\": Tensor([8, num_features]),     # 5m window\n    \"market_data_15Minute\": Tensor([8, num_features]),    # 15m window\n    \"market_data_1Hour\": Tensor([24, num_features]),      # 1h window\n    \"account_state\": Tensor([7]),                         # Account state\n}\n\n# Account state (7 elements):\n# [cash, position_size, position_value, entry_price,\n#  current_price, unrealized_pnl_pct, holding_time]\n</code></pre>"},{"location":"environments/offline/#action-space","title":"Action Space","text":"<p>Default (include_hold_action=True) - Discrete(3): - Action 0: SELL - Close current position - Action 1: HOLD - Do nothing - Action 2: BUY - Open position (100% of cash)</p> <p>Without HOLD (include_hold_action=False) - Discrete(2): - Action 0: SELL - Close current position - Action 1: BUY - Open position (100% of cash)</p>"},{"location":"environments/offline/#example-usage","title":"Example Usage","text":"<pre><code>import pandas as pd\nfrom torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\n\n# Load data\ndf = pd.read_csv(\"btcusdt_1m.csv\")\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n\n# Configure environment\nconfig = SeqLongOnlyEnvConfig(\n    time_frames=[\"1min\", \"5min\", \"15min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n    initial_cash=1000\n)\n\nenv = SeqLongOnlyEnv(df, config)\n\n# Run episode\ntensordict = env.reset()\ndone = False\n\nwhile not done:\n    # Your policy selects action\n    action = policy(tensordict)  # Returns action in [0, 1, 2]\n    tensordict[\"action\"] = action\n\n    # Step environment\n    tensordict = env.step(tensordict)\n    done = tensordict[\"done\"].item()\n\n    print(f\"Reward: {tensordict['reward'].item():.4f}\")\n</code></pre>"},{"location":"environments/offline/#seqlongonlysltpenv","title":"SeqLongOnlySLTPEnv","text":"<p>Long-only environment with stop-loss and take-profit bracket orders. This is the offline backtesting equivalent of the AlpacaSLTPTorchTradingEnv live environment, adding risk management through bracket orders for stock and crypto trading.</p>"},{"location":"environments/offline/#features_1","title":"Features","text":"<ul> <li>All features of <code>SeqLongOnlyEnv</code></li> <li>Bracket orders: Each buy includes SL and TP levels</li> <li>Combinatorial action space: HOLD + (num_sl \u00d7 num_tp) buy actions</li> <li>Automatic position management: Orders close on SL/TP trigger</li> </ul>"},{"location":"environments/offline/#configuration_1","title":"Configuration","text":"<pre><code>from torchtrade.envs.offline import SeqLongOnlySLTPEnv, SeqLongOnlySLTPEnvConfig\n\nconfig = SeqLongOnlySLTPEnvConfig(\n    # Stop-loss / Take-profit levels\n    stoploss_levels=[-0.02, -0.05],     # -2%, -5%\n    takeprofit_levels=[0.05, 0.10],     # +5%, +10%\n    include_hold_action=True,           # Optional: set False to remove HOLD\n\n    # Multi-timeframe setup (same as SeqLongOnlyEnv)\n    time_frames=[\"1min\", \"5min\", \"15min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n\n    # Trading parameters\n    initial_cash=1000,\n    transaction_fee=0.0025,\n    slippage=0.001,\n)\n\nenv = SeqLongOnlySLTPEnv(df, config)\n</code></pre>"},{"location":"environments/offline/#action-space_1","title":"Action Space","text":"<p>Formula: - With HOLD (include_hold_action=True): Discrete(1 + num_sl \u00d7 num_tp) - Without HOLD (include_hold_action=False): Discrete(num_sl \u00d7 num_tp)</p> <p>Example with include_hold_action=True:</p> <p>With <code>stoploss_levels=[-0.02, -0.05]</code> and <code>takeprofit_levels=[0.05, 0.10]</code>:</p> <ul> <li>Action 0: HOLD / Close position</li> <li>Action 1: BUY with SL=-2%, TP=+5%</li> <li>Action 2: BUY with SL=-2%, TP=+10%</li> <li>Action 3: BUY with SL=-5%, TP=+5%</li> <li>Action 4: BUY with SL=-5%, TP=+10%</li> </ul> <p>Total: 1 + (2 \u00d7 2) = 5 actions</p> <p>Example with include_hold_action=False:</p> <p>Same levels but no HOLD action:</p> <ul> <li>Action 0: BUY with SL=-2%, TP=+5%</li> <li>Action 1: BUY with SL=-2%, TP=+10%</li> <li>Action 2: BUY with SL=-5%, TP=+5%</li> <li>Action 3: BUY with SL=-5%, TP=+10%</li> </ul> <p>Total: 2 \u00d7 2 = 4 actions</p>"},{"location":"environments/offline/#example-usage_1","title":"Example Usage","text":"<pre><code>from torchtrade.envs.offline import SeqLongOnlySLTPEnv, SeqLongOnlySLTPEnvConfig\n\nconfig = SeqLongOnlySLTPEnvConfig(\n    stoploss_levels=[-0.02, -0.05, -0.10],   # 3 SL levels\n    takeprofit_levels=[0.05, 0.10, 0.15],    # 3 TP levels\n    include_hold_action=True,                # Optional: set False to remove HOLD\n    time_frames=[\"1min\", \"5min\", \"15min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n    initial_cash=1000\n)\n\nenv = SeqLongOnlySLTPEnv(df, config)\n\n# Action space: 1 + (3 \u00d7 3) = 10 actions (or 9 without HOLD)\nprint(f\"Action space size: {env.action_spec.space.n}\")\n</code></pre>"},{"location":"environments/offline/#longonlyonestepenv","title":"LongOnlyOneStepEnv","text":"<p>One-step episodic environment optimized for GRPO and contextual bandit algorithms. The agent receives a randomly sampled observation from historical data, takes a single action, and then the environment simulates a future rollout until bracket orders are triggered (stop-loss or take-profit hit) or the maximum rollout length is reached, completing the episode.</p>"},{"location":"environments/offline/#features_2","title":"Features","text":"<ul> <li>One-step episodes: Each episode is a single decision point</li> <li>Episodic rollout: Action taken, then position held for fixed duration</li> <li>Fast iteration: Train policies quickly with episodic rollouts</li> <li>Bracket orders: Includes SL/TP like <code>SeqLongOnlySLTPEnv</code></li> </ul> <p>Deployment to Sequential Environments</p> <p>Policies trained on <code>LongOnlyOneStepEnv</code> can be directly deployed to <code>SeqLongOnlySLTPEnv</code> for step-by-step trading, since both environments share the same observation and action spaces. This allows fast GRPO-like training followed by sequential backtesting or live trading.</p>"},{"location":"environments/offline/#configuration_2","title":"Configuration","text":"<pre><code>from torchtrade.envs.offline import LongOnlyOneStepEnv, LongOnlyOneStepEnvConfig\n\nconfig = LongOnlyOneStepEnvConfig(\n    # Stop-loss / Take-profit\n    stoploss_levels=[-0.02, -0.05],\n    takeprofit_levels=[0.05, 0.10],\n    include_hold_action=True,           # Optional: set False to remove HOLD\n\n    # Multi-timeframe setup\n    time_frames=[\"1min\", \"5min\", \"15min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n\n    # Rollout configuration\n    rollout_steps=24,                   # Hold position for 24 steps (2 hours)\n\n    # Trading parameters\n    initial_cash=1000,\n    transaction_fee=0.0025,\n    slippage=0.001,\n)\n\nenv = LongOnlyOneStepEnv(df, config)\n</code></pre>"},{"location":"environments/offline/#how-it-works_1","title":"How It Works","text":"<ol> <li>Reset: Environment starts at random market state</li> <li>Action: Agent selects one action (HOLD or BUY with SL/TP)</li> <li>Rollout: Position held for <code>rollout_steps</code>, tracking PnL</li> <li>Reward: Terminal reward computed from rollout returns</li> <li>Done: Episode ends after one action</li> </ol>"},{"location":"environments/offline/#example-usage_2","title":"Example Usage","text":"<pre><code>from torchtrade.envs.offline import LongOnlyOneStepEnv, LongOnlyOneStepEnvConfig\n\nconfig = LongOnlyOneStepEnvConfig(\n    stoploss_levels=[-0.02, -0.05],\n    takeprofit_levels=[0.05, 0.10],\n    time_frames=[\"1min\", \"5min\", \"15min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n    rollout_steps=24,  # 2 hours (24 \u00d7 5min)\n    initial_cash=1000\n)\n\nenv = LongOnlyOneStepEnv(df, config)\n\n# Run episode\ntensordict = env.reset()\naction = policy(tensordict)  # Select action once\ntensordict[\"action\"] = action\ntensordict = env.step(tensordict)  # Episode immediately done\n\nprint(f\"Terminal reward: {tensordict['reward'].item()}\")\nprint(f\"Done: {tensordict['done'].item()}\")  # Always True\n</code></pre>"},{"location":"environments/offline/#seqfuturesenv","title":"SeqFuturesEnv","text":"<p>Futures trading environment with leverage, margin management, and liquidation mechanics. This is the offline backtesting equivalent of the BinanceFuturesTorchTradingEnv and BitgetFuturesTorchTradingEnv live environments, designed for training futures trading strategies on historical data before deploying to live crypto markets.</p>"},{"location":"environments/offline/#features_3","title":"Features","text":"<ul> <li>Futures trading: Long and short positions</li> <li>Leverage support: 1-125x leverage</li> <li>Margin management: Tracks margin ratio and liquidation price</li> <li>Liquidation mechanics: Positions liquidated if margin insufficient</li> <li>3-action discrete: SHORT (0), HOLD (1), LONG (2)</li> </ul>"},{"location":"environments/offline/#configuration_3","title":"Configuration","text":"<pre><code>from torchtrade.envs.offline import SeqFuturesEnv, SeqFuturesEnvConfig\n\nconfig = SeqFuturesEnvConfig(\n    # Multi-timeframe setup\n    time_frames=[\"1min\", \"5min\", \"15min\", \"1hour\"],\n    window_sizes=[12, 8, 8, 24],\n    execute_on=(5, \"Minute\"),\n\n    # Futures parameters\n    leverage=10,                        # 10x leverage\n    margin_call_threshold=0.2,          # Liquidate at 20% margin ratio\n\n    # Trading parameters\n    initial_cash=10000,\n    transaction_fee=0.0004,             # 0.04% (futures have lower fees)\n    slippage=0.001,\n    include_hold_action=True,           # Include HOLD action (default: True)\n\n    # Feature preprocessing (optional)\n    feature_preprocessing_fn=None,\n\n    # Reward function (optional)\n    reward_function=None,\n)\n\nenv = SeqFuturesEnv(df, config)\n</code></pre>"},{"location":"environments/offline/#observation-space_1","title":"Observation Space","text":"<pre><code>observation = {\n    \"market_data_1Minute\": Tensor([12, num_features]),\n    \"market_data_5Minute\": Tensor([8, num_features]),\n    \"market_data_15Minute\": Tensor([8, num_features]),\n    \"market_data_1Hour\": Tensor([24, num_features]),\n    \"account_state\": Tensor([10]),  # 10 elements (futures-specific)\n}\n\n# Account state (10 elements):\n# [cash, position_size, position_value, entry_price, current_price,\n#  unrealized_pnl_pct, leverage, margin_ratio, liquidation_price, holding_time]\n</code></pre>"},{"location":"environments/offline/#action-space_2","title":"Action Space","text":"<p>Default (include_hold_action=True) - Discrete(3): - Action 0: SHORT - Open short position with leverage - Action 1: HOLD - Do nothing or maintain position - Action 2: LONG - Open long position with leverage</p> <p>Without HOLD (include_hold_action=False) - Discrete(2): - Action 0: SHORT - Open short position with leverage - Action 1: LONG - Open long position with leverage</p>"},{"location":"environments/offline/#liquidation-mechanics","title":"Liquidation Mechanics","text":"<p>Positions are liquidated when: - Margin ratio &lt; margin_call_threshold - Margin ratio = (equity / position_value) - Equity = cash + unrealized PnL</p> <p>Example: <pre><code>Initial cash: $10,000\nLeverage: 10x\nPosition value: $100,000 (10x leverage)\nInitial margin: $10,000\n\nIf position loses 20%:\nUnrealized PnL: -$20,000\nEquity: $10,000 - $20,000 = -$10,000 (liquidated!)\n</code></pre></p>"},{"location":"environments/offline/#example-usage_3","title":"Example Usage","text":"<pre><code>from torchtrade.envs.offline import SeqFuturesEnv, SeqFuturesEnvConfig\n\nconfig = SeqFuturesEnvConfig(\n    time_frames=[\"1min\", \"5min\", \"15min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n    leverage=5,                         # 5x leverage\n    margin_call_threshold=0.2,          # 20% margin ratio\n    initial_cash=10000\n)\n\nenv = SeqFuturesEnv(df, config)\n\n# Run episode\ntensordict = env.reset()\n\nwhile not tensordict[\"done\"].item():\n    action = policy(tensordict)\n    tensordict[\"action\"] = action\n    tensordict = env.step(tensordict)\n\n    # Check account state\n    account = tensordict[\"account_state\"]\n    margin_ratio = account[7].item()\n\n    if margin_ratio &lt; 0.3:\n        print(f\"\u26a0\ufe0f Warning: Low margin ratio {margin_ratio:.2f}\")\n</code></pre>"},{"location":"environments/offline/#seqfuturessltpenv","title":"SeqFuturesSLTPEnv","text":"<p>Futures environment with stop-loss/take-profit bracket orders. This is the offline backtesting equivalent of the BinanceFuturesSLTPTorchTradingEnv and BitgetFuturesSLTPTorchTradingEnv live environments, adding risk management through bracket orders for futures trading strategies.</p>"},{"location":"environments/offline/#features_4","title":"Features","text":"<ul> <li>All features of <code>SeqFuturesEnv</code></li> <li>Bracket orders: SL/TP for both long and short positions</li> <li>Combinatorial action space: HOLD + (long SL/TP) + (short SL/TP)</li> </ul>"},{"location":"environments/offline/#configuration_4","title":"Configuration","text":"<pre><code>from torchtrade.envs.offline import SeqFuturesSLTPEnv, SeqFuturesSLTPEnvConfig\n\nconfig = SeqFuturesSLTPEnvConfig(\n    # Stop-loss / Take-profit\n    stoploss_levels=[-0.02, -0.05],\n    takeprofit_levels=[0.05, 0.10],\n    include_hold_action=True,            # Optional: set False to remove HOLD\n    include_short_positions=True,        # Enable short bracket orders\n\n    # Futures parameters\n    leverage=10,\n    margin_call_threshold=0.2,\n\n    # Multi-timeframe setup\n    time_frames=[\"1min\", \"5min\", \"15min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n\n    # Trading parameters\n    initial_cash=10000,\n    transaction_fee=0.0004,\n    slippage=0.001,\n)\n\nenv = SeqFuturesSLTPEnv(df, config)\n</code></pre>"},{"location":"environments/offline/#action-space_3","title":"Action Space","text":"<p>Formula (with <code>include_short_positions=True</code>): - With HOLD (include_hold_action=True): Discrete(1 + 2 \u00d7 (num_sl \u00d7 num_tp)) - Without HOLD (include_hold_action=False): Discrete(2 \u00d7 (num_sl \u00d7 num_tp))</p> <p>Example with include_hold_action=True:</p> <p>With 2 SL levels and 2 TP levels: - Action 0: HOLD / Close position - Actions 1-4: LONG with SL/TP combinations - Actions 5-8: SHORT with SL/TP combinations</p> <p>Total: 1 + 2 \u00d7 (2 \u00d7 2) = 9 actions</p> <p>Example with include_hold_action=False:</p> <p>Same levels but no HOLD action: - Actions 0-3: LONG with SL/TP combinations - Actions 4-7: SHORT with SL/TP combinations</p> <p>Total: 2 \u00d7 (2 \u00d7 2) = 8 actions</p>"},{"location":"environments/offline/#example-usage_4","title":"Example Usage","text":"<pre><code>from torchtrade.envs.offline import SeqFuturesSLTPEnv, SeqFuturesSLTPEnvConfig\n\nconfig = SeqFuturesSLTPEnvConfig(\n    stoploss_levels=[-0.02, -0.05],\n    takeprofit_levels=[0.05, 0.10],\n    include_short_positions=True,\n    leverage=10,\n    margin_call_threshold=0.2,\n    time_frames=[\"1min\", \"5min\", \"15min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n    initial_cash=10000\n)\n\nenv = SeqFuturesSLTPEnv(df, config)\nprint(f\"Action space size: {env.action_spec.space.n}\")  # 9 actions\n</code></pre>"},{"location":"environments/offline/#futuresonestepenv","title":"FuturesOneStepEnv","text":"<p>One-step episodic futures environment optimized for GRPO training. The agent receives a randomly sampled observation from historical data, takes a single action (long/short with SL/TP or hold), and then the environment simulates a future rollout with leverage and margin mechanics until bracket orders are triggered or the maximum rollout length is reached, completing the episode.</p>"},{"location":"environments/offline/#features_5","title":"Features","text":"<ul> <li>Combines <code>SeqFuturesEnv</code> + <code>LongOnlyOneStepEnv</code></li> <li>One-step episodes: Single decision with episodic rollout</li> <li>Futures with leverage: Up to 125x leverage</li> <li>Bracket orders: SL/TP for long and short</li> </ul> <p>Deployment to Sequential Environments</p> <p>Policies trained on <code>FuturesOneStepEnv</code> can be directly deployed to <code>SeqFuturesSLTPEnv</code> for step-by-step trading, since both environments share the same observation and action spaces. This allows fast GRPO-like training followed by sequential backtesting or live trading.</p>"},{"location":"environments/offline/#configuration_5","title":"Configuration","text":"<pre><code>from torchtrade.envs.offline import FuturesOneStepEnv, FuturesOneStepEnvConfig\n\nconfig = FuturesOneStepEnvConfig(\n    # Stop-loss / Take-profit\n    stoploss_levels=[-0.02, -0.05],\n    takeprofit_levels=[0.05, 0.10],\n    include_hold_action=True,            # Optional: set False to remove HOLD\n    include_short_positions=True,\n\n    # Futures parameters\n    leverage=10,\n    margin_call_threshold=0.2,\n\n    # Rollout configuration\n    rollout_steps=24,\n\n    # Multi-timeframe setup\n    time_frames=[\"1min\", \"5min\", \"15min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n\n    # Trading parameters\n    initial_cash=10000,\n    transaction_fee=0.0004,\n)\n\nenv = FuturesOneStepEnv(df, config)\n</code></pre>"},{"location":"environments/offline/#example-usage_5","title":"Example Usage","text":"<pre><code>from torchtrade.envs.offline import FuturesOneStepEnv, FuturesOneStepEnvConfig\n\nconfig = FuturesOneStepEnvConfig(\n    stoploss_levels=[-0.02, -0.05],\n    takeprofit_levels=[0.05, 0.10],\n    include_short_positions=True,\n    leverage=5,\n    rollout_steps=24,  # Hold for 2 hours\n    time_frames=[\"1min\", \"5min\", \"15min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n    initial_cash=10000\n)\n\nenv = FuturesOneStepEnv(df, config)\n\n# Each episode is one decision\ntensordict = env.reset()\naction = policy(tensordict)\ntensordict[\"action\"] = action\ntensordict = env.step(tensordict)  # Done immediately\n\nprint(f\"Terminal reward: {tensordict['reward'].item()}\")\n</code></pre>"},{"location":"environments/offline/#state-management","title":"State Management","text":"<p>All TorchTrade environments (both offline and online) use structured state management through dedicated dataclasses found in <code>torchtrade/envs/state.py</code>.</p>"},{"location":"environments/offline/#positionstate","title":"PositionState","text":"<p>The <code>PositionState</code> dataclass encapsulates all position-related state in a single object:</p> <pre><code>from torchtrade.envs.state import PositionState\n\n# Used internally by all environments\nposition = PositionState()\n\n# Available attributes:\nposition.current_position  # 0=no position, 1=long, -1=short\nposition.position_size     # Number of units held (negative for shorts)\nposition.position_value    # Current market value of the position\nposition.entry_price       # Price at which position was entered\nposition.unrealized_pnlpc  # Unrealized P&amp;L as percentage\nposition.hold_counter      # Number of steps position has been held\n\n# Reset all fields at once\nposition.reset()\n</code></pre> <p>Benefits: - Groups related state variables together - Provides single <code>.reset()</code> method for all position fields - Makes position state explicit and easier to track - Used consistently across all TorchTrade environments (offline and online)</p> <p>See <code>torchtrade/envs/state.py:8-30</code> for implementation details.</p>"},{"location":"environments/offline/#historytracker","title":"HistoryTracker","text":"<p>The <code>HistoryTracker</code> class records episode data for analysis and visualization:</p> <pre><code>from torchtrade.envs.state import HistoryTracker, FuturesHistoryTracker\n\n# For long-only environments\nhistory = HistoryTracker()\n\n# During episode\nhistory.record_step(\n    price=50000.0,\n    action=1.0,\n    reward=0.05,\n    portfolio_value=10500.0\n)\n\n# Export for plotting/analysis\ndata = history.to_dict()\n# Returns: {'base_prices': [...], 'actions': [...], 'rewards': [...], 'portfolio_values': [...]}\n\n# Reset for new episode\nhistory.reset()\n</code></pre> <p>FuturesHistoryTracker extends <code>HistoryTracker</code> with position tracking:</p> <pre><code># For futures environments (SeqFuturesEnv, etc.)\nhistory = FuturesHistoryTracker()\n\nhistory.record_step(\n    price=50000.0,\n    action=2.0,\n    reward=0.03,\n    portfolio_value=10300.0,\n    position=0.5  # Positive=long, negative=short\n)\n\n# Export includes position history\ndata = history.to_dict()\n# Returns: {..., 'positions': [0.5, ...]}\n</code></pre> <p>Use Cases: - Plot portfolio value over time - Analyze action distributions - Track position holding patterns - Debug environment behavior - Generate performance metrics</p> <p>Note: History tracking is available in both offline environments (SeqLongOnlyEnv, SeqFuturesEnv, etc.) and online environments (AlpacaTorchTradingEnv, BinanceFuturesTorchTradingEnv, etc.).</p> <p>See <code>torchtrade/envs/state.py:33-148</code> for implementation details.</p>"},{"location":"environments/offline/#visualizing-episode-history","title":"Visualizing Episode History","text":"<p>All offline environments support <code>render_history()</code> to visualize trading performance. The method automatically creates plots showing:</p> <ul> <li>Long-only environments: Price + actions, portfolio vs buy-and-hold (2 subplots)</li> <li>Futures environments: Price + actions, portfolio vs buy-and-hold, position history (3 subplots)</li> </ul> <p>See Visualization Guide for detailed usage, examples, and implementation details.</p>"},{"location":"environments/offline/#choosing-the-right-environment","title":"Choosing the Right Environment","text":""},{"location":"environments/offline/#for-beginners","title":"For Beginners","text":"<p>\u2192 SeqLongOnlyEnv - Simple long-only trading - Easy to understand - Good for learning RL basics</p>"},{"location":"environments/offline/#for-risk-management-research","title":"For Risk Management Research","text":"<p>\u2192 SeqLongOnlySLTPEnv or SeqFuturesSLTPEnv - Stop-loss and take-profit orders - Study risk-reward trade-offs - Combinatorial action spaces</p>"},{"location":"environments/offline/#for-fast-iteration-grpo","title":"For Fast Iteration / GRPO","text":"<p>\u2192 LongOnlyOneStepEnv or FuturesOneStepEnv - One-step episodes - Fast training loops - Episodic rollouts</p>"},{"location":"environments/offline/#for-advanced-futures-trading","title":"For Advanced Futures Trading","text":"<p>\u2192 SeqFuturesEnv or SeqFuturesSLTPEnv - Leverage up to 125x - Margin management - Liquidation mechanics - Long and short positions</p>"},{"location":"environments/offline/#next-steps","title":"Next Steps","text":"<ul> <li>Online Environments - Live trading with exchange APIs</li> <li>Loss Functions - Training objectives (GRPOLoss for OneStepEnv, CTRL for representation learning)</li> <li>Transforms - Data preprocessing (ChronosEmbeddingTransform, CoverageTracker)</li> <li>Actors - Alternative policies (RuleBasedActor for baselines, LLMActor for LLM trading)</li> <li>Feature Engineering - Add technical indicators</li> <li>Reward Functions - Design better rewards</li> <li>Understanding the Sampler - How data sampling works</li> </ul>"},{"location":"environments/online/","title":"Online Environments","text":"<p>Online environments connect to real trading APIs for paper trading or live execution. They provide the same TorchTrade interface as offline environments but fetch real-time market data from exchanges.</p> <p>Supported Exchanges:</p> <ul> <li> <p>Alpaca - Commission-free US stocks and crypto with paper trading</p> </li> <li> <p>Binance - Cryptocurrency futures with high leverage and testnet</p> </li> <li> <p>Bitget - Cryptocurrency futures with competitive fees and testnet</p> </li> </ul>"},{"location":"environments/online/#overview","title":"Overview","text":"<p>TorchTrade provides 6 live trading environments across these exchanges:</p> Environment Exchange Asset Type Futures Leverage Bracket Orders AlpacaTorchTradingEnv Alpaca Crypto/Stocks \u274c \u274c \u274c AlpacaSLTPTorchTradingEnv Alpaca Crypto/Stocks \u274c \u274c \u2705 BinanceFuturesTorchTradingEnv Binance Crypto \u2705 \u2705 \u274c BinanceFuturesSLTPTorchTradingEnv Binance Crypto \u2705 \u2705 \u2705 BitgetFuturesTorchTradingEnv Bitget Crypto \u2705 \u2705 \u274c BitgetFuturesSLTPTorchTradingEnv Bitget Crypto \u2705 \u2705 \u2705"},{"location":"environments/online/#fractional-position-sizing","title":"Fractional Position Sizing","text":"<p>Non-SLTP live environments (<code>AlpacaTorchTradingEnv</code>, <code>BinanceFuturesTorchTradingEnv</code>, <code>BitgetFuturesTorchTradingEnv</code>) support fractional position sizing where action values represent the fraction of balance to allocate.</p>"},{"location":"environments/online/#how-it-works","title":"How It Works","text":"<p>Action Interpretation: - Action values range from -1.0 to 1.0 (futures) or 0.0 to 1.0 (long-only) - Magnitude = fraction of balance to allocate (0.5 = 50%, 1.0 = 100%) - Sign = direction (positive = long, negative = short) - Zero = market neutral (close all positions, stay in cash)</p> <p>Key Feature: When adjusting positions (e.g., 100% \u2192 50%), the environment only trades the delta (50%), reducing transaction fees.</p> <p>Examples:</p> <pre><code># Binance Futures - 5x leverage, $10k balance\nfrom torchtrade.envs.binance import BinanceFuturesTorchTradingEnv, BinanceFuturesTradingEnvConfig\n\nconfig = BinanceFuturesTradingEnvConfig(\n    symbol=\"BTCUSDT\",\n    leverage=5,  # Fixed global leverage\n    action_levels=[-1.0, -0.5, 0.0, 0.5, 1.0],  # Custom levels\n    demo=True  # Use testnet\n)\nenv = BinanceFuturesTorchTradingEnv(config, api_key=\"...\", api_secret=\"...\")\n\n# Action interpretation (similar to offline environments):\n# action = -1.0  \u2192 100% short: (balance \u00d7 1.0 \u00d7 5) / price\n# action = -0.5  \u2192 50% short\n# action =  0.0  \u2192 Market neutral (close all)\n# action =  0.5  \u2192 50% long\n# action =  1.0  \u2192 100% long\n</code></pre> <pre><code># Alpaca - Long-only, no leverage\nfrom torchtrade.envs.alpaca import AlpacaTorchTradingEnv, AlpacaTradingEnvConfig\n\nconfig = AlpacaTradingEnvConfig(\n    symbol=\"BTC/USD\",\n    action_levels=[0.0, 0.5, 1.0],  # Only long positions\n    paper=True\n)\nenv = AlpacaTorchTradingEnv(config, api_key=\"...\", api_secret=\"...\")\n\n# Action interpretation:\n# action = 0.0  \u2192 0% invested (all cash)\n# action = 0.5  \u2192 50% of portfolio invested\n# action = 1.0  \u2192 100% of portfolio invested\n</code></pre>"},{"location":"environments/online/#customizing-action-levels","title":"Customizing Action Levels","text":"<p>Same flexibility as offline environments - any list of values in [-1.0, 1.0]:</p> <pre><code># Fine-grained (9 actions)\naction_levels = [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0]\n\n# More precision near neutral (asymmetric)\naction_levels = [-1.0, -0.3, -0.1, 0.0, 0.1, 0.3, 1.0]\n\n# Conservative\naction_levels = [-0.5, -0.25, 0.0, 0.25, 0.5]\n\n# Long-only with fine control\naction_levels = [0.0, 0.25, 0.5, 0.75, 1.0]\n</code></pre> <p>Default Values: - Binance/Bitget Futures: <code>[-1.0, -0.5, 0.0, 0.5, 1.0]</code> - Alpaca (long-only): <code>[0.0, 0.5, 1.0]</code></p>"},{"location":"environments/online/#query-first-pattern","title":"Query-First Pattern","text":"<p>Live environments use a query-first pattern for reliability:</p> <ol> <li>Query actual position from exchange (source of truth)</li> <li>Calculate target position based on action and actual balance</li> <li>Round to exchange constraints (lot size, step size, min notional)</li> <li>Trade only the delta between current and target</li> <li>Use exchange close APIs for flat (action=0.0)</li> </ol> <p>This ensures positions stay synchronized with exchange state even if network issues occur.</p>"},{"location":"environments/online/#leverage-design-futures-environments","title":"Leverage Design (Futures Environments)","text":"<p>For futures environments (<code>BinanceFuturesTorchTradingEnv</code>, <code>BitgetFuturesTorchTradingEnv</code>), leverage is a fixed global parameter, not part of the action space.</p> <p>Design Philosophy: - Leverage = \"How much risk?\" (configuration/risk management) - Action = \"How much to deploy?\" (learned policy)</p> <p>Benefits: 1. Simpler learning: Smaller action space 2. Better risk control: Global leverage constraint 3. Matches trader workflows: Set leverage once, size positions with actions</p> <p>Custom Action Levels:</p> <p>You can customize action levels for finer position control:</p> <pre><code># Example: More granular position sizing\nconfig = BinanceFuturesTorchTradingEnvConfig(\n    action_levels=[-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n    leverage=10,\n)\n# Each action level controls the fraction of balance to allocate\n# Action values must be in range [-1.0, 1.0]\n# Leverage is set separately and applies uniformly to all positions\n</code></pre> <p>Important: Action levels must be in range <code>[-1.0, 1.0]</code>. Values outside this range will fail validation. The <code>leverage</code> parameter controls position sizing, not action values.</p> <p>Timeframe Format - Critical for Model Compatibility</p> <p>When specifying <code>time_frames</code>, always use canonical forms:</p> <ul> <li>\u2705 Correct: <code>[\"1Min\", \"5Min\", \"15Min\", \"1Hour\", \"1Day\"]</code> (Alpaca format)</li> <li>\u2705 Correct: <code>[\"1m\", \"5m\", \"15m\", \"1h\", \"1d\"]</code> (Binance/Bitget format)</li> <li>\u274c Wrong: <code>[\"60min\"]</code>, <code>[\"60m\"]</code>, <code>[\"24hour\"]</code>, <code>[\"24h\"]</code></li> </ul> <p>Why this matters:</p> <ul> <li><code>time_frames=[\"60min\"]</code> creates observation key <code>\"market_data_60Minute\"</code></li> <li><code>time_frames=[\"1hour\"]</code> creates observation key <code>\"market_data_1Hour\"</code></li> <li>These are DIFFERENT keys - your model trained with <code>\"60min\"</code> won't work with config using <code>\"1hour\"</code></li> </ul> <p>The framework will issue a warning if you use non-canonical forms. Use the suggested canonical forms to ensure model compatibility and cleaner observation keys.</p> <p>Common conversions:</p> <ul> <li><code>60min</code> / <code>60m</code> \u2192 use <code>1Hour</code> / <code>1h</code></li> <li><code>120min</code> / <code>120m</code> \u2192 use <code>2Hour</code> / <code>2h</code></li> <li><code>1440min</code> \u2192 use <code>1Day</code> / <code>1d</code></li> <li><code>24hour</code> / <code>24h</code> \u2192 use <code>1Day</code> / <code>1d</code></li> </ul>"},{"location":"environments/online/#alpaca-environments","title":"Alpaca Environments","text":"<p>Alpaca provides commission-free trading for US stocks and cryptocurrencies with paper trading support.</p>"},{"location":"environments/online/#alpacatorchtradingenv","title":"AlpacaTorchTradingEnv","text":"<p>Simple long-only spot trading with Alpaca API.</p>"},{"location":"environments/online/#setup","title":"Setup","text":"<pre><code># Get API keys from: https://alpaca.markets/signup\n# Create .env file\ncat &gt; .env &lt;&lt; EOF\nAPI_KEY=your_alpaca_api_key\nSECRET_KEY=your_alpaca_secret_key\nEOF\n</code></pre>"},{"location":"environments/online/#configuration","title":"Configuration","text":"<pre><code>from torchtrade.envs.alpaca import AlpacaTorchTradingEnv, AlpacaTradingEnvConfig\n\nconfig = AlpacaTradingEnvConfig(\n    symbol=\"BTC/USD\",                       # Trading symbol\n    time_frames=[\"1Min\", \"5Min\", \"15Min\", \"1Hour\"],  # 1min, 5min, 15min, 1hour\n    window_sizes=[12, 8, 8, 24],            # Lookback per timeframe\n    execute_on=\"5Min\",                      # Execute every 5 minutes\n    paper=True,                             # Paper trading (recommended!)\n    feature_preprocessing_fn=None,          # Optional custom features\n    reward_function=None,                   # Optional custom reward\n)\n\nenv = AlpacaTorchTradingEnv(config)\n</code></pre>"},{"location":"environments/online/#features","title":"Features","text":"<ul> <li>Paper trading: Safe testing without real money</li> <li>Commission-free: No transaction fees</li> <li>Real-time data: Live market data from Alpaca</li> <li>US markets: Stocks and crypto available</li> </ul>"},{"location":"environments/online/#example-usage","title":"Example Usage","text":"<pre><code>from torchtrade.envs.alpaca import AlpacaTorchTradingEnv, AlpacaTradingEnvConfig\nimport torch\n\n# Configure environment\nconfig = AlpacaTradingEnvConfig(\n    symbol=\"BTC/USD\",\n    time_frames=[\"1Min\", \"5Min\", \"15Min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=\"5Min\",\n    paper=True  # Always start with paper trading!\n)\n\n# Create environment\nenv = AlpacaTorchTradingEnv(config)\n\n# Run episode\ntensordict = env.reset()\n\nfor step in range(100):\n    # Your policy selects action\n    action = policy(tensordict)\n    tensordict[\"action\"] = action\n\n    # Execute trade\n    tensordict = env.step(tensordict)\n\n    print(f\"Step {step}: Reward = {tensordict['reward'].item():.4f}\")\n</code></pre>"},{"location":"environments/online/#alpacasltptorchtradingenv","title":"AlpacaSLTPTorchTradingEnv","text":"<p>Alpaca environment with stop-loss/take-profit bracket orders.</p>"},{"location":"environments/online/#configuration_1","title":"Configuration","text":"<pre><code>from torchtrade.envs.alpaca import AlpacaSLTPTorchTradingEnv, AlpacaSLTPTradingEnvConfig\n\nconfig = AlpacaSLTPTradingEnvConfig(\n    symbol=\"BTC/USD\",\n    time_frames=[\"1Min\", \"5Min\", \"15Min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=\"5Min\",\n\n    # Bracket order configuration\n    stoploss_levels=[-0.02, -0.05],         # -2%, -5%\n    takeprofit_levels=[0.05, 0.10],         # +5%, +10%\n\n    paper=True,\n)\n\nenv = AlpacaSLTPTorchTradingEnv(config)\n</code></pre>"},{"location":"environments/online/#action-space","title":"Action Space","text":"<p>With <code>stoploss_levels=[-0.02, -0.05]</code> and <code>takeprofit_levels=[0.05, 0.10]</code>:</p> <ul> <li>Action 0: HOLD / Close position</li> <li>Action 1: BUY with SL=-2%, TP=+5%</li> <li>Action 2: BUY with SL=-2%, TP=+10%</li> <li>Action 3: BUY with SL=-5%, TP=+5%</li> <li>Action 4: BUY with SL=-5%, TP=+10%</li> </ul> <p>Total: 5 actions</p>"},{"location":"environments/online/#binance-environments","title":"Binance Environments","text":"<p>Binance is a leading cryptocurrency exchange with futures trading and testnet support.</p>"},{"location":"environments/online/#binancefuturestorchtradingenv","title":"BinanceFuturesTorchTradingEnv","text":"<p>Futures trading environment with leverage support.</p>"},{"location":"environments/online/#setup_1","title":"Setup","text":"<pre><code># Get API keys from: https://www.binance.com/en/my/settings/api-management\n# Create .env file\ncat &gt; .env &lt;&lt; EOF\nBINANCE_API_KEY=your_binance_api_key\nBINANCE_SECRET_KEY=your_binance_secret_key\nEOF\n</code></pre>"},{"location":"environments/online/#configuration_2","title":"Configuration","text":"<pre><code>from torchtrade.envs.binance import (\n    BinanceFuturesTorchTradingEnv,\n    BinanceFuturesTradingEnvConfig\n)\n\nconfig = BinanceFuturesTradingEnvConfig(\n    symbol=\"BTCUSDT\",                       # Trading pair\n    intervals=[\"1m\", \"5m\", \"15m\"],          # Timeframes\n    window_sizes=[12, 8, 8],                # Lookback per timeframe\n    execute_on=\"1m\",                        # Execute every 1 minute\n\n    # Futures parameters\n    leverage=5,                             # 5x leverage\n    quantity_per_trade=0.01,                # Position size (BTC)\n\n    # Trading mode\n    demo=True,                              # Use testnet (recommended!)\n\n    # Optional\n    feature_preprocessing_fn=None,\n    reward_function=None,\n)\n\nenv = BinanceFuturesTorchTradingEnv(config)\n</code></pre>"},{"location":"environments/online/#features_1","title":"Features","text":"<ul> <li>Leverage: Up to 125x leverage</li> <li>Testnet: Safe testing with fake funds</li> <li>Real-time data: Live market data from Binance</li> <li>Low fees: Competitive maker/taker fees</li> <li>Long and short: Both directions supported</li> </ul>"},{"location":"environments/online/#example-usage_1","title":"Example Usage","text":"<pre><code>from torchtrade.envs.binance import (\n    BinanceFuturesTorchTradingEnv,\n    BinanceFuturesTradingEnvConfig\n)\nimport os\n\nconfig = BinanceFuturesTradingEnvConfig(\n    symbol=\"BTCUSDT\",\n    intervals=[\"1m\", \"5m\"],\n    window_sizes=[12, 8],\n    execute_on=\"1m\",\n    leverage=5,\n    quantity_per_trade=0.01,\n    demo=True,  # Testnet\n)\n\nenv = BinanceFuturesTorchTradingEnv(config)\n\n# Run trading loop\ntensordict = env.reset()\n\nfor step in range(100):\n    action = policy(tensordict)\n    tensordict[\"action\"] = action\n    tensordict = env.step(tensordict)\n\n    # Check margin ratio\n    account = tensordict[\"account_state\"]\n    margin_ratio = account[7].item()\n\n    if margin_ratio &lt; 0.3:\n        print(f\"\u26a0\ufe0f Low margin ratio: {margin_ratio:.2f}\")\n</code></pre>"},{"location":"environments/online/#binancefuturessltptorchtradingenv","title":"BinanceFuturesSLTPTorchTradingEnv","text":"<p>Binance futures with stop-loss/take-profit bracket orders.</p>"},{"location":"environments/online/#configuration_3","title":"Configuration","text":"<pre><code>from torchtrade.envs.binance import (\n    BinanceFuturesSLTPTorchTradingEnv,\n    BinanceFuturesSLTPTradingEnvConfig\n)\n\nconfig = BinanceFuturesSLTPTradingEnvConfig(\n    symbol=\"BTCUSDT\",\n    intervals=[\"1m\", \"5m\"],\n    window_sizes=[12, 8],\n    execute_on=\"1m\",\n\n    # Bracket orders\n    stoploss_levels=[-0.02, -0.05],         # -2%, -5%\n    takeprofit_levels=[0.03, 0.06, 0.10],   # +3%, +6%, +10%\n    include_short_positions=True,           # Enable short bracket orders\n\n    # Futures parameters\n    leverage=5,\n    quantity_per_trade=0.01,\n\n    demo=True,\n)\n\nenv = BinanceFuturesSLTPTorchTradingEnv(config)\n</code></pre>"},{"location":"environments/online/#action-space_1","title":"Action Space","text":"<p>With <code>stoploss_levels=[-0.02, -0.05]</code>, <code>takeprofit_levels=[0.03, 0.06, 0.10]</code>, and <code>include_short_positions=True</code>:</p> <ul> <li>Action 0: HOLD / Close position</li> <li>Actions 1-6: LONG with SL/TP combinations (2 \u00d7 3)</li> <li>Actions 7-12: SHORT with SL/TP combinations (2 \u00d7 3)</li> </ul> <p>Total: 1 + 2 \u00d7 (2 \u00d7 3) = 13 actions</p>"},{"location":"environments/online/#bitget-environments","title":"Bitget Environments","text":"<p>Bitget is a fast-growing cryptocurrency exchange with competitive fees and testnet support. TorchTrade uses CCXT library to interface with Bitget's V2 API.</p> <p>CCXT Symbol Format</p> <p>Bitget environments use CCXT's perpetual swap format: <code>\"BTC/USDT:USDT\"</code> (not <code>\"BTCUSDT\"</code>). Format: <code>{BASE}/{QUOTE}:{SETTLE}</code> where BASE=BTC, QUOTE=USDT, SETTLE=USDT.</p>"},{"location":"environments/online/#bitgetfuturestorchtradingenv","title":"BitgetFuturesTorchTradingEnv","text":"<p>Futures trading environment for Bitget exchange with configurable margin and position modes.</p>"},{"location":"environments/online/#setup_2","title":"Setup","text":"<pre><code># Get API keys from: https://www.bitget.com/en/support/articles/360038859731\n# Or use demo trading: https://www.bitget.com/demo-trading\n# Create .env file\ncat &gt; .env &lt;&lt; EOF\nBITGETACCESSAPIKEY=your_bitget_api_key\nBITGETSECRETKEY=your_bitget_secret_key\nBITGETPASSPHRASE=your_bitget_passphrase\nEOF\n</code></pre>"},{"location":"environments/online/#configuration_4","title":"Configuration","text":"<pre><code>from torchtrade.envs.bitget import (\n    BitgetFuturesTorchTradingEnv,\n    BitgetFuturesTradingEnvConfig\n)\nfrom torchtrade.envs.bitget.futures_order_executor import (\n    MarginMode,\n    PositionMode,\n)\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nconfig = BitgetFuturesTradingEnvConfig(\n    symbol=\"BTC/USDT:USDT\",                 # CCXT perpetual swap format\n    time_frames=[\"5min\", \"15min\"],          # Timeframes for observations\n    window_sizes=[6, 32],                   # Lookback per timeframe\n    execute_on=\"1min\",                      # Execute every 1 minute\n\n    # Futures parameters\n    product_type=\"USDT-FUTURES\",            # V2 API: USDT-FUTURES, COIN-FUTURES\n    leverage=5,                             # Leverage (1-125)\n    quantity_per_trade=0.002,               # Position size\n    margin_mode=MarginMode.ISOLATED,        # ISOLATED or CROSSED\n    position_mode=PositionMode.ONE_WAY,     # ONE_WAY (recommended) or HEDGE\n\n    # Trading mode\n    demo=True,                              # Testnet (recommended!)\n\n    # Optional\n    feature_preprocessing_fn=None,\n    reward_function=None,\n)\n\nenv = BitgetFuturesTorchTradingEnv(\n    config,\n    api_key=os.getenv(\"BITGETACCESSAPIKEY\"),\n    api_secret=os.getenv(\"BITGETSECRETKEY\"),\n    api_passphrase=os.getenv(\"BITGETPASSPHRASE\"),\n)\n</code></pre>"},{"location":"environments/online/#margin-modes","title":"Margin Modes","text":"<p>ISOLATED (Recommended): - Each position has separate margin allocation - If liquidated, only that position's margin is lost - Lower risk, better for beginners</p> <p>CROSSED: - All positions share the entire account balance - Higher risk but more capital efficient - Advanced users only</p> <pre><code>from torchtrade.envs.bitget.futures_order_executor import MarginMode\n\nconfig = BitgetFuturesTradingEnvConfig(\n    margin_mode=MarginMode.ISOLATED,  # Safer default\n    # margin_mode=MarginMode.CROSSED,  # Advanced\n)\n</code></pre> <p>Margin Mode Implementation</p> <p>Bitget's <code>set_margin_mode()</code> API doesn't work reliably. TorchTrade sets margin mode per-order by including <code>marginMode</code> in each order's parameters. This is the recommended approach per CCXT issue #21435. Each order will use your configured margin mode.</p>"},{"location":"environments/online/#position-modes","title":"Position Modes","text":"<p>ONE_WAY (Recommended): - Single net position per symbol - Going LONG when SHORT automatically closes the short first - Simpler position management</p> <p>HEDGE: - Can hold separate long and short positions simultaneously - More complex, for advanced hedging strategies</p> <pre><code>from torchtrade.envs.bitget.futures_order_executor import PositionMode\n\nconfig = BitgetFuturesTradingEnvConfig(\n    position_mode=PositionMode.ONE_WAY,  # Recommended\n    # position_mode=PositionMode.HEDGE,   # Advanced\n)\n</code></pre>"},{"location":"environments/online/#features_2","title":"Features","text":"<ul> <li>CCXT Integration: Uses CCXT library with Bitget V2 API</li> <li>Leverage: Up to 125x leverage</li> <li>Testnet: Safe testing environment (Demo Trading)</li> <li>Competitive fees: Maker 0.02% / Taker 0.06%</li> <li>Flexible Risk Management: Configurable margin and position modes</li> <li>Alternative to Binance: Growing liquidity</li> </ul>"},{"location":"environments/online/#example-usage_2","title":"Example Usage","text":"<pre><code>from torchtrade.envs.bitget import (\n    BitgetFuturesTorchTradingEnv,\n    BitgetFuturesTradingEnvConfig\n)\nfrom torchtrade.envs.bitget.futures_order_executor import (\n    MarginMode,\n    PositionMode,\n)\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nconfig = BitgetFuturesTradingEnvConfig(\n    symbol=\"BTC/USDT:USDT\",                 # CCXT format!\n    time_frames=[\"5min\", \"15min\"],\n    window_sizes=[6, 32],\n    execute_on=\"1min\",\n    leverage=5,\n    quantity_per_trade=0.002,\n    margin_mode=MarginMode.ISOLATED,        # Safer for testing\n    position_mode=PositionMode.ONE_WAY,     # Simpler management\n    demo=True,\n)\n\nenv = BitgetFuturesTorchTradingEnv(\n    config,\n    api_key=os.getenv(\"BITGETACCESSAPIKEY\"),\n    api_secret=os.getenv(\"BITGETSECRETKEY\"),\n    api_passphrase=os.getenv(\"BITGETPASSPHRASE\"),\n)\n\n# Trading loop\ntensordict = env.reset()\n\nfor step in range(100):\n    action = policy(tensordict)\n    tensordict[\"action\"] = action\n    tensordict = env.step(tensordict)\n\n    # Monitor account state (10 elements for Bitget futures)\n    account = tensordict[\"account_state\"]\n    # [cash, position_size, position_value, entry_price, current_price,\n    #  unrealized_pnl_pct, leverage, margin_ratio, liquidation_price, holding_time]\n</code></pre>"},{"location":"environments/online/#bitgetfuturessltptorchtradingenv","title":"BitgetFuturesSLTPTorchTradingEnv","text":"<p>Bitget futures with stop-loss/take-profit bracket orders.</p>"},{"location":"environments/online/#configuration_5","title":"Configuration","text":"<pre><code>from torchtrade.envs.bitget import (\n    BitgetFuturesSLTPTorchTradingEnv,\n    BitgetFuturesSLTPTradingEnvConfig\n)\nfrom torchtrade.envs.bitget.futures_order_executor import (\n    MarginMode,\n    PositionMode,\n)\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nconfig = BitgetFuturesSLTPTradingEnvConfig(\n    symbol=\"BTC/USDT:USDT\",                 # CCXT perpetual swap format\n    time_frames=[\"5min\", \"15min\"],\n    window_sizes=[6, 32],\n    execute_on=\"1min\",\n\n    # Bracket orders\n    stoploss_levels=(-0.025, -0.05, -0.1),  # -2.5%, -5%, -10%\n    takeprofit_levels=(0.05, 0.1, 0.2),     # 5%, 10%, 20%\n    include_short_positions=True,           # Enable short bracket orders\n    include_hold_action=True,               # Include HOLD action\n\n    # Futures parameters\n    product_type=\"USDT-FUTURES\",\n    leverage=5,\n    quantity_per_trade=0.002,\n    margin_mode=MarginMode.ISOLATED,\n    position_mode=PositionMode.ONE_WAY,\n\n    demo=True,\n)\n\nenv = BitgetFuturesSLTPTorchTradingEnv(\n    config,\n    api_key=os.getenv(\"BITGETACCESSAPIKEY\"),\n    api_secret=os.getenv(\"BITGETSECRETKEY\"),\n    api_passphrase=os.getenv(\"BITGETPASSPHRASE\"),\n)\n</code></pre>"},{"location":"environments/online/#action-space_2","title":"Action Space","text":"<p>With the configuration above (3 SL \u00d7 3 TP, both long/short):</p> <ul> <li>Action 0: HOLD / Close position</li> <li>Actions 1-9: LONG with SL/TP combinations (3 \u00d7 3)</li> <li>Actions 10-18: SHORT with SL/TP combinations (3 \u00d7 3)</li> </ul> <p>Total: 1 + 2 \u00d7 (3 \u00d7 3) = 19 actions</p> <p>For detailed documentation and examples, see: - Bitget Examples README</p>"},{"location":"environments/online/#safety-and-best-practices","title":"Safety and Best Practices","text":""},{"location":"environments/online/#always-start-with-papertestnet-trading","title":"Always Start with Paper/Testnet Trading","text":"<pre><code># Alpaca\nconfig = AlpacaTradingEnvConfig(paper=True)  # \u2705 Paper trading\n\n# Binance\nconfig = BinanceFuturesTradingEnvConfig(demo=True)  # \u2705 Testnet\n\n# Bitget\nconfig = BitgetFuturesTradingEnvConfig(demo=True)  # \u2705 Testnet\n</code></pre>"},{"location":"environments/online/#use-low-leverage-initially","title":"Use Low Leverage Initially","text":"<pre><code>config = BinanceFuturesTradingEnvConfig(\n    leverage=2,  # Start with 2x, not 125x!\n    ...\n)\n</code></pre>"},{"location":"environments/online/#monitor-margin-ratio-futures","title":"Monitor Margin Ratio (Futures)","text":"<pre><code>while not done:\n    tensordict = env.step(tensordict)\n    account = tensordict[\"account_state\"]\n    margin_ratio = account[7].item()\n\n    if margin_ratio &lt; 0.3:\n        print(f\"\u26a0\ufe0f WARNING: Low margin ratio {margin_ratio:.2f}\")\n        # Consider reducing position or closing\n</code></pre>"},{"location":"environments/online/#handle-api-rate-limits","title":"Handle API Rate Limits","text":"<pre><code>import time\n\nfor step in range(1000):\n    tensordict = env.step(tensordict)\n\n    # Add small delay to avoid rate limits\n    if step % 10 == 0:\n        time.sleep(0.5)\n</code></pre>"},{"location":"environments/online/#use-environment-variables-for-api-keys","title":"Use Environment Variables for API Keys","text":"<p>Never hardcode API keys! Always use environment variables:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()  # Load from .env file\n\napi_key = os.getenv(\"BINANCE_API_KEY\")\napi_secret = os.getenv(\"BINANCE_SECRET_KEY\")\n</code></pre>"},{"location":"environments/online/#deployment-workflow","title":"Deployment Workflow","text":""},{"location":"environments/online/#1-train-offline","title":"1. Train Offline","text":"<p>First, train and validate your policy on historical data:</p> <pre><code>from torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\n\n# Train on historical data\noffline_config = SeqLongOnlyEnvConfig(...)\noffline_env = SeqLongOnlyEnv(df, offline_config)\n\n# Train policy with PPO/IQL/GRPO\ntrain_policy(offline_env, policy)\n</code></pre>"},{"location":"environments/online/#2-test-on-paper-trading","title":"2. Test on Paper Trading","text":"<p>Deploy to paper trading to validate in live conditions:</p> <pre><code>from torchtrade.envs.alpaca import AlpacaTorchTradingEnv, AlpacaTradingEnvConfig\n\n# Deploy to paper trading\nonline_config = AlpacaTradingEnvConfig(paper=True, ...)\nonline_env = AlpacaTorchTradingEnv(online_config)\n\n# Run policy in paper trading mode\nevaluate_policy(online_env, policy)\n</code></pre>"},{"location":"environments/online/#3-monitor-performance","title":"3. Monitor Performance","text":"<p>Track key metrics during paper trading: - Sharpe ratio: Risk-adjusted returns - Max drawdown: Worst peak-to-trough decline - Win rate: Percentage of profitable trades - Average trade duration: How long positions are held</p>"},{"location":"environments/online/#4-go-live-carefully","title":"4. Go Live (Carefully!)","text":"<p>Only after extensive paper trading validation:</p> <pre><code># Switch to live trading\nonline_config = AlpacaTradingEnvConfig(\n    paper=False,  # Live trading!\n    ...\n)\n\n# Start with small position sizes\nonline_config.quantity_per_trade = 0.001  # Very small\n</code></pre>"},{"location":"environments/online/#exchange-comparison","title":"Exchange Comparison","text":"Feature Alpaca Binance Bitget Asset Types Stocks, Crypto Crypto Crypto Futures \u274c \u2705 \u2705 Max Leverage 1x 125x 125x Paper Trading \u2705 \u2705 (Testnet) \u2705 (Testnet) Commission Free 0.02%/0.04% 0.02%/0.06% Best For US markets, stocks High leverage, low fees Binance alternative"},{"location":"environments/online/#requesting-new-exchanges","title":"Requesting New Exchanges","text":"<p>Need support for another exchange? We're open to adding more integrations!</p> <p>Supported exchanges we're considering: - OKX - Bybit - Kraken - Interactive Brokers - OANDA (Forex/CFDs)</p> <p>To request a new exchange: 1. Create an issue with the exchange name 2. Email us at: torchtradecontact@gmail.com</p>"},{"location":"environments/online/#next-steps","title":"Next Steps","text":"<ul> <li>Offline Environments - Train on historical data first</li> <li>Actors - Alternative policies (RuleBasedActor, LLMActor for live trading)</li> <li>Transforms - Data preprocessing and monitoring</li> <li>Feature Engineering - Add technical indicators</li> <li>Reward Functions - Design better rewards</li> <li>Building Custom Environments - Extend TorchTrade</li> </ul>"},{"location":"environments/online/#support","title":"Support","text":"<ul> <li>\ud83d\udcac Questions: GitHub Discussions</li> <li>\ud83d\udc1b Bug Reports: GitHub Issues</li> <li>\ud83d\udce7 Email: torchtradecontact@gmail.com</li> </ul>"},{"location":"environments/visualization/","title":"Visualizing Episode History","text":"<p>All offline environments support <code>render_history()</code> to visualize episode performance after training or evaluation. The method automatically detects the environment type and renders appropriate plots.</p>"},{"location":"environments/visualization/#overview","title":"Overview","text":"<p>The <code>render_history()</code> method is inherited from the <code>TorchTradeOfflineEnv</code> base class and provides consistent visualization across all 6 offline environments:</p> <ul> <li>SeqLongOnlyEnv</li> <li>SeqLongOnlySLTPEnv</li> <li>LongOnlyOneStepEnv</li> <li>SeqFuturesEnv</li> <li>SeqFuturesSLTPEnv</li> <li>FuturesOneStepEnv</li> </ul> <p>The visualization automatically adapts based on the environment type (long-only vs futures).</p>"},{"location":"environments/visualization/#usage","title":"Usage","text":"<pre><code># After running an episode\nenv.reset()\nfor _ in range(episode_length):\n    action = policy(obs)\n    obs, reward, done, truncated, info = env.step(action)\n    if done or truncated:\n        break\n\n# Visualize the episode\nenv.render_history()  # Display plots with buy-and-hold baseline\n\n# Without buy-and-hold baseline\nenv.render_history(plot_bh_baseline=False)\n\n# Or save to a variable for later\nfig = env.render_history(return_fig=True, plot_bh_baseline=True)\nfig.savefig(\"episode_history.png\")\n</code></pre>"},{"location":"environments/visualization/#visualization-types","title":"Visualization Types","text":"<p>The rendered plots automatically adapt based on environment type:</p>"},{"location":"environments/visualization/#long-only-environments","title":"Long-Only Environments","text":"<p><code>SeqLongOnlyEnv</code>, <code>SeqLongOnlySLTPEnv</code>, <code>LongOnlyOneStepEnv</code></p> <p>2 subplots:</p> <ol> <li>Price History with Actions: Shows the asset price over time with buy/sell actions marked as green upward triangles (buy) and red downward triangles (sell)</li> <li>Portfolio Value vs Buy-and-Hold: Compares your agent's portfolio value against a simple buy-and-hold baseline strategy</li> </ol>"},{"location":"environments/visualization/#futures-environments","title":"Futures Environments","text":"<p><code>SeqFuturesEnv</code>, <code>SeqFuturesSLTPEnv</code>, <code>FuturesOneStepEnv</code></p> <p>3 subplots:</p> <ol> <li>Price History with Actions: Shows the asset price over time with long/short actions marked as green upward triangles (long) and red downward triangles (short)</li> <li>Portfolio Value vs Buy-and-Hold: Compares your agent's portfolio value against a simple buy-and-hold baseline strategy</li> <li>Position History: Visualizes position size over time with green areas for long positions, red areas for short positions, and flat sections for no position</li> </ol>"},{"location":"environments/visualization/#example-output","title":"Example Output","text":"<p>Example plots will be added here showing typical visualization output for long-only and futures environments.</p>"},{"location":"environments/visualization/#long-only-environment-example","title":"Long-Only Environment Example","text":""},{"location":"environments/visualization/#futures-environment-example","title":"Futures Environment Example","text":""},{"location":"environments/visualization/#implementation-details","title":"Implementation Details","text":"<ul> <li>Automatic detection: Checks for <code>positions</code> key in history to determine environment type</li> <li>Matplotlib-based: Requires <code>matplotlib</code> to be installed</li> <li>Buy-and-hold baseline: Calculated from initial portfolio value (buys asset at t=0 and holds). Can be disabled with <code>plot_bh_baseline=False</code></li> <li>Action markers: All actions are marked on the price chart for easy analysis</li> <li>Consistent API: Same method works across all 6 offline environments</li> </ul>"},{"location":"environments/visualization/#parameters","title":"Parameters","text":"<ul> <li><code>return_fig</code> (bool, default=False): If True, returns the matplotlib figure instead of displaying it</li> <li><code>plot_bh_baseline</code> (bool, default=True): If True, plots the buy-and-hold baseline for comparison. Set to False to show only portfolio value without the baseline</li> </ul>"},{"location":"environments/visualization/#example-with-different-environments","title":"Example with Different Environments","text":"<pre><code>import pandas as pd\nfrom torchtrade.envs.offline import (\n    SeqLongOnlyEnv,\n    SeqLongOnlyEnvConfig,\n    SeqFuturesEnv,\n    SeqFuturesEnvConfig\n)\n\n# Long-only environment\nconfig = SeqLongOnlyEnvConfig(...)\nenv = SeqLongOnlyEnv(df, config)\n# ... run episode ...\nenv.render_history()  # Shows 2 subplots\n\n# Futures environment\nconfig = SeqFuturesEnvConfig(...)\nenv = SeqFuturesEnv(df, config)\n# ... run episode ...\nenv.render_history()  # Shows 3 subplots\n\n# One-step environments work too!\nfrom torchtrade.envs.offline import LongOnlyOneStepEnv, FuturesOneStepEnv\n\nenv = LongOnlyOneStepEnv(df, config)\n# ... run episode ...\nenv.render_history()  # Shows 2 subplots\n\nenv = FuturesOneStepEnv(df, config)\n# ... run episode ...\nenv.render_history()  # Shows 3 subplots\n</code></pre>"},{"location":"environments/visualization/#technical-notes","title":"Technical Notes","text":""},{"location":"environments/visualization/#base-class-implementation","title":"Base Class Implementation","text":"<p>The method is implemented in <code>TorchTradeOfflineEnv</code> base class at <code>torchtrade/envs/offline/base.py:457-614</code>.</p>"},{"location":"environments/visualization/#detection-logic","title":"Detection Logic","text":"<pre><code>def render_history(self, return_fig=False, plot_bh_baseline=True):\n    history_dict = self.history.to_dict()\n\n    # Automatically detect environment type\n    is_futures = 'positions' in history_dict\n\n    if is_futures:\n        # Render 3-subplot layout (price, portfolio, position)\n    else:\n        # Render 2-subplot layout (price, portfolio)\n</code></pre>"},{"location":"environments/visualization/#dependencies","title":"Dependencies","text":"<ul> <li><code>matplotlib.pyplot</code> - For plotting</li> <li><code>datetime</code> - For timestamp formatting (if using datetime indices)</li> </ul>"},{"location":"environments/visualization/#history-tracking","title":"History Tracking","text":"<p>All offline environments use <code>HistoryTracker</code> (or <code>FuturesHistoryTracker</code> for futures environments) to record:</p> <ul> <li>Price at each step</li> <li>Action taken</li> <li>Reward received</li> <li>Portfolio value</li> <li>Position size (futures only)</li> </ul> <p>See <code>torchtrade/envs/state.py:33-148</code> for history tracking implementation details.</p>"},{"location":"environments/visualization/#next-steps","title":"Next Steps","text":"<ul> <li>Offline Environments - Learn about the 6 offline environments</li> <li>History Tracking - Understanding episode history data</li> <li>Metrics - Computing performance metrics from history</li> </ul>"},{"location":"guides/custom-environment/","title":"Building Custom Environments","text":"<p>This guide shows how to extend TorchTrade for custom trading environments.</p>"},{"location":"guides/custom-environment/#when-to-build-a-custom-environment","title":"When to Build a Custom Environment","text":"<p>Build a custom environment when you need:</p> <ul> <li>Custom asset types: Options, forex, commodities</li> <li>Complex order types: Market-on-close, iceberg orders</li> <li>Custom state: Order book data, sentiment, news</li> <li>Specific trading rules: Pattern day trading, portfolio constraints</li> <li>New exchange integrations: Unsupported brokers/APIs</li> </ul>"},{"location":"guides/custom-environment/#environment-architecture","title":"Environment Architecture","text":"<p>TorchTrade environments inherit from TorchRL's <code>EnvBase</code>:</p> <pre><code>EnvBase (TorchRL)\n    \u2193\nBaseTorchTradeEnv (Abstract base - optional)\n    \u2193\nYourCustomEnv\n</code></pre>"},{"location":"guides/custom-environment/#required-methods","title":"Required Methods","text":"Method Purpose Returns <code>_reset()</code> Initialize episode state TensorDict with initial observation <code>_step(tensordict)</code> Execute action, update state TensorDict with next observation <code>_set_seed(seed)</code> Set random seed for reproducibility None <code>observation_spec</code> Define observation space CompositeSpec <code>action_spec</code> Define action space DiscreteTensorSpec or ContinuousTensorSpec <code>reward_spec</code> Define reward space UnboundedContinuousTensorSpec"},{"location":"guides/custom-environment/#example-1-simple-custom-environment","title":"Example 1: Simple Custom Environment","text":"<p>Minimal environment from scratch:</p> <pre><code>from torchrl.envs import EnvBase\nfrom torchrl.data import CompositeSpec, UnboundedContinuousTensorSpec, DiscreteTensorSpec\nfrom tensordict import TensorDict\nimport torch\n\nclass SimpleCustomEnv(EnvBase):\n    \"\"\"\n    Minimal custom trading environment.\n\n    State: [price, position]\n    Actions: 0 (HOLD), 1 (BUY), 2 (SELL)\n    Reward: Log return\n    \"\"\"\n\n    def __init__(self, prices: torch.Tensor, **kwargs):\n        super().__init__(**kwargs)\n        self.prices = prices\n        self.current_step = 0\n        self.position = 0  # 0 or 1\n        self.entry_price = 0.0\n\n        # Define specs\n        self._observation_spec = CompositeSpec({\n            \"price\": UnboundedContinuousTensorSpec(shape=(1,)),\n            \"position\": UnboundedContinuousTensorSpec(shape=(1,)),\n        })\n\n        self._action_spec = DiscreteTensorSpec(n=3)\n        self._reward_spec = UnboundedContinuousTensorSpec(shape=(1,))\n\n    def _reset(self, tensordict=None, **kwargs):\n        \"\"\"Reset to initial state\"\"\"\n        self.current_step = 0\n        self.position = 0\n        self.entry_price = 0.0\n\n        return TensorDict({\n            \"price\": torch.tensor([self.prices[0].item()]),\n            \"position\": torch.tensor([0.0]),\n        }, batch_size=self.batch_size)\n\n    def _step(self, tensordict: TensorDict) -&gt; TensorDict:\n        \"\"\"Execute one step\"\"\"\n        action = tensordict[\"action\"].item()\n        current_price = self.prices[self.current_step].item()\n\n        # Execute action\n        reward = 0.0\n        if action == 1 and self.position == 0:  # BUY\n            self.position = 1\n            self.entry_price = current_price\n        elif action == 2 and self.position == 1:  # SELL\n            reward = (current_price - self.entry_price) / self.entry_price\n            self.position = 0\n            self.entry_price = 0.0\n\n        # Move to next step\n        self.current_step += 1\n        done = self.current_step &gt;= len(self.prices) - 1\n\n        # Build output tensordict\n        next_price = self.prices[self.current_step].item() if not done else current_price\n\n        return TensorDict({\n            \"price\": torch.tensor([next_price]),\n            \"position\": torch.tensor([float(self.position)]),\n            \"reward\": torch.tensor([reward]),\n            \"done\": torch.tensor([done]),\n        }, batch_size=self.batch_size)\n\n    def _set_seed(self, seed: int):\n        \"\"\"Set random seed\"\"\"\n        torch.manual_seed(seed)\n\n# Usage\nprices = torch.randn(1000).cumsum(0) + 100  # Random walk prices\nenv = SimpleCustomEnv(prices, batch_size=[])\n\nobs = env.reset()\nfor _ in range(100):\n    action = env.action_spec.rand()  # Random action\n    obs = env.step(action)\n    if obs[\"done\"]:\n        break\n</code></pre>"},{"location":"guides/custom-environment/#example-2-extending-existing-environments","title":"Example 2: Extending Existing Environments","text":"<p>Extend <code>SeqLongOnlyEnv</code> to add custom features:</p> <pre><code>from torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\nfrom tensordict import TensorDict\nimport torch\n\nclass CustomLongOnlyEnv(SeqLongOnlyEnv):\n    \"\"\"\n    Extended SeqLongOnlyEnv with sentiment data.\n    \"\"\"\n\n    def __init__(self, df, config: SeqLongOnlyEnvConfig, sentiment_data: torch.Tensor):\n        super().__init__(df, config)\n        self.sentiment_data = sentiment_data  # Timeseries sentiment scores\n\n        # Extend observation spec\n        from torchrl.data import UnboundedContinuousTensorSpec\n        self._observation_spec[\"sentiment\"] = UnboundedContinuousTensorSpec(shape=(1,))\n\n    def _reset(self, tensordict=None, **kwargs):\n        \"\"\"Add sentiment to observations\"\"\"\n        obs = super()._reset(tensordict, **kwargs)\n\n        # Add current sentiment\n        sentiment_idx = self.sampler.reset_index\n        obs[\"sentiment\"] = torch.tensor([self.sentiment_data[sentiment_idx].item()])\n\n        return obs\n\n    def _step(self, tensordict: TensorDict) -&gt; TensorDict:\n        \"\"\"Add sentiment to step observations\"\"\"\n        obs = super()._step(tensordict)\n\n        # Add current sentiment\n        sentiment_idx = self.sampler.current_index\n        obs[\"sentiment\"] = torch.tensor([self.sentiment_data[sentiment_idx].item()])\n\n        return obs\n\n# Usage\nimport pandas as pd\n\ndf = pd.read_csv(\"prices.csv\")\nsentiment = torch.randn(len(df))  # Random sentiment scores\n\nconfig = SeqLongOnlyEnvConfig(\n    time_frames=[\"1min\", \"5min\"],\n    window_sizes=[12, 8],\n    execute_on=(5, \"Minute\"),\n)\n\nenv = CustomLongOnlyEnv(df, config, sentiment)\n\n# Policy network sees sentiment in observations\nobs = env.reset()\nprint(obs.keys())  # [..., 'sentiment']\n</code></pre>"},{"location":"guides/custom-environment/#design-patterns","title":"Design Patterns","text":""},{"location":"guides/custom-environment/#1-composition-over-inheritance","title":"1. Composition Over Inheritance","text":"<p>Prefer composing existing components:</p> <pre><code>class CustomEnv(SeqLongOnlyEnv):\n    def __init__(self, df, config, custom_component):\n        super().__init__(df, config)\n        self.custom_component = custom_component  # Inject custom logic\n\n    def _step(self, tensordict):\n        obs = super()._step(tensordict)\n        # Modify obs with custom_component\n        obs[\"custom_feature\"] = self.custom_component.compute(obs)\n        return obs\n</code></pre>"},{"location":"guides/custom-environment/#2-observation-spec-extension","title":"2. Observation Spec Extension","text":"<p>Always update observation specs when adding new fields:</p> <pre><code># In __init__\nself._observation_spec[\"new_field\"] = UnboundedContinuousTensorSpec(shape=(N,))\n</code></pre>"},{"location":"guides/custom-environment/#3-state-management","title":"3. State Management","text":"<p>TorchTrade provides structured state management classes for consistent state handling across all environments. See State Management for full details.</p> <p>Quick Reference - PositionState:</p> <p>The <code>PositionState</code> dataclass encapsulates position-related state in a structured way:</p> <pre><code>from torchtrade.envs.state import PositionState\n\nclass CustomEnv(EnvBase):\n    def __init__(self):\n        super().__init__()\n        # Use PositionState to group position-related variables\n        self.position = PositionState()\n        # Provides: position.current_position, position.position_size,\n        #          position.position_value, position.entry_price,\n        #          position.unrealized_pnlpc, position.hold_counter\n\n        # Other environment state\n        self.current_step = 0\n        self.cash = 1000.0\n        self.current_timestamp = None\n\n    def _reset(self, tensordict=None, **kwargs):\n        \"\"\"Reset all state including position\"\"\"\n        self.position.reset()  # Resets all position fields to defaults\n        self.current_step = 0\n        self.cash = 1000.0\n        # ... reset other state\n\n    def _step(self, tensordict):\n        \"\"\"Use position state in step logic\"\"\"\n        if action == \"BUY\" and self.position.position_size == 0:\n            self.position.position_size = 100\n            self.position.entry_price = current_price\n            self.position.current_position = 1.0\n        # ... rest of step logic\n</code></pre> <p>Benefits of PositionState: - Groups related state variables together (better organization) - Provides a single <code>.reset()</code> method for all position fields - Makes position state explicit and easier to track - Used consistently across all TorchTrade environments</p> <p>Implementation: <code>torchtrade/envs/state.py:8-30</code></p> <p>Quick Reference - HistoryTracker:</p> <p>Use <code>HistoryTracker</code> or <code>FuturesHistoryTracker</code> to record episode data for analysis (available in both offline and online environments - see State Management docs for full examples):</p> <pre><code>from torchtrade.envs.state import HistoryTracker, FuturesHistoryTracker\n\nclass CustomEnv(EnvBase):\n    def __init__(self):\n        super().__init__()\n        # For long-only environments (spot trading)\n        self.history = HistoryTracker()\n\n        # For futures environments (tracks position size)\n        # self.history = FuturesHistoryTracker()\n\n    def _step(self, tensordict):\n        # ... execute step logic ...\n\n        # Record step data for analysis\n        self.history.record_step(\n            price=current_price,\n            action=action.item(),\n            reward=reward,\n            portfolio_value=self.cash + self.position.position_value\n        )\n\n        # For FuturesHistoryTracker:\n        # self.history.record_step(..., position=self.position.position_size)\n\n    def _reset(self, tensordict=None, **kwargs):\n        self.history.reset()  # Clear history at episode start\n        # ... rest of reset logic\n</code></pre> <p>Access history for plotting or analysis: <code>history.to_dict()</code> returns <code>{'base_prices': [...], 'actions': [...], 'rewards': [...], 'portfolio_values': [...]}</code></p> <p>Implementation: <code>torchtrade/envs/state.py:33-148</code></p>"},{"location":"guides/custom-environment/#testing-custom-environments","title":"Testing Custom Environments","text":""},{"location":"guides/custom-environment/#1-spec-compliance","title":"1. Spec Compliance","text":"<p>Verify specs match actual outputs:</p> <pre><code>env = CustomEnv(...)\n\n# Check reset\nobs = env.reset()\nassert env.observation_spec.is_in(obs), \"Reset observation doesn't match spec\"\n\n# Check step\naction = env.action_spec.rand()\nobs = env.step(action)\nassert env.observation_spec.is_in(obs), \"Step observation doesn't match spec\"\nassert env.reward_spec.is_in(obs[\"reward\"]), \"Reward doesn't match spec\"\n</code></pre>"},{"location":"guides/custom-environment/#2-episode-completion","title":"2. Episode Completion","text":"<p>Ensure episodes terminate correctly:</p> <pre><code>env = CustomEnv(...)\nobs = env.reset()\n\nfor i in range(10000):  # Safety limit\n    action = env.action_spec.rand()\n    obs = env.step(action)\n    if obs[\"done\"]:\n        print(f\"Episode ended at step {i}\")\n        break\nelse:\n    raise AssertionError(\"Episode never ended!\")\n</code></pre>"},{"location":"guides/custom-environment/#3-reward-sanity","title":"3. Reward Sanity","text":"<p>Check reward values are reasonable:</p> <pre><code>rewards = []\nfor episode in range(100):\n    obs = env.reset()\n    episode_reward = 0\n    while not obs[\"done\"]:\n        action = env.action_spec.rand()\n        obs = env.step(action)\n        episode_reward += obs[\"reward\"].item()\n    rewards.append(episode_reward)\n\nprint(f\"Mean reward: {sum(rewards)/len(rewards):.2f}\")\nprint(f\"Reward range: [{min(rewards):.2f}, {max(rewards):.2f}]\")\n</code></pre>"},{"location":"guides/custom-environment/#common-pitfalls","title":"Common Pitfalls","text":"Issue Problem Solution Spec mismatch Observation shape != spec shape Update <code>_observation_spec</code> in <code>__init__</code> Forgotten batch_size TensorDict missing batch_size Always pass <code>batch_size=self.batch_size</code> Missing done signal Episode never ends Set <code>done=True</code> in terminal state Mutable state State persists across episodes Reset ALL state variables in <code>_reset()</code> Incorrect device Tensors on wrong device Use <code>self.device</code> for all tensors"},{"location":"guides/custom-environment/#next-steps","title":"Next Steps","text":"<ul> <li>Offline Environments - Understand existing environment architecture</li> <li>Reward Functions - Define custom reward logic</li> <li>Feature Engineering - Add technical indicators</li> <li>TorchRL EnvBase - Base class documentation</li> </ul>"},{"location":"guides/custom-features/","title":"Feature Engineering","text":"<p>TorchTrade allows you to add custom technical indicators and features to your market observations. This guide shows you how to preprocess your OHLCV data with custom features before it's fed to your policy.</p>"},{"location":"guides/custom-features/#how-it-works","title":"How It Works","text":"<p>The <code>feature_preprocessing_fn</code> parameter in environment configs transforms raw OHLCV data into custom features. This function is called on each resampled timeframe during environment initialization.</p> <p>IMPORTANT: All feature columns must start with <code>features_</code> prefix (e.g., <code>features_close</code>, <code>features_rsi_14</code>). Only columns with this prefix will be included in the observation space.</p> <p>Timeframe Format Matters</p> <p>When specifying <code>time_frames</code>, use canonical forms to avoid confusion:</p> <ul> <li>\u2705 Use: <code>\"1hour\"</code>, <code>\"2hours\"</code>, <code>\"1day\"</code></li> <li>\u274c Avoid: <code>\"60min\"</code>, <code>\"120min\"</code>, <code>\"24hour\"</code>, <code>\"1440min\"</code></li> </ul> <p>Why? Different formats create different observation keys:</p> <ul> <li><code>time_frames=[\"60min\"]</code> \u2192 observation key: <code>\"market_data_60Minute\"</code></li> <li><code>time_frames=[\"1hour\"]</code> \u2192 observation key: <code>\"market_data_1Hour\"</code></li> </ul> <p>These are treated as DIFFERENT timeframes. Models trained with one format won't work with the other. The framework will issue a warning if you use non-canonical forms like <code>\"60min\"</code> to guide you toward cleaner observation keys.</p>"},{"location":"guides/custom-features/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/custom-features/#example-1-adding-technical-indicators","title":"Example 1: Adding Technical Indicators","text":"<pre><code>import pandas as pd\nimport ta  # Technical Analysis library\nfrom torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\n\ndef custom_preprocessing(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Add technical indicators as features.\n\n    IMPORTANT: All feature columns must start with 'features_' prefix.\n    \"\"\"\n    # Basic OHLCV features (always include these)\n    df[\"features_open\"] = df[\"open\"]\n    df[\"features_high\"] = df[\"high\"]\n    df[\"features_low\"] = df[\"low\"]\n    df[\"features_close\"] = df[\"close\"]\n    df[\"features_volume\"] = df[\"volume\"]\n\n    # RSI (Relative Strength Index)\n    df[\"features_rsi_14\"] = ta.momentum.RSIIndicator(\n        df[\"close\"], window=14\n    ).rsi()\n\n    # MACD (Moving Average Convergence Divergence)\n    macd = ta.trend.MACD(df[\"close\"])\n    df[\"features_macd\"] = macd.macd()\n    df[\"features_macd_signal\"] = macd.macd_signal()\n    df[\"features_macd_histogram\"] = macd.macd_diff()\n\n    # Bollinger Bands\n    bollinger = ta.volatility.BollingerBands(df[\"close\"], window=20, window_dev=2)\n    df[\"features_bb_high\"] = bollinger.bollinger_hband()\n    df[\"features_bb_mid\"] = bollinger.bollinger_mavg()\n    df[\"features_bb_low\"] = bollinger.bollinger_lband()\n\n    # Fill NaN values (important!)\n    df.fillna(0, inplace=True)\n\n    return df\n\n# Use in environment config\nconfig = SeqLongOnlyEnvConfig(\n    feature_preprocessing_fn=custom_preprocessing,\n    time_frames=[\"1min\", \"5min\", \"15min\"],  # Note: use \"1hour\" not \"60min\"\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n    initial_cash=1000\n)\n\nenv = SeqLongOnlyEnv(df, config)\n</code></pre>"},{"location":"guides/custom-features/#example-2-normalized-features-recommended","title":"Example 2: Normalized Features (Recommended)","text":"<p>Feature normalization is critical for stable RL training. The recommended approach is to normalize features during preprocessing using sklearn's StandardScaler, which avoids device-related issues with TorchRL's VecNorm transforms.</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef normalized_preprocessing(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Normalize features using StandardScaler for stable training.\n\n    This approach is preferred over VecNormV2/ObservationNorm transforms\n    which can have device compatibility issues on GPU.\n    \"\"\"\n    # Basic OHLCV\n    df[\"features_open\"] = df[\"open\"]\n    df[\"features_high\"] = df[\"high\"]\n    df[\"features_low\"] = df[\"low\"]\n    df[\"features_close\"] = df[\"close\"]\n    df[\"features_volume\"] = df[\"volume\"]\n\n    # Price changes (returns)\n    df[\"features_return\"] = df[\"close\"].pct_change()\n\n    # Normalize features\n    scaler = StandardScaler()\n    feature_cols = [col for col in df.columns if col.startswith(\"features_\")]\n\n    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n\n    # Fill NaN values\n    df.fillna(0, inplace=True)\n\n    return df\n\nconfig = SeqLongOnlyEnvConfig(\n    feature_preprocessing_fn=normalized_preprocessing,\n    ...\n)\n</code></pre> <p>Alternative approaches: - TorchRL transforms - VecNormV2 and ObservationNorm are available but may have device compatibility issues - Network level - Use BatchNorm, LayerNorm, or other normalization layers in your policy network</p>"},{"location":"guides/custom-features/#advanced-normalization-considerations","title":"Advanced Normalization Considerations","text":"<p>The StandardScaler approach above uses fixed statistics from the training data. For financial data with regime changes (market conditions that shift over time), you might eventually want more adaptive approaches:</p> <p>Per-regime normalization: - Detect market regimes (bull/bear, high/low volatility) - Maintain separate scalers for each regime - Apply the appropriate scaler based on current market conditions</p> <p>Rolling window statistics: - Use a sliding window (e.g., last 252 trading days) to compute normalization statistics - Updates statistics as new data arrives - Adapts to gradual market shifts</p> <p>Adaptive normalization: - Exponentially weighted moving average of statistics - Continuously updates during training - Balances stability with adaptation</p> <p>Example of rolling window approach: <pre><code>def rolling_normalization(df: pd.DataFrame, window=252) -&gt; pd.DataFrame:\n    \"\"\"Normalize using rolling window statistics.\"\"\"\n    df = df.copy()\n\n    # Create features\n    df[\"features_close\"] = df[\"close\"]\n    df[\"features_volume\"] = df[\"volume\"]\n\n    # Apply rolling normalization\n    for col in [c for c in df.columns if c.startswith(\"features_\")]:\n        rolling_mean = df[col].rolling(window=window, min_periods=1).mean()\n        rolling_std = df[col].rolling(window=window, min_periods=1).std()\n        df[col] = (df[col] - rolling_mean) / (rolling_std + 1e-8)\n\n    df.fillna(0, inplace=True)\n    return df\n</code></pre></p> <p>Trade-offs: - Fixed StandardScaler: \u2705 Simpler, reproducible | \u26a0\ufe0f Less adaptive - Adaptive methods: \u2705 Handles regime changes | \u26a0\ufe0f More complex, harder to reproduce</p> <p>For most use cases, StandardScaler is sufficient. Consider adaptive methods if you observe performance degradation over time due to market regime changes.</p>"},{"location":"guides/custom-features/#important-rules","title":"Important Rules","text":""},{"location":"guides/custom-features/#1-feature-column-naming","title":"1. Feature Column Naming","text":"<p>All feature columns MUST start with <code>features_</code> prefix:</p> <pre><code># \u2705 Correct\ndf[\"features_rsi_14\"] = ...\ndf[\"features_macd\"] = ...\ndf[\"features_close\"] = ...\n\n# \u274c Wrong - will be ignored\ndf[\"rsi_14\"] = ...\ndf[\"macd\"] = ...\ndf[\"close\"] = ...\n</code></pre>"},{"location":"guides/custom-features/#2-handle-nan-values","title":"2. Handle NaN Values","text":"<p>Technical indicators often produce NaN values at the beginning. Always fill NaN values:</p> <pre><code># Option 1: Fill with 0 (recommended for most indicators)\ndf.fillna(0, inplace=True)\n\n# Option 2: Forward fill (use for prices)\ndf.fillna(method='ffill', inplace=True)\n\n# Option 3: Backward fill\ndf.fillna(method='bfill', inplace=True)\n</code></pre>"},{"location":"guides/custom-features/#3-return-the-modified-dataframe","title":"3. Return the Modified DataFrame","text":"<p>Your function must return the modified DataFrame:</p> <pre><code>def preprocessing(df: pd.DataFrame) -&gt; pd.DataFrame:\n    # ... add features ...\n    return df  # \u2705 Return modified DataFrame\n</code></pre>"},{"location":"guides/custom-features/#4-avoid-lookahead-bias","title":"4. Avoid Lookahead Bias","text":"<p>Do NOT use future data in your features:</p> <pre><code># \u274c Lookahead bias - using future data\ndf[\"features_future_return\"] = df[\"close\"].pct_change().shift(-1)\n\n# \u2705 Correct - only past data\ndf[\"features_past_return\"] = df[\"close\"].pct_change()\n</code></pre>"},{"location":"guides/custom-features/#common-technical-indicators","title":"Common Technical Indicators","text":""},{"location":"guides/custom-features/#quick-reference-table","title":"Quick Reference Table","text":"Category Indicator ta Library Code Use Case Momentum RSI <code>ta.momentum.RSIIndicator(close, window=14).rsi()</code> Overbought/oversold detection Stochastic <code>ta.momentum.StochasticOscillator(high, low, close).stoch()</code> Momentum confirmation Williams %R <code>ta.momentum.WilliamsRIndicator(high, low, close, lbp=14).williams_r()</code> Short-term overbought/oversold Trend SMA <code>ta.trend.SMAIndicator(close, window=20).sma_indicator()</code> Trend direction EMA <code>ta.trend.EMAIndicator(close, window=20).ema_indicator()</code> Responsive trend following MACD <code>ta.trend.MACD(close).macd()</code> Trend changes ADX <code>ta.trend.ADXIndicator(high, low, close, window=14).adx()</code> Trend strength Volatility Bollinger Bands <code>ta.volatility.BollingerBands(close, window=20)</code> Volatility and price bounds ATR <code>ta.volatility.AverageTrueRange(high, low, close, window=14).average_true_range()</code> Volatility measurement Keltner <code>ta.volatility.KeltnerChannel(high, low, close)</code> Alternative to Bollinger Volume OBV <code>ta.volume.OnBalanceVolumeIndicator(close, volume).on_balance_volume()</code> Accumulation/distribution VPT <code>ta.volume.VolumePriceTrendIndicator(close, volume).volume_price_trend()</code> Volume-price confirmation ADI <code>ta.volume.AccDistIndexIndicator(high, low, close, volume).acc_dist_index()</code> Money flow"},{"location":"guides/custom-features/#usage-pattern","title":"Usage Pattern","text":"<pre><code>import ta\n\ndef add_indicators(df: pd.DataFrame) -&gt; pd.DataFrame:\n    # Basic OHLCV\n    df[\"features_close\"] = df[\"close\"]\n    df[\"features_volume\"] = df[\"volume\"]\n    # ... other OHLCV ...\n\n    # Pick indicators from table above\n    df[\"features_rsi_14\"] = ta.momentum.RSIIndicator(df[\"close\"], window=14).rsi()\n    df[\"features_sma_20\"] = ta.trend.SMAIndicator(df[\"close\"], window=20).sma_indicator()\n    df[\"features_atr\"] = ta.volatility.AverageTrueRange(\n        df[\"high\"], df[\"low\"], df[\"close\"], window=14\n    ).average_true_range()\n\n    df.fillna(0, inplace=True)\n    return df\n</code></pre>"},{"location":"guides/custom-features/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/custom-features/#vectorize-operations","title":"Vectorize Operations","text":"<p>Use pandas vectorized operations instead of loops for faster computation:</p> <pre><code># \u274c Slow - using loops\nfor i in range(len(df)):\n    df.loc[i, \"features_return\"] = (df.loc[i, \"close\"] / df.loc[i-1, \"close\"]) - 1\n\n# \u2705 Fast - vectorized (100x faster)\ndf[\"features_return\"] = df[\"close\"].pct_change()\n</code></pre> <p>General principle: If you're using a loop, there's probably a pandas method that does it faster.</p>"},{"location":"guides/custom-features/#debugging-tips","title":"Debugging Tips","text":""},{"location":"guides/custom-features/#check-for-nan-values","title":"Check for NaN Values","text":"<pre><code>def safe_preprocessing(df: pd.DataFrame) -&gt; pd.DataFrame:\n    df[\"features_close\"] = df[\"close\"]\n    # ... add features ...\n\n    # Check for NaN values before filling\n    nan_counts = df.isna().sum()\n    if nan_counts.any():\n        print(\"\u26a0\ufe0f Warning: NaN values detected:\")\n        print(nan_counts[nan_counts &gt; 0])\n\n    df.fillna(0, inplace=True)\n    return df\n</code></pre> <p>This helps catch issues with indicator configuration or missing data.</p>"},{"location":"guides/custom-features/#recommended-libraries","title":"Recommended Libraries","text":"Library Indicators Installation Best For ta 40+ <code>pip install ta</code> Standard indicators, easy API pandas-ta 130+ <code>pip install pandas-ta</code> Comprehensive collection TA-Lib 150+ <code>pip install TA-Lib</code> Performance, industry standard sklearn N/A <code>pip install scikit-learn</code> Feature scaling, normalization <p>Recommendation: Start with <code>ta</code> for simplicity, use <code>TA-Lib</code> if you need maximum performance.</p>"},{"location":"guides/custom-features/#next-steps","title":"Next Steps","text":"<ul> <li>Reward Functions - Design reward signals that work with your features</li> <li>Understanding the Sampler - How multi-timeframe sampling works</li> <li>Transforms - Alternative feature engineering with Chronos embeddings</li> <li>Offline Environments - Apply custom features to environments</li> </ul>"},{"location":"guides/metrics/","title":"Performance Metrics","text":"<p>TorchTrade provides a comprehensive set of trading performance metrics to evaluate agent performance. These metrics help assess risk-adjusted returns, drawdown characteristics, and trading efficiency.</p>"},{"location":"guides/metrics/#overview","title":"Overview","text":"<p>The <code>torchtrade.metrics</code> module provides standard trading metrics including:</p> Metric Description Typical Good Value Total Return Overall portfolio return &gt; 0% Sharpe Ratio Risk-adjusted return (volatility) &gt; 1.0 (&gt; 2.0 excellent) Sortino Ratio Risk-adjusted return (downside volatility) &gt; 1.5 Calmar Ratio Return per unit of maximum drawdown &gt; 1.0 Max Drawdown Largest peak-to-trough decline &lt; 20% Max DD Duration Length of maximum drawdown period Shorter is better Win Rate Percentage of profitable periods &gt; 50% Profit Factor Total profits / total losses &gt; 1.5 Number of Trades Total trades executed Depends on strategy"},{"location":"guides/metrics/#available-metrics","title":"Available Metrics","text":""},{"location":"guides/metrics/#1-maximum-drawdown","title":"1. Maximum Drawdown","text":"<p>Measures the largest peak-to-trough decline in portfolio value.</p> <pre><code>from torchtrade.metrics import compute_max_drawdown\nimport torch\n\n# Portfolio values over time\nportfolio_values = torch.tensor([1000, 1100, 1050, 900, 950, 1200])\n\ndd_metrics = compute_max_drawdown(portfolio_values)\n\nprint(f\"Max Drawdown: {dd_metrics['max_drawdown']:.2%}\")\nprint(f\"DD Duration: {dd_metrics['max_drawdown_duration']} periods\")\nprint(f\"Current Drawdown: {dd_metrics['current_drawdown']:.2%}\")\n</code></pre> <p>Returns: <pre><code>{\n    'max_drawdown': -0.1818,        # -18.18% drawdown\n    'max_drawdown_duration': 2,      # 2 periods from peak to trough\n    'current_drawdown': 0.0,         # Currently at new peak\n    'peak_value': 1100.0,            # Peak before drawdown\n    'trough_value': 900.0            # Lowest point in drawdown\n}\n</code></pre></p>"},{"location":"guides/metrics/#2-sharpe-ratio","title":"2. Sharpe Ratio","text":"<p>Measures risk-adjusted returns by comparing excess returns to volatility.</p> <pre><code>from torchtrade.metrics import compute_sharpe_ratio\nimport torch\n\n# Per-period returns\nreturns = torch.tensor([0.01, -0.02, 0.03, -0.01, 0.02, 0.04])\n\n# For 1-minute crypto data: 525,600 periods per year\nsharpe = compute_sharpe_ratio(\n    returns,\n    periods_per_year=525600,\n    rf_annual=0.0  # Risk-free rate (default: 0%)\n)\n\nprint(f\"Sharpe Ratio: {sharpe:.2f}\")\n</code></pre> <p>Interpretation: - &lt; 1.0: Poor risk-adjusted returns - 1.0-2.0: Good performance - &gt; 2.0: Very good performance - &gt; 3.0: Excellent performance</p>"},{"location":"guides/metrics/#3-sortino-ratio","title":"3. Sortino Ratio","text":"<p>Similar to Sharpe, but only penalizes downside volatility (returns below target).</p> <pre><code>from torchtrade.metrics import compute_sortino_ratio\n\nsortino = compute_sortino_ratio(\n    returns,\n    periods_per_year=525600,\n    rf_annual=0.0,\n    target_return=0.0  # Target return threshold\n)\n\nprint(f\"Sortino Ratio: {sortino:.2f}\")\n</code></pre> <p>Why use Sortino? - More relevant for trading strategies where upside volatility is desirable - Only penalizes downside risk below target - Often higher than Sharpe for profitable strategies</p>"},{"location":"guides/metrics/#4-calmar-ratio","title":"4. Calmar Ratio","text":"<p>Measures annualized return per unit of maximum drawdown.</p> <pre><code>from torchtrade.metrics import compute_calmar_ratio\n\ncalmar = compute_calmar_ratio(\n    portfolio_values,\n    periods_per_year=525600\n)\n\nprint(f\"Calmar Ratio: {calmar:.2f}\")\n</code></pre> <p>Interpretation: - &lt; 0.5: Poor - 0.5-1.0: Acceptable - 1.0-3.0: Good - &gt; 3.0: Excellent</p>"},{"location":"guides/metrics/#5-win-rate-and-profit-metrics","title":"5. Win Rate and Profit Metrics","text":"<p>Note: Due to the possibility of customized reward functions, we define win rate as the percentage of steps where <code>reward &gt; 0</code> rather than using traditional profit/loss definitions.</p> <pre><code>from torchtrade.metrics import compute_win_rate\n\n# Rewards from episode\nrewards = torch.tensor([0.01, -0.02, 0.03, -0.01, 0.02, 0.04])\n\nwin_metrics = compute_win_rate(rewards)\n\nprint(f\"Win Rate: {win_metrics['win_rate (reward&gt;0)']:.2%}\")\nprint(f\"Average Win: {win_metrics['avg_win']:.4f}\")\nprint(f\"Average Loss: {win_metrics['avg_loss']:.4f}\")\nprint(f\"Profit Factor: {win_metrics['profit_factor']:.2f}\")\n</code></pre> <p>Returns: <pre><code>{\n    'win_rate (reward&gt;0)': 0.6667,   # 66.67% winning periods\n    'avg_win': 0.025,                 # Average winning return\n    'avg_loss': -0.015,               # Average losing return\n    'profit_factor': 2.5              # 2.5x more profits than losses\n}\n</code></pre></p>"},{"location":"guides/metrics/#computing-all-metrics-at-once","title":"Computing All Metrics at Once","text":"<p>Use <code>compute_all_metrics()</code> to calculate all standard metrics in one call:</p> <pre><code>from torchtrade.metrics import compute_all_metrics\nimport torch\n\n# Episode data\nportfolio_values = torch.tensor([1000, 1050, 1100, 1080, 1150, 1200])\nrewards = torch.tensor([0.05, 0.048, -0.018, 0.065, 0.043])\naction_history = [2, 1, 0, 2, 1]  # Actions taken during episode\n\n# Compute all metrics\n# For 1-minute crypto: 525,600 periods/year\n# For 5-minute: 105,120 periods/year\n# For 1-hour: 8,760 periods/year\n# For daily: 365 periods/year\nmetrics = compute_all_metrics(\n    portfolio_values=portfolio_values,\n    rewards=rewards,\n    action_history=action_history,\n    periods_per_year=525600\n)\n\n# Print results\nfor metric_name, value in metrics.items():\n    print(f\"{metric_name}: {value}\")\n</code></pre> <p>Output: <pre><code>total_return: 0.2\nsharpe_ratio: 1.85\nsortino_ratio: 2.34\ncalmar_ratio: 3.12\nmax_drawdown: -0.0182\nmax_dd_duration: 1\nnum_trades: 2\nwin_rate (reward&gt;0): 0.8\navg_win: 0.052\navg_loss: -0.018\nprofit_factor: 5.78\n</code></pre></p>"},{"location":"guides/metrics/#integration-with-training-loops","title":"Integration with Training Loops","text":""},{"location":"guides/metrics/#example-1-evaluating-policy-performance","title":"Example 1: Evaluating Policy Performance","text":"<pre><code>from torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\nfrom torchtrade.metrics import compute_all_metrics\nfrom torchrl.collectors import SyncDataCollector\nimport torch\n\n# Create evaluation environment\neval_env = SeqLongOnlyEnv(test_df, config)\n\n# Collect evaluation rollouts\ncollector = SyncDataCollector(\n    eval_env,\n    policy,\n    frames_per_batch=10000,\n    total_frames=10000,\n    device=\"cuda\"\n)\n\n# Collect episode data\nfor batch in collector:\n    # Extract episode data\n    rewards = batch[\"reward\"]\n    # Note: Portfolio values need to be tracked in environment or computed from rewards\n\n    # Compute metrics\n    metrics = compute_all_metrics(\n        portfolio_values=portfolio_values,\n        rewards=rewards,\n        action_history=batch[\"action\"].tolist(),\n        periods_per_year=525600  # Adjust based on execute_on frequency\n    )\n\n    print(f\"Sharpe Ratio: {metrics['sharpe_ratio']:.2f}\")\n    print(f\"Max Drawdown: {metrics['max_drawdown']:.2%}\")\n    print(f\"Win Rate: {metrics['win_rate (reward&gt;0)']:.2%}\")\n</code></pre>"},{"location":"guides/metrics/#example-2-logging-metrics-to-weights-biases","title":"Example 2: Logging Metrics to Weights &amp; Biases","text":"<pre><code>import wandb\nfrom torchtrade.metrics import compute_all_metrics\n\n# Initialize W&amp;B\nwandb.init(project=\"torchtrade-training\")\n\n# During training loop\nfor epoch in range(num_epochs):\n    # ... training code ...\n\n    # Evaluate on test set\n    metrics = compute_all_metrics(\n        portfolio_values=eval_portfolio_values,\n        rewards=eval_rewards,\n        action_history=eval_actions,\n        periods_per_year=525600\n    )\n\n    # Log to W&amp;B\n    wandb.log({\n        f\"eval/{k}\": v for k, v in metrics.items()\n    })\n</code></pre>"},{"location":"guides/metrics/#creating-custom-metrics","title":"Creating Custom Metrics","text":"<p>You can create custom metrics by following the same pattern as built-in functions:</p>"},{"location":"guides/metrics/#example-maximum-consecutive-wins","title":"Example: Maximum Consecutive Wins","text":"<pre><code>import torch\n\ndef compute_max_consecutive_wins(rewards: torch.Tensor) -&gt; int:\n    \"\"\"\n    Compute the maximum number of consecutive winning periods.\n\n    Args:\n        rewards: 1D torch.Tensor of per-period rewards\n\n    Returns:\n        Maximum consecutive wins\n    \"\"\"\n    if len(rewards) == 0:\n        return 0\n\n    # Create boolean mask for wins\n    wins = rewards &gt; 0\n\n    # Count consecutive wins\n    max_streak = 0\n    current_streak = 0\n\n    for is_win in wins:\n        if is_win:\n            current_streak += 1\n            max_streak = max(max_streak, current_streak)\n        else:\n            current_streak = 0\n\n    return max_streak\n\n# Usage\nrewards = torch.tensor([0.01, 0.02, 0.03, -0.01, 0.02, 0.01])\nmax_wins = compute_max_consecutive_wins(rewards)\nprint(f\"Max Consecutive Wins: {max_wins}\")  # 3\n</code></pre>"},{"location":"guides/metrics/#example-average-trade-duration","title":"Example: Average Trade Duration","text":"<pre><code>def compute_avg_trade_duration(action_history: list) -&gt; float:\n    \"\"\"\n    Compute average duration of trades (time in position).\n\n    Args:\n        action_history: List of actions (0=sell/close, 1=hold, 2=buy/long)\n\n    Returns:\n        Average trade duration in periods\n    \"\"\"\n    trade_durations = []\n    current_duration = 0\n    in_position = False\n\n    for action in action_history:\n        if action == 2:  # Enter position\n            in_position = True\n            current_duration = 1\n        elif action == 1 and in_position:  # Hold position\n            current_duration += 1\n        elif action == 0 and in_position:  # Exit position\n            trade_durations.append(current_duration)\n            in_position = False\n            current_duration = 0\n\n    if len(trade_durations) == 0:\n        return 0.0\n\n    return sum(trade_durations) / len(trade_durations)\n\n# Usage\nactions = [2, 1, 1, 0, 2, 1, 0, 1, 1]\navg_duration = compute_avg_trade_duration(actions)\nprint(f\"Average Trade Duration: {avg_duration:.1f} periods\")\n</code></pre>"},{"location":"guides/metrics/#periods-per-year-configuration","title":"Periods Per Year Configuration","text":"<p>Choose <code>periods_per_year</code> based on your <code>execute_on</code> frequency:</p> Execute On Periods Per Year Calculation 1 minute 525,600 60 \u00d7 24 \u00d7 365 5 minutes 105,120 12 \u00d7 24 \u00d7 365 15 minutes 35,040 4 \u00d7 24 \u00d7 365 1 hour 8,760 24 \u00d7 365 4 hours 2,190 6 \u00d7 365 1 day 365 365 <pre><code># Example: 5-minute execution\nconfig = SeqLongOnlyEnvConfig(\n    execute_on=(5, \"Minute\"),\n    # ... other config\n)\n\n# Use 105,120 periods per year for metrics\nmetrics = compute_all_metrics(\n    portfolio_values=portfolio_values,\n    rewards=rewards,\n    action_history=action_history,\n    periods_per_year=105120  # 5-minute periods\n)\n</code></pre>"},{"location":"guides/metrics/#best-practices","title":"Best Practices","text":""},{"location":"guides/metrics/#1-choose-appropriate-benchmarks","title":"1. Choose Appropriate Benchmarks","text":"<p>Compare metrics to relevant benchmarks:</p> <pre><code># Compute buy &amp; hold baseline\nbuy_hold_return = (df['close'].iloc[-1] - df['close'].iloc[0]) / df['close'].iloc[0]\n\nprint(f\"Agent Return: {metrics['total_return']:.2%}\")\nprint(f\"Buy &amp; Hold: {buy_hold_return:.2%}\")\nprint(f\"Outperformance: {metrics['total_return'] - buy_hold_return:.2%}\")\n</code></pre>"},{"location":"guides/metrics/#2-consider-multiple-metrics","title":"2. Consider Multiple Metrics","text":"<p>Don't rely on a single metric. Look at: - Return metrics: Total return, Sharpe, Sortino - Risk metrics: Max drawdown, drawdown duration - Efficiency metrics: Win rate, profit factor, number of trades</p>"},{"location":"guides/metrics/#3-account-for-transaction-costs","title":"3. Account for Transaction Costs","text":"<p>Ensure your environment includes realistic transaction fees and slippage:</p> <pre><code>config = SeqLongOnlyEnvConfig(\n    transaction_fee=0.0025,  # 0.25% per trade\n    slippage=0.001           # 0.1% slippage\n)\n</code></pre>"},{"location":"guides/metrics/#4-use-rolling-windows-for-stability","title":"4. Use Rolling Windows for Stability","text":"<p>For long episodes, compute metrics over rolling windows:</p> <pre><code>def compute_rolling_sharpe(portfolio_values, window_size=100, periods_per_year=525600):\n    \"\"\"Compute Sharpe ratio over rolling window.\"\"\"\n    returns = torch.diff(portfolio_values) / portfolio_values[:-1]\n\n    rolling_sharpes = []\n    for i in range(len(returns) - window_size + 1):\n        window_returns = returns[i:i+window_size]\n        sharpe = compute_sharpe_ratio(window_returns, periods_per_year)\n        rolling_sharpes.append(sharpe)\n\n    return rolling_sharpes\n</code></pre>"},{"location":"guides/metrics/#interpreting-metrics-together","title":"Interpreting Metrics Together","text":""},{"location":"guides/metrics/#example-evaluating-a-strategy","title":"Example: Evaluating a Strategy","text":"<pre><code>metrics = compute_all_metrics(portfolio_values, rewards, actions, periods_per_year=525600)\n\n# Check profitability\nif metrics['total_return'] &gt; 0:\n    print(\"\u2713 Strategy is profitable\")\nelse:\n    print(\"\u2717 Strategy is unprofitable\")\n\n# Check risk-adjusted returns\nif metrics['sharpe_ratio'] &gt; 1.5:\n    print(f\"\u2713 Good risk-adjusted returns (Sharpe: {metrics['sharpe_ratio']:.2f})\")\nelse:\n    print(f\"\u2717 Poor risk-adjusted returns (Sharpe: {metrics['sharpe_ratio']:.2f})\")\n\n# Check drawdown risk\nif abs(metrics['max_drawdown']) &lt; 0.2:  # Less than 20%\n    print(f\"\u2713 Acceptable drawdown ({metrics['max_drawdown']:.2%})\")\nelse:\n    print(f\"\u2717 High drawdown risk ({metrics['max_drawdown']:.2%})\")\n\n# Check trade efficiency\nif metrics['win_rate (reward&gt;0)'] &gt; 0.5 and metrics['profit_factor'] &gt; 1.5:\n    print(f\"\u2713 Efficient trading (Win rate: {metrics['win_rate (reward&gt;0)']:.2%}, PF: {metrics['profit_factor']:.2f})\")\nelse:\n    print(f\"\u2717 Inefficient trading (Win rate: {metrics['win_rate (reward&gt;0)']:.2%}, PF: {metrics['profit_factor']:.2f})\")\n</code></pre>"},{"location":"guides/metrics/#common-issues","title":"Common Issues","text":""},{"location":"guides/metrics/#issue-1-extreme-sharpe-ratios","title":"Issue 1: Extreme Sharpe Ratios","text":"<p>Problem: Sharpe ratio is unrealistically high (&gt; 10) or low (&lt; -10)</p> <p>Solution: - Check that <code>periods_per_year</code> matches your execution frequency - Ensure returns are not scaled incorrectly - Verify no division by zero in volatility calculation</p>"},{"location":"guides/metrics/#issue-2-zero-max-drawdown","title":"Issue 2: Zero Max Drawdown","text":"<p>Problem: Max drawdown is 0 even with losses</p> <p>Solution: - Ensure portfolio values are monotonically increasing at some point - Check that portfolio values are computed correctly - Verify you're passing portfolio values, not returns</p>"},{"location":"guides/metrics/#issue-3-inconsistent-metrics","title":"Issue 3: Inconsistent Metrics","text":"<p>Problem: High Sharpe but low Calmar, or vice versa</p> <p>Explanation: - High Sharpe, Low Calmar: Strategy has consistent small gains but occasional large drawdowns - Low Sharpe, High Calmar: Strategy has volatile returns but recovers quickly from drawdowns</p> <p>Both are valid - consider which risk profile suits your use case.</p>"},{"location":"guides/metrics/#next-steps","title":"Next Steps","text":"<ul> <li>Reward Functions - Design rewards to optimize for specific metrics</li> <li>Feature Engineering - Add features that correlate with performance</li> <li>Offline Environments - Backtest strategies with historical data</li> <li>Examples - See metrics in complete training scripts</li> </ul>"},{"location":"guides/metrics/#references","title":"References","text":"<ul> <li>Sharpe Ratio - Original paper and interpretation</li> <li>Sortino Ratio - Downside risk-adjusted returns</li> <li>Calmar Ratio - Return per unit of drawdown</li> <li>Maximum Drawdown - Peak-to-trough decline</li> </ul>"},{"location":"guides/reward-functions/","title":"Reward Functions","text":"<p>Reward functions shape your agent's behavior. TorchTrade provides a flexible system for defining custom reward functions that go beyond simple log returns.</p>"},{"location":"guides/reward-functions/#default-behavior","title":"Default Behavior","text":"<p>By default, all environments use log returns:</p> <pre><code>reward = log(portfolio_value_t / portfolio_value_t-1)\n</code></pre> <p>You can potentially improve learning and generalization by customizing this reward function. See below for some references on reward function design.</p>"},{"location":"guides/reward-functions/#how-it-works","title":"How It Works","text":"<p>Custom reward functions receive a <code>RewardContext</code> object and return a float reward:</p> <pre><code>from torchtrade.envs.reward import RewardContext\n\ndef my_reward(ctx: RewardContext) -&gt; float:\n    \"\"\"Custom reward function\"\"\"\n    return reward_value\n</code></pre>"},{"location":"guides/reward-functions/#rewardcontext-fields","title":"RewardContext Fields","text":""},{"location":"guides/reward-functions/#quick-reference-table","title":"Quick Reference Table","text":"Category Field Type Description Core <code>old_portfolio_value</code> float Portfolio value before action <code>new_portfolio_value</code> float Portfolio value after action <code>action</code> int Action taken (index) <code>current_step</code> int Current episode step (0-indexed) <code>max_steps</code> int Maximum steps in episode <code>trade_executed</code> bool Whether a trade occurred <code>fee_paid</code> float Transaction fees this step <code>slippage_amount</code> float Slippage incurred this step Sequential Envs <code>portfolio_value_history</code> list[float] Past portfolio values <code>action_history</code> list[int] Past actions <code>reward_history</code> list[float] Past rewards <code>buy_and_hold_value</code> float Buy &amp; hold benchmark (terminal only) Futures Envs <code>leverage</code> float Current leverage <code>margin_ratio</code> float Current margin ratio <code>liquidation_price</code> float Liquidation price <code>liquidated</code> bool Whether position was liquidated One-Step Envs <code>rollout_returns</code> tensor Returns during rollout"},{"location":"guides/reward-functions/#basic-examples","title":"Basic Examples","text":""},{"location":"guides/reward-functions/#example-1-transaction-cost-penalty","title":"Example 1: Transaction Cost Penalty","text":"<pre><code>from torchtrade.envs.reward import RewardContext\nimport numpy as np\n\ndef cost_penalty_reward(ctx: RewardContext) -&gt; float:\n    \"\"\"Penalize transaction costs to discourage overtrading\"\"\"\n    if ctx.old_portfolio_value &lt;= 0:\n        return 0.0\n\n    # Base reward: log return\n    log_return = np.log(ctx.new_portfolio_value / ctx.old_portfolio_value)\n\n    # Penalty for fees and slippage (normalized by portfolio value)\n    cost_penalty = -(ctx.fee_paid + ctx.slippage_amount) / ctx.old_portfolio_value\n\n    return log_return + cost_penalty\n\n# Use in environment\nconfig = SeqLongOnlyEnvConfig(\n    reward_fn=cost_penalty_reward,\n    ...\n)\n</code></pre>"},{"location":"guides/reward-functions/#example-2-sharpe-ratio-reward","title":"Example 2: Sharpe Ratio Reward","text":"<pre><code>def sharpe_ratio_reward(ctx: RewardContext) -&gt; float:\n    \"\"\"Reward based on running Sharpe ratio\"\"\"\n    # Access portfolio history\n    history = ctx.metadata.get('portfolio_value_history', [])\n\n    if len(history) &lt; 10:  # Need minimum history\n        if ctx.old_portfolio_value &lt;= 0:\n            return 0.0\n        return np.log(ctx.new_portfolio_value / ctx.old_portfolio_value)\n\n    # Compute returns from history\n    returns = [np.log(history[i] / history[i-1]) for i in range(1, len(history))]\n\n    # Compute Sharpe ratio\n    mean_return = np.mean(returns)\n    std_return = np.std(returns) + 1e-9\n    sharpe = mean_return / std_return\n\n    return np.clip(sharpe, -10.0, 10.0)\n</code></pre>"},{"location":"guides/reward-functions/#example-3-sparse-terminal-reward","title":"Example 3: Sparse Terminal Reward","text":"<pre><code>def terminal_reward(ctx: RewardContext) -&gt; float:\n    \"\"\"Reward only at episode end (for episodic optimization)\"\"\"\n    # No reward during episode\n    if ctx.current_step &lt; ctx.max_steps - 1:\n        return 0.0\n\n    # Terminal reward: compare to buy &amp; hold\n    buy_hold = ctx.metadata.get('buy_and_hold_value', ctx.old_portfolio_value)\n\n    if ctx.new_portfolio_value &gt; buy_hold:\n        return 1.0  # Beat buy &amp; hold\n    else:\n        return -1.0  # Lost to buy &amp; hold\n</code></pre>"},{"location":"guides/reward-functions/#when-to-use-each-reward","title":"When to Use Each Reward","text":"Reward Type Use Case Pros Cons Log Return (default) General purpose Simple, stable Ignores costs, risk Cost Penalty Reduce overtrading Discourages frequent trades May miss opportunities Sharpe Ratio Risk-adjusted performance Considers volatility Requires history, unstable early Terminal Sparse Episodic tasks Clear objective Sparse signal, slow learning Drawdown Penalty Risk management Controls max loss May exit winners early Win Rate Bonus Consistency focus Rewards reliability May sacrifice returns"},{"location":"guides/reward-functions/#design-principles","title":"Design Principles","text":""},{"location":"guides/reward-functions/#1-dense-vs-sparse-rewards","title":"1. Dense vs Sparse Rewards","text":"<p>Dense rewards (every step): - Faster learning - More signal for gradient updates - Risk: May optimize for short-term gains</p> <p>Sparse rewards (terminal only): - Aligns with true objective (final return) - Less noise - Risk: Slower learning, credit assignment problem</p>"},{"location":"guides/reward-functions/#2-normalize-rewards","title":"2. Normalize Rewards","text":"<p>Keep rewards in a consistent range (e.g., [-1, 1]) for stable learning:</p> <pre><code># Clip extreme values\nreward = np.clip(reward, -10.0, 10.0)\n\n# Or normalize by running statistics\nreward = (reward - running_mean) / (running_std + 1e-9)\n</code></pre>"},{"location":"guides/reward-functions/#3-avoid-lookahead-bias","title":"3. Avoid Lookahead Bias","text":"<p>Only use information available at decision time:</p> <pre><code># \u274c Lookahead bias\nfuture_return = history[t+1] / history[t] - 1\n\n# \u2705 Correct - past data only\npast_return = history[t] / history[t-1] - 1\n</code></pre>"},{"location":"guides/reward-functions/#4-consider-environment-type","title":"4. Consider Environment Type","text":"<p>Sequential Environments: Can use history for complex rewards One-Step Environments: Limited to immediate reward (no history) Futures Environments: Consider liquidation risk in reward</p>"},{"location":"guides/reward-functions/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guides/reward-functions/#multi-objective-rewards","title":"Multi-Objective Rewards","text":"<p>Combine multiple objectives with weighted sum:</p> <pre><code>def multi_objective_reward(ctx: RewardContext) -&gt; float:\n    # Component 1: Returns\n    log_return = np.log(ctx.new_portfolio_value / ctx.old_portfolio_value)\n\n    # Component 2: Cost penalty\n    cost = -(ctx.fee_paid + ctx.slippage_amount) / ctx.old_portfolio_value\n\n    # Component 3: Risk penalty (drawdown)\n    history = ctx.metadata.get('portfolio_value_history', [])\n    max_value = max(history) if history else ctx.old_portfolio_value\n    drawdown = (ctx.new_portfolio_value - max_value) / max_value\n    drawdown_penalty = min(0, drawdown) * 0.5  # Penalize only negative\n\n    # Weighted combination\n    return (\n        1.0 * log_return +      # 100% weight on returns\n        0.5 * cost +             # 50% weight on costs\n        0.3 * drawdown_penalty   # 30% weight on drawdown\n    )\n</code></pre>"},{"location":"guides/reward-functions/#adaptive-rewards","title":"Adaptive Rewards","text":"<p>Adjust rewards based on training progress:</p> <pre><code>class AdaptiveReward:\n    def __init__(self):\n        self.episode_count = 0\n\n    def __call__(self, ctx: RewardContext) -&gt; float:\n        # Early training: Dense rewards for exploration\n        if self.episode_count &lt; 1000:\n            return np.log(ctx.new_portfolio_value / ctx.old_portfolio_value)\n\n        # Later training: Sparse terminal rewards for optimization\n        if ctx.current_step &lt; ctx.max_steps - 1:\n            return 0.0\n        return ctx.new_portfolio_value - ctx.old_portfolio_value\n\n    def on_episode_end(self):\n        self.episode_count += 1\n\n# Note: You'll need to manually call on_episode_end in your training loop\n</code></pre>"},{"location":"guides/reward-functions/#debugging-rewards","title":"Debugging Rewards","text":""},{"location":"guides/reward-functions/#log-reward-statistics","title":"Log Reward Statistics","text":"<pre><code>def debug_reward(ctx: RewardContext) -&gt; float:\n    reward = my_reward_fn(ctx)\n\n    # Log statistics periodically\n    if ctx.current_step % 100 == 0:\n        print(f\"Step {ctx.current_step}:\")\n        print(f\"  Reward: {reward:.4f}\")\n        print(f\"  Portfolio: {ctx.new_portfolio_value:.2f}\")\n        print(f\"  Action: {ctx.action}\")\n\n    return reward\n</code></pre>"},{"location":"guides/reward-functions/#check-reward-distribution","title":"Check Reward Distribution","text":"<pre><code># Collect rewards during training\nrewards = []\n\n# After some episodes\nimport matplotlib.pyplot as plt\n\nplt.hist(rewards, bins=50)\nplt.title(\"Reward Distribution\")\nplt.xlabel(\"Reward\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# Check statistics\nprint(f\"Mean: {np.mean(rewards):.4f}\")\nprint(f\"Std: {np.std(rewards):.4f}\")\nprint(f\"Min: {np.min(rewards):.4f}\")\nprint(f\"Max: {np.max(rewards):.4f}\")\n</code></pre> <p>Good distributions: - Not constant: Should vary with performance - Reasonable range: Typically [-10, 10] for log returns - Some positive values: Agent can achieve rewards</p> <p>Bad patterns: - All zeros: Reward function may have bug - Extreme outliers: May need clipping - Constant value: Reward not discriminating actions</p>"},{"location":"guides/reward-functions/#references","title":"References","text":""},{"location":"guides/reward-functions/#research-papers-on-reward-design","title":"Research Papers on Reward Design","text":"<ul> <li>Reward Shaping for Reinforcement Learning in Financial Trading - Comprehensive study on reward function design for trading applications</li> <li>Deep Reinforcement Learning for Trading - Research on RL approaches and reward engineering for financial markets</li> <li>Reward Function Design in Deep Reinforcement Learning for Financial Trading - Analysis of different reward formulations and their impact on trading performance</li> </ul>"},{"location":"guides/reward-functions/#next-steps","title":"Next Steps","text":"<ul> <li>Feature Engineering - Engineer features that support your reward function</li> <li>Understanding the Sampler - How data flows through environments</li> <li>Loss Functions - Training objectives that work with rewards</li> <li>Offline Environments - Apply custom rewards to environments</li> </ul>"},{"location":"guides/sampler/","title":"Understanding the Sampler","text":"<p>The <code>MarketDataObservationSampler</code> (found in <code>torchtrade/envs/offline/sampler.py</code>) handles multi-timeframe data sampling in TorchTrade's offline environments. It resamples high-frequency data (1-minute bars) to multiple timeframes and creates synchronized observation windows while preventing lookahead bias. This allows RL agents to observe market patterns across different time scales simultaneously, from short-term momentum to long-term trends.</p>"},{"location":"guides/sampler/#what-is-the-sampler","title":"What Is the Sampler?","text":"<p>The sampler:</p> <ol> <li>Resamples 1-minute OHLCV to multiple timeframes (5m, 15m, 1h)</li> <li>Applies feature preprocessing to each timeframe</li> <li>Creates sliding windows of market data</li> <li>Prevents lookahead bias by correct bar indexing</li> </ol> <pre><code>1-Minute Data \u2192 Sampler \u2192 Multi-Timeframe Observations\n                  \u251c\u2500\u2500 Resample to timeframes\n                  \u251c\u2500\u2500 Apply preprocessing\n                  \u2514\u2500\u2500 Create windows\n</code></pre>"},{"location":"guides/sampler/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/sampler/#how-it-works","title":"How It Works","text":"<p>The sampler takes your 1-minute OHLCV data, resamples it to multiple timeframes, and provides synchronized observation windows at each execution step. Here's a direct example of using the sampler:</p> <pre><code>import pandas as pd\nfrom torchtrade.envs.offline.sampler import MarketDataObservationSampler\nfrom torchtrade.envs.offline.utils import TimeFrame, TimeFrameUnit\n\n# Load your OHLCV data\ndf = pd.read_csv(\"btcusdt_1m.csv\")\n# Required columns: timestamp, open, high, low, close, volume\n\n# Create sampler\nsampler = MarketDataObservationSampler(\n    df=df,\n    time_frames=[\n        TimeFrame(1, TimeFrameUnit.Minute),\n        TimeFrame(5, TimeFrameUnit.Minute),\n        TimeFrame(15, TimeFrameUnit.Minute),\n    ],\n    window_sizes=[12, 8, 8],\n    execute_on=TimeFrame(5, TimeFrameUnit.Minute),\n)\n\n# Get observations\nobs_dict, timestamp, truncated = sampler.get_sequential_observation()\n\n# obs_dict contains:\n# {\n#   \"market_data_1Minute\": torch.Tensor([12, num_features]),\n#   \"market_data_5Minute\": torch.Tensor([8, num_features]),\n#   \"market_data_15Minute\": torch.Tensor([8, num_features]),\n# }\n\n# Reset for new episode\nsampler.reset(random_start=True)\n</code></pre>"},{"location":"guides/sampler/#usage-in-offline-environments","title":"Usage in Offline Environments","text":"<p>The sampler is used in all offline environments (SeqLongOnlyEnv, SeqFuturesEnv, etc.) and allows flexible selection of timeframes through the environment configuration:</p> <pre><code>from torchtrade.envs.offline import SeqLongOnlyEnv, SeqLongOnlyEnvConfig\n\n# Configure multi-timeframe sampling\nconfig = SeqLongOnlyEnvConfig(\n    time_frames=[\"1min\", \"5min\", \"15min\", \"1hour\"],\n    window_sizes=[12, 8, 8, 24],\n    execute_on=(5, \"Minute\"),\n)\n\nenv = SeqLongOnlyEnv(df, config)\n\n# Observations contain all timeframes\nobs = env.reset()\n# obs[\"market_data_1Minute\"]: (12, features)\n# obs[\"market_data_5Minute\"]: (8, features)\n# obs[\"market_data_15Minute\"]: (8, features)\n# obs[\"market_data_1Hour\"]: (24, features)\n</code></pre>"},{"location":"guides/sampler/#how-resampling-works","title":"How Resampling Works","text":""},{"location":"guides/sampler/#timeframe-alignment","title":"Timeframe Alignment","text":"<p>The sampler resamples your 1-minute OHLCV data to multiple timeframes (5min, 15min, 1hour, etc.) and ensures all observations are synchronized at each execution step.</p> <p>Example: execute_on=5Minute</p> <pre><code>Time (minutes):     0    5    10   15   20   25   30\n                    |----|----|----|----|----|----|\n1-minute bars:      60 bars available\n5-minute bars:      |  A  |  B  |  C  |  D  |  E  |  F  |\n15-minute bars:     |      X      |      Y      |      Z      |\n\nExecute at:              \u2191         \u2191         \u2191\n                        t=5      t=10      t=15\n</code></pre> <p>At t=10 (executing on 5-minute bar B): - 1-minute data: Last 12 bars (from recent history) - 5-minute data: Bar A (completed at t=5) - 15-minute data: Bar X (completed at t=0)</p>"},{"location":"guides/sampler/#lookahead-bias-prevention","title":"Lookahead Bias Prevention","text":"<p>The Problem: In real trading, you can't use a bar's data until it has fully closed. A 15-minute bar spanning 0-15 minutes isn't complete until minute 15.</p> <p>The Solution: The sampler indexes higher timeframe bars by their END time, not their START time:</p> <pre><code># Without fix (WRONG - causes lookahead bias):\n# 15-min bar covering [0-15] is indexed at t=0\n# At t=10, agent could see bar [0-15] before it closes at t=15 \u274c\n\n# With fix (CORRECT - in sampler.py lines 71-77):\n# 15-min bar covering [0-15] is indexed at t=15 (its END time)\n# At t=10, agent can only see bars that closed BEFORE t=10 \u2705\n</code></pre> <p>Detailed Example at t=10:</p> <p>When your agent executes at minute 10, here's what data is available:</p> <pre><code>\u2705 CAN use (completed bars only):\n  - 1-min bars: [1, 2, 3, ..., 9] (bar 10 is still forming)\n  - 5-min bar A [0-5]: Closed at t=5, fully complete\n  - 15-min bar covering previous period: Only if it ended before t=10\n\n\u274c CANNOT use (incomplete bars):\n  - 5-min bar B [5-10]: Still forming, closes at t=10\n  - 15-min bar X [0-15]: Still forming, closes at t=15\n</code></pre> <p>Why This Matters: Without this protection, your agent would train on future information (looking into bars that haven't closed yet), leading to unrealistic backtest results that won't work in live trading.</p> <p>Implementation Detail (from <code>sampler.py:71-77</code>):</p> <p>Higher timeframes (coarser than <code>execute_on</code>) are shifted forward by their period during resampling. This ensures bars are indexed by their END time. When the agent queries data at execution time, <code>searchsorted</code> automatically excludes any bars that haven't closed yet.</p>"},{"location":"guides/sampler/#common-configuration-patterns","title":"Common Configuration Patterns","text":"Pattern time_frames window_sizes execute_on Use Case Single Timeframe <code>[\"1min\"]</code> <code>[100]</code> <code>(1, \"Minute\")</code> High-frequency, simple features Multi-Timeframe <code>[\"1min\", \"5min\", \"15min\"]</code> <code>[12, 8, 8]</code> <code>(5, \"Minute\")</code> Capture multiple market rhythms Hierarchical <code>[\"1min\", \"5min\", \"15min\", \"60min\", \"240min\"]</code> <code>[12, 8, 8, 24, 48]</code> <code>(5, \"Minute\")</code> Complex strategies, trend analysis Long-Term <code>[\"60min\", \"240min\", \"1440min\"]</code> <code>[24, 24, 30]</code> <code>(60, \"Minute\")</code> Position trading, low frequency"},{"location":"guides/sampler/#key-parameters","title":"Key Parameters","text":"Parameter Type Description Example <code>time_frames</code> list[str] Timeframes as strings (e.g., \"1min\", \"5min\", \"1h\") <code>[\"1min\", \"5min\", \"15min\"]</code> <code>window_sizes</code> list[int] Lookback window per timeframe <code>[12, 8, 8]</code> <code>execute_on</code> tuple (value, \"Minute\"/\"Hour\") <code>(5, \"Minute\")</code> <code>feature_preprocessing_fn</code> callable Transform OHLCV before windowing <code>add_indicators</code>"},{"location":"guides/sampler/#window-size-selection","title":"Window Size Selection","text":"<p>Choose window sizes based on the information needed:</p> <p>For 1-minute timeframe: - 12 bars = 12 minutes of data (short-term) - 60 bars = 1 hour of data (medium-term) - 240 bars = 4 hours of data (long-term)</p> <p>For 5-minute timeframe: - 8 bars = 40 minutes - 12 bars = 1 hour - 24 bars = 2 hours</p> <p>Rule of thumb: Higher timeframes need fewer bars (they already capture more history per bar).</p>"},{"location":"guides/sampler/#common-issues","title":"Common Issues","text":"Issue Symptom Solution NaN values in observations Training crashes Fill NaN in <code>feature_preprocessing_fn</code> with <code>df.fillna(0)</code> Episode too short Episode ends after few steps Check data length covers <code>max(window_sizes) * max(time_frames) + episode_length</code> Misaligned timeframes Unexpected data patterns Use <code>execute_on</code> that's a multiple of all <code>time_frames</code> Memory issues OOM errors Reduce <code>window_sizes</code> or number of <code>time_frames</code> Slow sampling Environment init takes long Cache preprocessing results or simplify indicator calculations"},{"location":"guides/sampler/#performance-tips","title":"Performance Tips","text":""},{"location":"guides/sampler/#1-efficient-preprocessing","title":"1. Efficient Preprocessing","text":"<pre><code># \u274c Slow - recalculating indicators\ndef slow_preprocessing(df):\n    for i in range(len(df)):\n        df.loc[i, \"sma\"] = df[\"close\"][i-20:i].mean()\n    return df\n\n# \u2705 Fast - vectorized operations\ndef fast_preprocessing(df):\n    df[\"sma\"] = df[\"close\"].rolling(20).mean()\n    return df\n</code></pre>"},{"location":"guides/sampler/#2-appropriate-window-sizes","title":"2. Appropriate Window Sizes","text":"<p>Larger windows = more memory and computation:</p> <pre><code># Memory usage \u2248 batch_size \u00d7 num_envs \u00d7 sum(window_sizes) \u00d7 num_features \u00d7 4 bytes\n\n# Example: 32 batch \u00d7 8 envs \u00d7 (12+8+8) windows \u00d7 10 features \u00d7 4 bytes \u2248 290 KB\n</code></pre> <p>Keep <code>sum(window_sizes) \u00d7 num_features</code> reasonable (&lt; 1000 total values per observation).</p>"},{"location":"guides/sampler/#technical-reference","title":"Technical Reference","text":"<ul> <li>Source: <code>torchtrade/envs/offline/sampler.py</code></li> <li>Resampling Logic: Uses pandas <code>resample().agg()</code> with OHLCV aggregation rules</li> <li>Indexing: Execution times mapped to 1-minute bar indices, then resampled timeframes aligned</li> </ul>"},{"location":"guides/sampler/#next-steps","title":"Next Steps","text":"<ul> <li>Feature Engineering - Add technical indicators via preprocessing</li> <li>Reward Functions - Design rewards that work with your sampled data</li> <li>Offline Environments - Apply sampler configuration to environments</li> </ul>"}]}