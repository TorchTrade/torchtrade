{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#torchtrade-documentation","title":"TorchTrade Documentation","text":"<p>Welcome to the TorchTrade documentation! TorchTrade is a machine learning framework for algorithmic trading built on TorchRL.</p> <p>TorchTrade's goal is to provide accessible deployment of RL methods to trading. The framework supports various RL methodologies including online RL, offline RL, model-based RL, contrastive learning, and many more areas of reinforcement learning research. Beyond RL, TorchTrade integrates traditional trading methods such as rule-based strategies, as well as modern approaches including LLMs (both local models and frontier model integrations) as trading actors.</p>"},{"location":"#what-is-torchtrade","title":"What is TorchTrade?","text":"<p>TorchTrade provides modular environments for both live trading with major exchanges and offline backtesting. The framework supports:</p> <ul> <li>\ud83c\udfaf Multi-Timeframe Observations - Train on 1m, 5m, 15m, 1h bars simultaneously</li> <li>\ud83e\udd16 Multiple RL Algorithms - PPO, DQN, IQL, GRPO, DSAC implementations</li> <li>\ud83d\udcca Feature Engineering - Add technical indicators and custom features</li> <li>\ud83d\udd34 Live Trading - Direct API integration with major exchanges</li> <li>\ud83d\udcc9 Risk Management - Stop-loss/take-profit, margin, leverage, liquidation mechanics</li> <li>\ud83d\udd2e Futures Trading - Up to 125x leverage with proper margin management</li> <li>\ud83d\udce6 Ready-to-Use Datasets - Pre-processed OHLCV data available at HuggingFace/Torch-Trade</li> </ul>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation &amp; Setup - Get up and running in minutes</li> <li>First Environment - Create and run your first trading environment</li> <li>First Training Run - Train a PPO policy</li> </ul>"},{"location":"#environments","title":"Environments","text":"<ul> <li>Offline Environments - Backtesting with historical data<ul> <li>SequentialTradingEnv, SequentialTradingEnvSLTP, OneStepTradingEnv</li> </ul> </li> <li>Online Environments - Live trading with exchange APIs<ul> <li>Alpaca, Binance, Bitget integrations</li> </ul> </li> </ul>"},{"location":"#components","title":"Components","text":"<ul> <li>Loss Functions - Training objectives (GRPOLoss, CTRLLoss, CTRLPPOLoss)</li> <li>Transforms - Data preprocessing (CoverageTracker, ChronosEmbeddingTransform)</li> <li>Actors - Trading policies (RuleBasedActor, FrontierLLMActor, LocalLLMActor)</li> </ul>"},{"location":"#advanced-customization","title":"Advanced Customization","text":"<ul> <li>Feature Engineering - Add technical indicators and features</li> <li>Reward Functions - Design reward functions for your strategy</li> <li>Performance Metrics - Evaluate and customize trading performance metrics</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#multi-timeframe-support","title":"Multi-Timeframe Support","text":"<p>Observe market data at multiple time scales simultaneously:</p> <pre><code>config = SequentialTradingEnvConfig(\n    time_frames=[\"1min\", \"5min\", \"15min\", \"60min\"],\n    window_sizes=[12, 8, 8, 24],       # Lookback per timeframe\n    execute_on=(5, \"Minute\")           # Execute every 5 minutes\n)\n</code></pre>"},{"location":"#futures-trading-with-leverage","title":"Futures Trading with Leverage","text":"<p>Trade with leverage and manage margin:</p> <pre><code>config = SequentialTradingEnvConfig(\n    leverage=10,                       # 10x leverage\n    initial_cash=10000,\n    margin_call_threshold=0.2,         # 20% margin ratio triggers liquidation\n)\n</code></pre>"},{"location":"#stop-loss-take-profit-bracket-orders","title":"Stop-Loss / Take-Profit Bracket Orders","text":"<p>Risk management with combinatorial action spaces:</p> <pre><code>config = SequentialTradingEnvSLTPConfig(\n    stoploss_levels=[-0.02, -0.05],    # -2%, -5%\n    takeprofit_levels=[0.05, 0.10],    # +5%, +10%\n    include_hold_action=True,          # Optional: set False to remove HOLD\n)\n# Action space: HOLD + (2 SL \u00d7 2 TP) = 5 actions (or 4 without HOLD)\n</code></pre>"},{"location":"#environment-comparison","title":"Environment Comparison","text":""},{"location":"#offline-environments-backtesting","title":"Offline Environments (Backtesting)","text":"<p>All environments support both spot and futures trading via config (<code>leverage=1</code> for spot, <code>leverage&gt;1</code> for futures with margin/liquidation mechanics).</p> Environment Bracket Orders One-Step Best For SequentialTradingEnv \u274c \u274c Standard sequential trading SequentialTradingEnvSLTP \u2705 \u274c Risk management with SL/TP OneStepTradingEnv \u2705 \u2705 GRPO, contextual bandits"},{"location":"#live-environments-exchange-apis","title":"Live Environments (Exchange APIs)","text":"Environment Exchange Futures Leverage Bracket Orders AlpacaTorchTradingEnv Alpaca \u274c \u274c \u274c AlpacaSLTPTorchTradingEnv Alpaca \u274c \u274c \u2705 BinanceFuturesTorchTradingEnv Binance \u2705 \u2705 \u274c BinanceFuturesSLTPTorchTradingEnv Binance \u2705 \u2705 \u2705 BitgetFuturesTorchTradingEnv Bitget \u2705 \u2705 \u274c BitgetFuturesSLTPTorchTradingEnv Bitget \u2705 \u2705 \u2705"},{"location":"#next-steps","title":"Next Steps","text":"<p>Getting Started Guide - Install TorchTrade and run your first environment.</p>"},{"location":"getting-started/","title":"Getting Started with TorchTrade","text":"<p>This guide will help you install TorchTrade and run your first trading environment.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>CUDA (optional, for GPU acceleration)</li> <li>UV - Fast Python package installer</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#1-install-uv","title":"1. Install UV","text":"<p>UV is a fast Python package installer and environment manager:</p> <pre><code># On Unix/macOS\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# On Windows (PowerShell)\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre>"},{"location":"getting-started/#2-clone-the-repository","title":"2. Clone the Repository","text":"<pre><code>git clone https://github.com/TorchTrade/torchtrade.git\ncd torchtrade\n</code></pre>"},{"location":"getting-started/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code># Install TorchTrade and all dependencies\nuv sync\n\n# Optional: Install with extra features\nuv sync --extra llm              # LLM actors (OpenAI API + local vLLM/transformers)\nuv sync --extra chronos          # Chronos forecasting transforms\nuv sync --all-extras             # Install all optional dependencies\n\n# Activate the virtual environment\nsource .venv/bin/activate  # On Unix/macOS\n# or\n.venv\\Scripts\\activate  # On Windows\n</code></pre>"},{"location":"getting-started/#4-verify-installation","title":"4. Verify Installation","text":"<pre><code># Run tests to verify everything works\nuv run pytest tests/ -v\n</code></pre>"},{"location":"getting-started/#your-first-environment","title":"Your First Environment","text":"<p>Let's create a simple trading environment using historical OHLCV data.</p>"},{"location":"getting-started/#step-1-prepare-your-data","title":"Step 1: Prepare Your Data","text":"<p>TorchTrade expects OHLCV data with the following columns:</p> <pre><code>import pandas as pd\n\n# Your data should have these columns\ndf = pd.DataFrame({\n    'timestamp': [...],  # datetime or parseable strings\n    'open': [...],       # opening prices\n    'high': [...],       # high prices\n    'low': [...],        # low prices\n    'close': [...],      # closing prices\n    'volume': [...]      # trading volume\n})\n</code></pre> <p>Note: You can also use our pre-processed datasets from HuggingFace/Torch-Trade which include various cryptocurrency pairs with 1-minute OHLCV data.</p>"},{"location":"getting-started/#step-2-create-an-environment","title":"Step 2: Create an Environment","text":"<pre><code>from torchtrade.envs.offline import SequentialTradingEnv, SequentialTradingEnvConfig\nimport pandas as pd\n\n# Load your data\ndf = pd.read_csv(\"btcusdt_1m.csv\")\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n\n# Configure environment\nconfig = SequentialTradingEnvConfig(\n    time_frames=[\"1min\", \"5min\", \"15min\"],        # 1m, 5m, 15m bars\n    window_sizes=[12, 8, 8],       # Lookback windows\n    execute_on=(5, \"Minute\"),      # Execute every 5 minutes\n    initial_cash=1000,             # Starting capital\n    transaction_fee=0.0025,        # 0.25% fee\n    slippage=0.001                 # 0.1% slippage\n)\n\n# Create environment\nenv = SequentialTradingEnv(df, config)\n</code></pre>"},{"location":"getting-started/#training-your-first-policy","title":"Training Your First Policy","text":""},{"location":"getting-started/#quick-training-run","title":"Quick Training Run","text":"<pre><code># Train PPO with default config\nuv run python examples/online_rl/ppo/train.py\n\n# Customize with Hydra overrides\nuv run python examples/online_rl/ppo/train.py \\\n    env.symbol=\"BTC/USD\" \\\n    optim.lr=1e-4 \\\n    loss.gamma=0.95\n</code></pre> <p>See the Examples page for all available algorithms (PPO, DQN, IQL, DSAC, GRPO) and usage guides.</p>"},{"location":"getting-started/#common-use-cases","title":"Common Use Cases","text":""},{"location":"getting-started/#loading-historical-data-from-huggingface","title":"Loading Historical Data from HuggingFace","text":"<pre><code>import datasets\n\n# Load pre-processed Bitcoin data\nds = datasets.load_dataset(\"Torch-Trade/btcusdt_spot_1m_01_2020_to_12_2025\")\ndf = ds[\"train\"].to_pandas()\ndf['0'] = pd.to_datetime(df['0'])  # First column is timestamp\ndf.columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n\n# Create environment\nenv = SequentialTradingEnv(df, config)\n</code></pre>"},{"location":"getting-started/#multi-timeframe-configuration","title":"Multi-Timeframe Configuration","text":"<pre><code>config = SequentialTradingEnvConfig(\n    time_frames=[\"1min\", \"5min\", \"15min\", \"60min\"],        # 1m, 5m, 15m, 1h\n    window_sizes=[12, 8, 8, 24],       # Lookback per timeframe\n    execute_on=(5, \"Minute\"),          # Execute every 5 minutes\n    initial_cash=[1000, 5000],         # Domain randomization\n)\n</code></pre> <p>The environment will provide observations: - <code>market_data_1Minute</code>: [12, num_features] - Last 12 one-minute bars - <code>market_data_5Minute</code>: [8, num_features] - Last 40 minutes - <code>market_data_15Minute</code>: [8, num_features] - Last 120 minutes - <code>market_data_60Minute</code>: [24, num_features] - Last 24 hours</p>"},{"location":"getting-started/#using-stop-loss-take-profit","title":"Using Stop-Loss / Take-Profit","text":"<pre><code>from torchtrade.envs.offline import SequentialTradingEnvSLTP, SequentialTradingEnvSLTPConfig\n\nconfig = SequentialTradingEnvSLTPConfig(\n    stoploss_levels=[-0.02, -0.05],     # -2%, -5%\n    takeprofit_levels=[0.05, 0.10],     # +5%, +10%\n    time_frames=[\"1min\", \"5min\", \"15min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n    initial_cash=1000\n)\n\nenv = SequentialTradingEnvSLTP(df, config)\n\n# Action space: 1 (HOLD) + 2\u00d72 (SL/TP combinations) = 5 actions\n# Action 0: HOLD\n# Action 1: BUY with SL=-2%, TP=+5%\n# Action 2: BUY with SL=-2%, TP=+10%\n# Action 3: BUY with SL=-5%, TP=+5%\n# Action 4: BUY with SL=-5%, TP=+10%\n</code></pre>"},{"location":"getting-started/#live-trading-setup","title":"Live Trading Setup","text":"<p>For live trading with real exchanges, you'll need API credentials.</p>"},{"location":"getting-started/#alpaca-us-stocks-crypto","title":"Alpaca (US Stocks &amp; Crypto)","text":"<p>Alpaca offers commission-free paper trading for testing strategies without risk. See Alpaca Paper Trading Docs for API credentials setup.</p> <pre><code># Create .env file\ncat &gt; .env &lt;&lt; EOF\nAPI_KEY=your_alpaca_api_key\nSECRET_KEY=your_alpaca_secret_key\nEOF\n</code></pre> <pre><code>from torchtrade.envs.alpaca import AlpacaTorchTradingEnv, AlpacaTradingEnvConfig\n\nconfig = AlpacaTradingEnvConfig(\n    symbol=\"BTC/USD\",\n    time_frames=[\"1Min\", \"5Min\"],\n    window_sizes=[12, 8],\n    execute_on=\"5Min\",\n    paper=True  # Start with paper trading!\n)\n\nenv = AlpacaTorchTradingEnv(config)\n</code></pre>"},{"location":"getting-started/#binance-crypto-futures","title":"Binance (Crypto Futures)","text":"<p>If you want to trade on Binance, register here in case you have no account. Binance also allows for demo trading, see here.</p> <pre><code># Add to .env\nBINANCE_API_KEY=your_binance_api_key\nBINANCE_SECRET_KEY=your_binance_secret_key\n</code></pre> <pre><code>from torchtrade.envs.binance import (\n    BinanceFuturesTorchTradingEnv,\n    BinanceFuturesTradingEnvConfig\n)\n\nconfig = BinanceFuturesTradingEnvConfig(\n    symbol=\"BTCUSDT\",\n    intervals=[\"1m\", \"5m\"],\n    window_sizes=[12, 8],\n    execute_on=\"1m\",\n    leverage=5,                        # 5x leverage\n    quantity_per_trade=0.01,\n    demo=True,                         # Use testnet\n)\n\nenv = BinanceFuturesTorchTradingEnv(config)\n</code></pre> <p>Note: Alpaca and Binance are just two examples of live environments/brokers that TorchTrade supports. For more details on all available exchanges and configurations, see Online Environments. We're always open to including additional brokers - if you'd like to request support for a new exchange, please create an issue or contact us directly at torchtradecontact@gmail.com.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Offline Environments - Deep dive into backtesting environments</li> <li>Online Environments - Live trading with exchange APIs</li> <li>Examples - Training scripts for all algorithms</li> </ul> Troubleshooting <ul> <li>\"No module named 'torchtrade'\" \u2192 Activate the venv: <code>source .venv/bin/activate</code></li> <li>\"CUDA out of memory\" \u2192 Reduce <code>frames_per_batch</code> or set <code>device: \"cpu\"</code></li> <li>\"Columns do not match required format\" \u2192 Ensure columns are <code>['timestamp', 'open', 'high', 'low', 'close', 'volume']</code></li> <li>\"Environment returns NaN rewards\" \u2192 Check for NaN/zero prices: <code>df = df.dropna()</code></li> </ul>"},{"location":"components/actors/","title":"Trading Actors","text":"<p>Trading actors implement the policy interface for TorchTrade environments. Beyond standard neural network policies, TorchTrade provides rule-based strategies and LLM-powered agents.</p>"},{"location":"components/actors/#available-actors","title":"Available Actors","text":"Actor Type Use Case RuleBasedActor Deterministic Strategy Baselines, debugging, research benchmarks MeanReversionActor Rule-Based (Bollinger + Stoch RSI) Ranging markets, baseline comparisons FrontierLLMActor LLM (API) Research, rapid prototyping with GPT/Claude LocalLLMActor LLM (Local inference) Production, privacy, cost efficiency"},{"location":"components/actors/#rulebasedactor","title":"RuleBasedActor","text":"<p>Abstract base class for deterministic trading strategies. Follows a two-phase pattern: preprocess (compute indicators on full dataset upfront) then decide (extract features and apply rules at each step).</p> <pre><code>from torchtrade.actor.rulebased.base import RuleBasedActor\n\nclass MyStrategy(RuleBasedActor):\n    def get_preprocessing_fn(self):\n        def preprocess(df):\n            df[\"features_sma_20\"] = df[\"close\"].rolling(20).mean()\n            df[\"features_rsi_14\"] = compute_rsi(df[\"close\"], 14)\n            return df\n        return preprocess\n\n    def select_action(self, observation):\n        data = self.extract_market_data(observation)\n        sma = self.get_feature(data, \"features_sma_20\")[-1]\n        rsi = self.get_feature(data, \"features_rsi_14\")[-1]\n\n        if rsi &lt; 30 and price &lt; sma:\n            return 2  # Buy\n        elif rsi &gt; 70 and price &gt; sma:\n            return 0  # Sell\n        return 1  # Hold\n</code></pre>"},{"location":"components/actors/#meanreversionactor","title":"MeanReversionActor","text":"<p>Concrete implementation using Bollinger Bands and Stochastic RSI. Buys when price is below lower band with oversold Stoch RSI, sells when above upper band with overbought Stoch RSI.</p> Parameter Default Description <code>bb_window</code> 20 Bollinger Bands period <code>bb_std</code> 2.0 Bollinger Bands standard deviations <code>stoch_rsi_window</code> 14 Stochastic RSI period <code>oversold_threshold</code> 20.0 Stoch RSI oversold level <code>overbought_threshold</code> 80.0 Stoch RSI overbought level <p>See <code>examples/rule_based/</code> for offline and live usage examples.</p>"},{"location":"components/actors/#frontierllmactor","title":"FrontierLLMActor","text":"<p>LLM-based actor using frontier model APIs (OpenAI, Anthropic, etc.) for trading decisions. Constructs prompts from market data and account state, queries the LLM, and parses actions from structured <code>&lt;think&gt;...&lt;answer&gt;</code> responses.</p> Parameter Default Description <code>model</code> <code>\"gpt-5-nano\"</code> Model identifier <code>symbol</code> <code>\"BTC/USD\"</code> Trading symbol for prompt context <code>action_dict</code> <code>{\"buy\": 2, \"sell\": 0, \"hold\": 1}</code> Action name to index mapping <code>debug</code> <code>False</code> Print prompts and responses <pre><code>from torchtrade.actor import FrontierLLMActor\n\nactor = FrontierLLMActor(\n    market_data_keys=env.market_data_keys,\n    account_state=env.account_state,\n    model=\"gpt-4-turbo\",\n)\n\nobservation = env.reset()\noutput = actor(observation)  # Returns tensordict with \"action\" and \"thinking\"\n</code></pre> <p>Requires <code>OPENAI_API_KEY</code> in <code>.env</code>. See <code>examples/llm/frontier/</code> for offline and live examples.</p>"},{"location":"components/actors/#localllmactor","title":"LocalLLMActor","text":"<p>Local LLM-based actor using vLLM or transformers for inference. Same prompt interface as FrontierLLMActor but runs models locally.</p> Parameter Default Description <code>model</code> <code>\"Qwen/Qwen2.5-0.5B-Instruct\"</code> HuggingFace model ID <code>backend</code> <code>\"vllm\"</code> <code>\"vllm\"</code> (faster, CUDA) or <code>\"transformers\"</code> (portable) <code>quantization</code> <code>None</code> <code>None</code>, <code>\"4bit\"</code>, or <code>\"8bit\"</code> <code>max_tokens</code> <code>512</code> Maximum tokens to generate <code>temperature</code> <code>0.7</code> Sampling temperature <code>action_space_type</code> <code>\"standard\"</code> <code>\"standard\"</code>, <code>\"sltp\"</code>, or <code>\"futures_sltp\"</code> <pre><code>from torchtrade.actor import LocalLLMActor\n\nactor = LocalLLMActor(\n    model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n    backend=\"vllm\",\n    quantization=\"4bit\",\n)\n\noutput = actor(observation)\n</code></pre> <p>For SLTP environments, pass <code>action_space_type=\"sltp\"</code> and <code>action_map=env.action_map</code>. See <code>examples/llm/local/</code> for offline and live examples.</p> <pre><code>pip install torchtrade[llm]  # Installs vllm, transformers, bitsandbytes\n</code></pre>"},{"location":"components/actors/#see-also","title":"See Also","text":"<ul> <li>Examples: LLM Actors - Full example scripts</li> <li>Examples: Rule-Based Actors - Mean reversion examples</li> <li>TorchRL Actors - Neural network policies</li> </ul>"},{"location":"components/losses/","title":"Loss Functions","text":"<p>TorchTrade provides specialized loss functions for training RL trading agents, built on TorchRL's <code>LossModule</code> interface.</p>"},{"location":"components/losses/#available-loss-functions","title":"Available Loss Functions","text":"Loss Function Type Use Case GRPOLoss Policy Gradient One-step RL with SLTP environments CTRLLoss Representation Learning Self-supervised encoder training CTRLPPOLoss Combined Joint policy + representation learning <p>For standard multi-step RL (PPO, DQN, SAC, IQL), use TorchRL's built-in loss modules directly.</p>"},{"location":"components/losses/#grpoloss","title":"GRPOLoss","text":"<p>Group Relative Policy Optimization for one-step RL. Designed for <code>OneStepTradingEnv</code> where episodes are single decisions with SL/TP bracket orders. Normalizes advantages within each batch: <code>advantage = (reward - mean) / std</code>.</p> Parameter Default Description <code>actor_network</code> Required Policy network (ProbabilisticTensorDictSequential) <code>entropy_coeff</code> 0.01 Entropy regularization coefficient <code>epsilon_low</code> / <code>epsilon_high</code> 0.2 Clipping bounds for policy ratio <pre><code>from torchtrade.losses import GRPOLoss\n\nloss_module = GRPOLoss(actor_network=actor, entropy_coeff=0.01)\n\nfor batch in collector:\n    loss_td = loss_module(batch)\n    loss = loss_td[\"loss_objective\"] + loss_td[\"loss_entropy\"]\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>Paper: DeepSeekMath (arXiv:2402.03300) \u2014 Section 2.2</p>"},{"location":"components/losses/#ctrlloss","title":"CTRLLoss","text":"<p>Cross-Trajectory Representation Learning for self-supervised encoder training. Trains encoders to recognize behavioral similarity across trajectories without rewards, improving zero-shot generalization.</p> Parameter Default Description <code>encoder_network</code> Required Encoder that produces embeddings <code>embedding_dim</code> Required Dimension of encoder output <code>num_prototypes</code> 512 Learnable prototype vectors <code>sinkhorn_iters</code> 3 Sinkhorn-Knopp iterations <code>temperature</code> 0.1 Softmax temperature <code>myow_coeff</code> 1.0 MYOW loss coefficient <pre><code>from torchtrade.losses import CTRLLoss\n\nctrl_loss = CTRLLoss(\n    encoder_network=encoder,\n    embedding_dim=128,\n    num_prototypes=512,\n)\n\nfor batch in collector:\n    loss_td = ctrl_loss(batch)\n    loss_td[\"loss_ctrl\"].backward()\n    optimizer.step()\n</code></pre> <p>Paper: Cross-Trajectory Representation Learning (arXiv:2106.02193)</p>"},{"location":"components/losses/#ctrlppoloss","title":"CTRLPPOLoss","text":"<p>Combines ClipPPOLoss with CTRLLoss for joint policy and encoder training. The encoder learns useful representations while the policy learns to act.</p> <pre><code>from torchtrade.losses import CTRLLoss, CTRLPPOLoss\nfrom torchrl.objectives import ClipPPOLoss\n\ncombined_loss = CTRLPPOLoss(\n    ppo_loss=ClipPPOLoss(actor, critic),\n    ctrl_loss=CTRLLoss(encoder, embedding_dim=128),\n    ctrl_coeff=0.5,\n)\n\nfor batch in collector:\n    loss_td = combined_loss(batch)\n    total_loss = loss_td[\"loss_objective\"] + loss_td[\"loss_critic\"] + loss_td[\"loss_ctrl\"]\n    total_loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"components/losses/#see-also","title":"See Also","text":"<ul> <li>Examples - Training scripts using these losses</li> <li>Environments - Compatible environment types</li> <li>TorchRL Objectives - Built-in PPO, DQN, SAC, IQL losses</li> </ul>"},{"location":"components/transforms/","title":"Transforms","text":"<p>TorchRL transforms are composable modules that modify environment observations, rewards, or actions. TorchTrade extends TorchRL's transform system with domain-specific transforms for trading.</p>"},{"location":"components/transforms/#available-transforms","title":"Available Transforms","text":"Transform Purpose Use Case CoverageTracker Track dataset coverage during training Monitor exploration, detect overfitting ChronosEmbeddingTransform Embed time series with Chronos T5 models Replace raw OHLCV with learned representations"},{"location":"components/transforms/#coveragetracker","title":"CoverageTracker","text":"<p>Tracks which portions of the dataset are visited during training with random episode resets. Monitors both episode start diversity (reset coverage) and full trajectory coverage (state coverage). Auto-detects environment settings and only activates when <code>random_start=True</code>.</p> Metric Range Meaning <code>reset_coverage</code> [0, 1] Fraction of positions used as episode starts <code>state_coverage</code> [0, 1] Fraction of all states visited during episodes <code>reset_entropy</code> / <code>state_entropy</code> [0, log(N)] Uniformity of visit distribution <pre><code>from torchtrade.envs.transforms import CoverageTracker\nfrom torchrl.collectors import SyncDataCollector\n\n# Use as postproc in collector (NOT in environment transform chain)\ncoverage_tracker = CoverageTracker()\n\ncollector = SyncDataCollector(\n    env, policy,\n    frames_per_batch=1000,\n    total_frames=100000,\n    postproc=coverage_tracker,\n)\n\nfor batch in collector:\n    # ... train on batch ...\n    stats = coverage_tracker.get_coverage_stats()\n    if stats[\"enabled\"]:\n        logger.log({\n            \"train/reset_coverage\": stats[\"reset_coverage\"],\n            \"train/state_coverage\": stats[\"state_coverage\"],\n        })\n</code></pre>"},{"location":"components/transforms/#chronosembeddingtransform","title":"ChronosEmbeddingTransform","text":"<p>Embeds time series observations using pretrained Chronos T5 forecasting models. Replaces raw OHLCV data with learned representations. Model is lazy-loaded on first use.</p> Model Parameters Embedding Dim Memory chronos-t5-tiny 8M 512 ~1GB chronos-t5-mini 20M 512 ~2GB chronos-t5-small 46M 768 ~3GB chronos-t5-base 200M 768 ~5GB chronos-t5-large 710M 1024 ~12GB Parameter Default Description <code>in_keys</code> Required Market data keys to embed <code>out_keys</code> Required Output embedding keys <code>model_name</code> <code>\"amazon/chronos-t5-large\"</code> HuggingFace model ID <code>aggregation</code> <code>\"mean\"</code> <code>\"mean\"</code>, <code>\"max\"</code>, or <code>\"concat\"</code> for multi-feature embeddings <code>del_keys</code> <code>True</code> Remove input keys after transformation <pre><code>from torchrl.envs import TransformedEnv, Compose\nfrom torchtrade.envs.transforms import ChronosEmbeddingTransform\n\nenv = TransformedEnv(\n    base_env,\n    Compose(\n        ChronosEmbeddingTransform(\n            in_keys=[\"market_data_1Minute_12\"],\n            out_keys=[\"chronos_embedding\"],\n            model_name=\"amazon/chronos-t5-base\",\n            aggregation=\"mean\",\n        ),\n    )\n)\n# Observation: {\"market_data_1Minute_12\": (12, 5)} \u2192 {\"chronos_embedding\": (768,)}\n</code></pre> <pre><code>pip install git+https://github.com/amazon-science/chronos-forecasting.git\n</code></pre>"},{"location":"components/transforms/#see-also","title":"See Also","text":"<ul> <li>TorchRL Transforms - Built-in transforms (VecNorm, ActionMask, etc.)</li> <li>Feature Engineering - Manual feature engineering</li> <li>Example: PPO + Chronos - Training with Chronos embeddings</li> </ul>"},{"location":"environments/offline-rl/","title":"Offline RL","text":"<p>TorchTrade supports offline reinforcement learning, enabling agents to learn from pre-collected datasets without requiring live environment interaction during training.</p>"},{"location":"environments/offline-rl/#overview","title":"Overview","text":"<p>TorchTrade provides TensorDict-based datasets that can be loaded and used directly with TorchRL's replay buffer. These datasets are available for download from HuggingFace/Torch-Trade and contain pre-collected trading trajectories for offline RL research.</p> <p>Offline RL can be performed using datasets collected from two sources:</p> <ol> <li>Offline Environment Interactions - Collect trajectories by running policies in backtesting environments (SequentialTradingEnv, SequentialTradingEnvSLTP, etc.)</li> <li>Real Online Environment Interactions - Record actual trading data from live exchanges (Alpaca, Binance, Bitget)</li> </ol> <p>This approach is particularly valuable for: - Learning from expert demonstrations or historical trading data - Training without market risk or transaction costs - Developing policies when live interaction is expensive or dangerous - Bootstrapping learning before deploying to real markets</p>"},{"location":"environments/offline-rl/#example-iql-implicit-q-learning","title":"Example: IQL (Implicit Q-Learning)","text":"<p>TorchTrade provides an example implementation of offline RL using Implicit Q-Learning (IQL) in <code>examples/offline/iql/</code>.</p> <pre><code># Example: Training IQL on pre-collected dataset\nfrom torchtrade.envs.offline import SequentialTradingEnv, SequentialTradingEnvConfig\nfrom torchrl.collectors import SyncDataCollector\nfrom torchrl.data import LazyTensorStorage, TensorDictReplayBuffer\n\n# 1. Create environment (for evaluation only)\nenv = SequentialTradingEnv(df, config)\n\n# 2. Load pre-collected dataset\n# Dataset should contain trajectories: (observation, action, reward, next_observation, done)\nreplay_buffer = TensorDictReplayBuffer(\n    storage=LazyTensorStorage(max_size=1_000_000),\n)\n\n# 3. Train IQL from offline data\nfor batch in replay_buffer:\n    loss = iql_loss_module(batch)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>For a complete implementation, see examples/offline/iql/.</p>"},{"location":"environments/offline-rl/#dataset-collection","title":"Dataset Collection","text":""},{"location":"environments/offline-rl/#from-offline-environments","title":"From Offline Environments","text":"<pre><code>from torchtrade.envs.offline import SequentialTradingEnv, SequentialTradingEnvConfig\nfrom torchrl.collectors import SyncDataCollector\nfrom torchrl.data import LazyTensorStorage, TensorDictReplayBuffer\n\n# Collect trajectories with any policy (random, rule-based, pre-trained)\ncollector = SyncDataCollector(\n    env,\n    policy,\n    frames_per_batch=10000,\n    total_frames=1_000_000,\n)\n\n# Store in replay buffer\nreplay_buffer = TensorDictReplayBuffer(\n    storage=LazyTensorStorage(max_size=1_000_000),\n)\n\nfor batch in collector:\n    replay_buffer.extend(batch)\n</code></pre>"},{"location":"environments/offline-rl/#from-real-online-environments","title":"From Real Online Environments","text":"<pre><code>from torchtrade.envs.alpaca import AlpacaTorchTradingEnv, AlpacaTradingEnvConfig\n\n# Collect real trading data (paper trading recommended)\nenv = AlpacaTorchTradingEnv(config)\n\n# Record interactions\nfor episode in range(num_episodes):\n    td = env.reset()\n    while not td[\"done\"].item():\n        action = policy(td)\n        td = env.step(td)\n        replay_buffer.add(td)\n</code></pre>"},{"location":"environments/offline-rl/#provided-datasets","title":"Provided Datasets","text":"<p>Coming Soon</p> <p>We plan to provide pre-collected datasets on HuggingFace for offline RL research, including:</p> <ul> <li>Expert demonstrations from rule-based strategies</li> <li>Random policy trajectories for benchmarking</li> <li>Real market interaction data (paper trading)</li> </ul> <p>Stay tuned at HuggingFace/Torch-Trade!</p>"},{"location":"environments/offline-rl/#additional-offline-rl-algorithms","title":"Additional Offline RL Algorithms","text":"<p>TorchTrade's offline RL support is compatible with any offline RL algorithm from TorchRL, including:</p> <ul> <li>CQL (Conservative Q-Learning) - Addresses overestimation in offline Q-learning</li> <li>TD3+BC - Combines TD3 with behavior cloning for offline learning</li> <li>Decision Transformer - Sequence modeling approach to offline RL</li> <li>Any TorchRL algorithm - Use replay buffers with offline data</li> </ul>"},{"location":"environments/offline-rl/#next-steps","title":"Next Steps","text":"<ul> <li>IQL Example - Complete offline RL implementation</li> <li>Offline Environments - Environments for dataset collection</li> <li>Online Environments - Live trading for data collection</li> <li>Examples - Browse all training examples</li> </ul>"},{"location":"environments/offline-rl/#references","title":"References","text":"<ul> <li>IQL Paper - Implicit Q-Learning algorithm</li> <li>TorchRL Replay Buffers - Data storage and sampling</li> <li>Offline RL Guide - Comprehensive offline RL guide</li> </ul>"},{"location":"environments/offline/","title":"Offline Environments","text":"<p>Offline environments are designed for training on historical data (backtesting). These are not \"offline RL\" methods like CQL or IQL, but rather environments that use pre-collected market data instead of live exchange APIs. For deploying trained policies to real exchanges, see Online Environments.</p>"},{"location":"environments/offline/#unified-architecture","title":"Unified Architecture","text":"<p>TorchTrade provides 3 unified environment classes that each support both spot and futures trading via configuration. Set <code>leverage=1</code> for spot (long-only) or <code>leverage&gt;1</code> for futures (with margin management and liquidation mechanics). Use negative <code>action_levels</code> to enable short positions.</p> Environment Bracket Orders One-Step Best For SequentialTradingEnv - - Standard sequential trading SequentialTradingEnvSLTP Yes - Risk management with SL/TP OneStepTradingEnv Yes Yes GRPO, contextual bandits <p>Sequential (<code>SequentialTradingEnv</code>) \u2014 Step-by-step trading with fractional position sizing. Action values represent the fraction of capital to deploy (e.g., 0.5 = 50% allocation).</p> <p>SL/TP (<code>SequentialTradingEnvSLTP</code>) \u2014 Extends sequential with bracket order risk management. Each trade includes configurable stop-loss and take-profit levels with a combinatorial action space.</p> <p>OneStep (<code>OneStepTradingEnv</code>) \u2014 Optimized for fast episodic training with GRPO. The agent takes one action, and the environment simulates a rollout until SL/TP triggers or max rollout length. Policies can be deployed to <code>SequentialTradingEnvSLTP</code> for step-by-step execution.</p> <p>Extensible Framework</p> <p>Users can create custom environments by inheriting from existing base classes. See Building Custom Environments.</p>"},{"location":"environments/offline/#account-state","title":"Account State","text":"<p>All environments expose a universal 6-element <code>account_state</code> tensor as part of the observation:</p> Index Element Description Spot Futures 0 <code>exposure_pct</code> Position value / portfolio value 0.0\u20131.0 0.0\u2013N (with leverage) 1 <code>position_direction</code> Sign of position size 0 or +1 -1, 0, or +1 2 <code>unrealized_pnl_pct</code> Unrealized P&amp;L as % of entry price \u22650 Any 3 <code>holding_time</code> Steps since position opened \u22650 \u22650 4 <code>leverage</code> Current leverage 1.0 1\u2013125 5 <code>distance_to_liquidation</code> Normalized distance to liquidation price 1.0 (no risk) Calculated <p>This structure is shared across offline and online environments, ensuring policies transfer seamlessly between training and live deployment.</p>"},{"location":"environments/offline/#fractional-position-sizing","title":"Fractional Position Sizing","text":"<p><code>SequentialTradingEnv</code> uses <code>action_levels</code> to define discrete fractional position sizes in [-1.0, 1.0]:</p> <ul> <li>Magnitude = fraction of balance to allocate (0.5 = 50%, 1.0 = 100%)</li> <li>Sign = direction (positive = long, negative = short, zero = flat/close)</li> <li>With leverage: position size = <code>balance \u00d7 |action| \u00d7 leverage / price</code></li> </ul> <pre><code>action_levels = [-1.0, 0.0, 1.0]              # Coarse: full short/flat/full long\naction_levels = [0.0, 0.25, 0.5, 0.75, 1.0]   # Long-only with granularity\naction_levels = [-0.5, -0.25, 0.0, 0.25, 0.5]  # Conservative, no full positions\n</code></pre> <p>SLTP environments use <code>include_hold_action</code> (default <code>True</code>) to optionally include a HOLD/no-op action alongside the SL/TP bracket combinations.</p>"},{"location":"environments/offline/#leverage","title":"Leverage","text":"<p>Leverage is a fixed global parameter, not part of the action space. <code>action=0.5</code> always means \"deploy 50% of capital\" regardless of leverage setting. This keeps the action space small and separates risk management (leverage) from the learned policy (position sizing).</p> <p>Timeframe Format</p> <p>Always use canonical forms: <code>[\"1min\", \"5min\", \"15min\", \"1hour\", \"1day\"]</code>. Non-canonical forms like <code>\"60min\"</code> create different observation keys (<code>market_data_60Minute</code> vs <code>market_data_1Hour</code>), breaking model compatibility.</p>"},{"location":"environments/offline/#sequentialtradingenv","title":"SequentialTradingEnv","text":"<p>The core sequential trading environment. Trading mode is determined by configuration: <code>leverage=1</code> for spot, <code>leverage&gt;1</code> for futures.</p>"},{"location":"environments/offline/#configuration","title":"Configuration","text":"Spot TradingFutures Trading <pre><code>from torchtrade.envs.offline import SequentialTradingEnv, SequentialTradingEnvConfig\n\nconfig = SequentialTradingEnvConfig(\n    time_frames=[\"1min\", \"5min\", \"15min\", \"1hour\"],\n    window_sizes=[12, 8, 8, 24],\n    execute_on=(5, \"Minute\"),\n    action_levels=[0.0, 0.5, 1.0],    # Close / 50% / 100% long\n    initial_cash=1000,\n    transaction_fee=0.0025,\n    slippage=0.001,\n)\n\nenv = SequentialTradingEnv(df, config)\n</code></pre> <pre><code>from torchtrade.envs.offline import SequentialTradingEnv, SequentialTradingEnvConfig\n\nconfig = SequentialTradingEnvConfig(\n    time_frames=[\"1min\", \"5min\", \"15min\", \"1hour\"],\n    window_sizes=[12, 8, 8, 24],\n    execute_on=(5, \"Minute\"),\n    leverage=5,\n    action_levels=[-1.0, -0.5, 0.0, 0.5, 1.0],  # Short/neutral/long\n    initial_cash=10000,\n    transaction_fee=0.0004,\n    slippage=0.001,\n)\n\nenv = SequentialTradingEnv(df, config)\n</code></pre>"},{"location":"environments/offline/#observation-space","title":"Observation Space","text":"<pre><code>observation = {\n    \"market_data_1Minute\": Tensor([12, num_features]),    # 1m window\n    \"market_data_5Minute\": Tensor([8, num_features]),     # 5m window\n    \"market_data_15Minute\": Tensor([8, num_features]),    # 15m window\n    \"market_data_1Hour\": Tensor([24, num_features]),      # 1h window\n    \"account_state\": Tensor([6]),                         # See Account State above\n}\n</code></pre>"},{"location":"environments/offline/#liquidation-futures","title":"Liquidation (Futures)","text":"<p>When <code>leverage &gt; 1</code>, positions are liquidated if margin is insufficient. E.g., with $10k cash at 10x leverage, a 20% loss exceeds equity and triggers liquidation.</p>"},{"location":"environments/offline/#sequentialtradingenvsltp","title":"SequentialTradingEnvSLTP","text":"<p>Extends <code>SequentialTradingEnv</code> with bracket order risk management. Supports both spot and futures modes via <code>leverage</code>.</p>"},{"location":"environments/offline/#configuration_1","title":"Configuration","text":"<pre><code>from torchtrade.envs.offline import SequentialTradingEnvSLTP, SequentialTradingEnvSLTPConfig\n\nconfig = SequentialTradingEnvSLTPConfig(\n    stoploss_levels=[-0.02, -0.05],\n    takeprofit_levels=[0.05, 0.10],\n    include_hold_action=True,\n\n    # Futures parameters (leverage &gt; 1 enables short bracket orders)\n    leverage=5,\n    margin_call_threshold=0.2,\n\n    time_frames=[\"1min\", \"5min\", \"15min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n    initial_cash=10000,\n    transaction_fee=0.0004,\n    slippage=0.001,\n)\n\nenv = SequentialTradingEnvSLTP(df, config)\n</code></pre>"},{"location":"environments/offline/#action-space","title":"Action Space","text":"<p>With 2 SL levels, 2 TP levels, and <code>leverage &gt; 1</code> (futures):</p> <ul> <li>Action 0: HOLD / Close position</li> <li>Actions 1-4: LONG with SL/TP combinations</li> <li>Actions 5-8: SHORT with SL/TP combinations</li> </ul> <p>Formula: <code>1 + 2 \u00d7 (num_sl \u00d7 num_tp)</code> = 9 actions. Without HOLD (<code>include_hold_action=False</code>): <code>2 \u00d7 (num_sl \u00d7 num_tp)</code> = 8 actions.</p>"},{"location":"environments/offline/#onesteptradingenv","title":"OneStepTradingEnv","text":"<p>One-step episodic environment for GRPO and contextual bandits. The agent takes a single action, and the environment simulates a rollout until SL/TP triggers or max rollout length. Supports spot and futures via <code>leverage</code>.</p> <p>Deployment</p> <p>Policies trained on <code>OneStepTradingEnv</code> can be deployed directly to <code>SequentialTradingEnvSLTP</code> \u2014 both share the same observation and action spaces.</p>"},{"location":"environments/offline/#configuration_2","title":"Configuration","text":"<pre><code>from torchtrade.envs.offline import OneStepTradingEnv, OneStepTradingEnvConfig\n\nconfig = OneStepTradingEnvConfig(\n    stoploss_levels=[-0.02, -0.05],\n    takeprofit_levels=[0.05, 0.10],\n    include_hold_action=True,\n    rollout_steps=24,\n\n    # Futures parameters (leverage &gt; 1 enables short bracket orders)\n    leverage=5,\n    margin_call_threshold=0.2,\n\n    time_frames=[\"1min\", \"5min\", \"15min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n    initial_cash=10000,\n    transaction_fee=0.0004,\n)\n\nenv = OneStepTradingEnv(df, config)\n</code></pre>"},{"location":"environments/offline/#visualization","title":"Visualization","text":"<p>All offline environments support <code>render_history()</code> to visualize episode performance:</p> <pre><code>env.render_history()  # Display after running an episode\nfig = env.render_history(return_fig=True)  # Or get the figure\n</code></pre> <p>All environments render 3 subplots: price + actions, portfolio vs buy-and-hold, and exposure history.</p> <p>See Visualization Guide for details.</p>"},{"location":"environments/offline/#next-steps","title":"Next Steps","text":"<ul> <li>Online Environments - Deploy to live exchanges</li> <li>Feature Engineering - Add technical indicators and custom rewards</li> </ul>"},{"location":"environments/online/","title":"Online Environments","text":"<p>Online environments connect to real trading APIs for paper trading or live execution. They provide the same TorchTrade interface as offline environments but fetch real-time market data from exchanges. Each offline environment has a corresponding live counterpart:</p> Offline (Training) Live (Deployment) SequentialTradingEnv (spot, <code>leverage=1</code>) AlpacaTorchTradingEnv SequentialTradingEnv (futures, <code>leverage&gt;1</code>) BinanceFutures / BitgetFuturesTorchTradingEnv SequentialTradingEnvSLTP (spot) AlpacaSLTPTorchTradingEnv SequentialTradingEnvSLTP (futures) BinanceFuturesSLTP / BitgetFuturesSLTPTorchTradingEnv OneStepTradingEnv (spot) AlpacaSLTPTorchTradingEnv OneStepTradingEnv (futures) BinanceFuturesSLTP / BitgetFuturesSLTPTorchTradingEnv <p>Supported Exchanges:</p> <ul> <li>Alpaca - Commission-free US stocks and crypto with paper trading</li> <li>Binance - Cryptocurrency futures with high leverage and testnet</li> <li>Bitget - Cryptocurrency futures with competitive fees and testnet</li> </ul>"},{"location":"environments/online/#overview","title":"Overview","text":"Environment Exchange Asset Type Futures Leverage Bracket Orders AlpacaTorchTradingEnv Alpaca Crypto/Stocks - - - AlpacaSLTPTorchTradingEnv Alpaca Crypto/Stocks - - Yes BinanceFuturesTorchTradingEnv Binance Crypto Yes Yes - BinanceFuturesSLTPTorchTradingEnv Binance Crypto Yes Yes Yes BitgetFuturesTorchTradingEnv Bitget Crypto Yes Yes - BitgetFuturesSLTPTorchTradingEnv Bitget Crypto Yes Yes Yes"},{"location":"environments/online/#fractional-position-sizing","title":"Fractional Position Sizing","text":"<p>Non-SLTP environments use <code>action_levels</code> for fractional position sizing \u2014 same as offline environments. Action values in [-1.0, 1.0] represent the fraction of balance to allocate. When adjusting positions (e.g., 100% to 50%), the environment only trades the delta, reducing transaction fees.</p> <p>Live environments use a query-first pattern: they query the actual exchange position (source of truth), calculate the target based on the action, round to exchange constraints (lot size, min notional), and trade only the delta.</p> <p>Timeframe Format - Critical for Model Compatibility</p> <p>Always use canonical timeframe forms:</p> <ul> <li>Alpaca: <code>[\"1Min\", \"5Min\", \"15Min\", \"1Hour\", \"1Day\"]</code></li> <li>Binance/Bitget: <code>[\"1m\", \"5m\", \"15m\", \"1h\", \"1d\"]</code></li> <li>Wrong: <code>[\"60min\"]</code>, <code>[\"60m\"]</code>, <code>[\"24hour\"]</code> \u2014 these create different observation keys and break model compatibility.</li> </ul>"},{"location":"environments/online/#alpaca-environments","title":"Alpaca Environments","text":"<p>Alpaca provides commission-free trading for US stocks and cryptocurrencies with paper trading support.</p>"},{"location":"environments/online/#alpacatorchtradingenv","title":"AlpacaTorchTradingEnv","text":"<pre><code>from torchtrade.envs.alpaca import AlpacaTorchTradingEnv, AlpacaTradingEnvConfig\n\nconfig = AlpacaTradingEnvConfig(\n    symbol=\"BTC/USD\",\n    time_frames=[\"1Min\", \"5Min\", \"15Min\", \"1Hour\"],\n    window_sizes=[12, 8, 8, 24],\n    execute_on=\"5Min\",\n    paper=True,  # Paper trading (recommended!)\n)\n\nenv = AlpacaTorchTradingEnv(config)\n</code></pre>"},{"location":"environments/online/#alpacasltptorchtradingenv","title":"AlpacaSLTPTorchTradingEnv","text":"<pre><code>from torchtrade.envs.alpaca import AlpacaSLTPTorchTradingEnv, AlpacaSLTPTradingEnvConfig\n\nconfig = AlpacaSLTPTradingEnvConfig(\n    symbol=\"BTC/USD\",\n    time_frames=[\"1Min\", \"5Min\", \"15Min\"],\n    window_sizes=[12, 8, 8],\n    execute_on=\"5Min\",\n    stoploss_levels=[-0.02, -0.05],\n    takeprofit_levels=[0.05, 0.10],\n    paper=True,\n)\n\nenv = AlpacaSLTPTorchTradingEnv(config)\n# Action space: HOLD + 4 SL/TP combinations = 5 actions\n</code></pre>"},{"location":"environments/online/#binance-environments","title":"Binance Environments","text":"<p>Binance provides cryptocurrency futures trading with up to 125x leverage and testnet support.</p>"},{"location":"environments/online/#binancefuturestorchtradingenv","title":"BinanceFuturesTorchTradingEnv","text":"<pre><code>from torchtrade.envs.binance import BinanceFuturesTorchTradingEnv, BinanceFuturesTradingEnvConfig\n\nconfig = BinanceFuturesTradingEnvConfig(\n    symbol=\"BTCUSDT\",\n    intervals=[\"1m\", \"5m\", \"15m\"],\n    window_sizes=[12, 8, 8],\n    execute_on=\"1m\",\n    leverage=5,\n    quantity_per_trade=0.01,\n    demo=True,  # Testnet (recommended!)\n)\n\nenv = BinanceFuturesTorchTradingEnv(config)\n</code></pre>"},{"location":"environments/online/#binancefuturessltptorchtradingenv","title":"BinanceFuturesSLTPTorchTradingEnv","text":"<pre><code>from torchtrade.envs.binance import BinanceFuturesSLTPTorchTradingEnv, BinanceFuturesSLTPTradingEnvConfig\n\nconfig = BinanceFuturesSLTPTradingEnvConfig(\n    symbol=\"BTCUSDT\",\n    intervals=[\"1m\", \"5m\"],\n    window_sizes=[12, 8],\n    execute_on=\"1m\",\n    stoploss_levels=[-0.02, -0.05],\n    takeprofit_levels=[0.03, 0.06, 0.10],\n    leverage=5,\n    quantity_per_trade=0.01,\n    demo=True,\n)\n\nenv = BinanceFuturesSLTPTorchTradingEnv(config)\n# Action space: HOLD + 2\u00d7(2 SL \u00d7 3 TP) = 13 actions (long + short)\n</code></pre>"},{"location":"environments/online/#bitget-environments","title":"Bitget Environments","text":"<p>Bitget provides cryptocurrency futures trading with competitive fees and testnet support. TorchTrade uses CCXT to interface with Bitget's V2 API.</p> <p>CCXT Symbol Format</p> <p>Bitget uses CCXT's perpetual swap format: <code>\"BTC/USDT:USDT\"</code> (not <code>\"BTCUSDT\"</code>).</p>"},{"location":"environments/online/#bitgetfuturestorchtradingenv","title":"BitgetFuturesTorchTradingEnv","text":"<pre><code>from torchtrade.envs.bitget import BitgetFuturesTorchTradingEnv, BitgetFuturesTradingEnvConfig\nfrom torchtrade.envs.bitget.futures_order_executor import MarginMode, PositionMode\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nconfig = BitgetFuturesTradingEnvConfig(\n    symbol=\"BTC/USDT:USDT\",\n    time_frames=[\"5min\", \"15min\"],\n    window_sizes=[6, 32],\n    execute_on=\"1min\",\n    product_type=\"USDT-FUTURES\",\n    leverage=5,\n    quantity_per_trade=0.002,\n    margin_mode=MarginMode.ISOLATED,     # ISOLATED (safer) or CROSSED\n    position_mode=PositionMode.ONE_WAY,  # ONE_WAY (simpler) or HEDGE\n    demo=True,\n)\n\nenv = BitgetFuturesTorchTradingEnv(\n    config,\n    api_key=os.getenv(\"BITGETACCESSAPIKEY\"),\n    api_secret=os.getenv(\"BITGETSECRETKEY\"),\n    api_passphrase=os.getenv(\"BITGETPASSPHRASE\"),\n)\n</code></pre>"},{"location":"environments/online/#bitgetfuturessltptorchtradingenv","title":"BitgetFuturesSLTPTorchTradingEnv","text":"<pre><code>from torchtrade.envs.bitget import BitgetFuturesSLTPTorchTradingEnv, BitgetFuturesSLTPTradingEnvConfig\nfrom torchtrade.envs.bitget.futures_order_executor import MarginMode, PositionMode\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nconfig = BitgetFuturesSLTPTradingEnvConfig(\n    symbol=\"BTC/USDT:USDT\",\n    time_frames=[\"5min\", \"15min\"],\n    window_sizes=[6, 32],\n    execute_on=\"1min\",\n    stoploss_levels=[-0.025, -0.05, -0.1],\n    takeprofit_levels=[0.05, 0.1, 0.2],\n    include_hold_action=True,\n    product_type=\"USDT-FUTURES\",\n    leverage=5,\n    quantity_per_trade=0.002,\n    margin_mode=MarginMode.ISOLATED,\n    position_mode=PositionMode.ONE_WAY,\n    demo=True,\n)\n\nenv = BitgetFuturesSLTPTorchTradingEnv(\n    config,\n    api_key=os.getenv(\"BITGETACCESSAPIKEY\"),\n    api_secret=os.getenv(\"BITGETSECRETKEY\"),\n    api_passphrase=os.getenv(\"BITGETPASSPHRASE\"),\n)\n# Action space: HOLD + 2\u00d7(3 SL \u00d7 3 TP) = 19 actions (long + short)\n</code></pre>"},{"location":"environments/online/#api-key-setup","title":"API Key Setup","text":"<p>Each exchange requires API keys stored in a <code>.env</code> file:</p> <pre><code># Alpaca (https://alpaca.markets/signup)\nAPI_KEY=your_alpaca_api_key\nSECRET_KEY=your_alpaca_secret_key\n\n# Binance (https://www.binance.com/en/my/settings/api-management)\nBINANCE_API_KEY=your_binance_api_key\nBINANCE_SECRET_KEY=your_binance_secret_key\n\n# Bitget (https://www.bitget.com/en/support/articles/360038859731)\nBITGETACCESSAPIKEY=your_bitget_api_key\nBITGETSECRETKEY=your_bitget_secret_key\nBITGETPASSPHRASE=your_bitget_passphrase\n</code></pre> <p>Always Start with Paper/Testnet Trading</p> <p>Set <code>paper=True</code> (Alpaca) or <code>demo=True</code> (Binance/Bitget) before using real funds. Start with low leverage (2-5x) for futures.</p>"},{"location":"environments/online/#exchange-comparison","title":"Exchange Comparison","text":"Feature Alpaca Binance Bitget Asset Types Stocks, Crypto Crypto Crypto Futures - Yes Yes Max Leverage 1x 125x 125x Paper Trading Yes Yes (Testnet) Yes (Testnet) Commission Free 0.02%/0.04% 0.02%/0.06%"},{"location":"environments/online/#requesting-new-exchanges","title":"Requesting New Exchanges","text":"<p>Need support for another exchange? Create an issue or email us at torchtradecontact@gmail.com.</p>"},{"location":"environments/visualization/","title":"Visualizing Episode History","text":"<p>All offline environments support <code>render_history()</code> to visualize episode performance after training or evaluation. The method automatically detects the environment type and renders appropriate plots.</p>"},{"location":"environments/visualization/#overview","title":"Overview","text":"<p>The <code>render_history()</code> method is inherited from the <code>TorchTradeOfflineEnv</code> base class and provides consistent visualization across all 3 offline environments:</p> <ul> <li>SequentialTradingEnv</li> <li>SequentialTradingEnvSLTP</li> <li>OneStepTradingEnv</li> </ul> <p>The visualization automatically adapts based on the trading mode (spot vs futures).</p>"},{"location":"environments/visualization/#usage","title":"Usage","text":"<pre><code># After running an episode\nenv.reset()\nfor _ in range(episode_length):\n    action = policy(obs)\n    obs, reward, done, truncated, info = env.step(action)\n    if done or truncated:\n        break\n\n# Visualize the episode\nenv.render_history()  # Display plots with buy-and-hold baseline\n\n# Without buy-and-hold baseline\nenv.render_history(plot_bh_baseline=False)\n\n# Or save to a variable for later\nfig = env.render_history(return_fig=True, plot_bh_baseline=True)\nfig.savefig(\"episode_history.png\")\n</code></pre>"},{"location":"environments/visualization/#visualization-types","title":"Visualization Types","text":"<p>All environments render 3 subplots:</p> <ol> <li>Price History with Actions: Shows the asset price over time with long/short actions marked as green upward triangles (long/buy) and red downward triangles (short/sell). Position closes are shown as orange (long close) and cyan (short close) markers.</li> <li>Portfolio Value vs Buy-and-Hold: Compares your agent's portfolio value against a simple buy-and-hold baseline strategy.</li> <li>Exposure History: Visualizes exposure percentage over time with green areas for long exposure, red areas for short exposure, and flat sections for no position.</li> </ol> <p></p>"},{"location":"environments/visualization/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>return_fig</code> bool <code>False</code> Return matplotlib figure instead of displaying <code>plot_bh_baseline</code> bool <code>True</code> Show buy-and-hold baseline comparison <p>Implemented in <code>torchtrade/envs/offline/base.py</code>. Requires <code>matplotlib</code>.</p>"},{"location":"examples/","title":"Examples","text":"<p>TorchTrade provides a collection of example training scripts to help you get started. These examples are designed for inspiration and learning - use them as starting points to build your own custom training pipelines.</p>"},{"location":"examples/#design-philosophy","title":"Design Philosophy","text":"<p>TorchTrade examples closely follow the structure of TorchRL's SOTA implementations, enabling near plug-and-play compatibility with any TorchRL algorithm. This means:</p> <ul> <li>Familiar structure if you've used TorchRL before</li> <li>Easy adaptation of TorchRL algorithms to trading environments</li> <li>Minimal boilerplate - focus on what's unique to your strategy</li> <li>Hydra configuration for easy experimentation</li> </ul>"},{"location":"examples/#direct-compatibility-with-torchrl-sota-implementations","title":"Direct Compatibility with TorchRL SOTA Implementations","text":"<p>To demonstrate the closeness to TorchRL's SOTA implementations, here is a direct comparison:</p> <p>TorchRL's A2C Example: <pre><code>from torchrl.envs import GymEnv\n\n# Environment setup\nenv = GymEnv(\"CartPole-v1\")\n\n# Everything else stays the same\ncollector = SyncDataCollector(env, policy, ...)\nloss_module = A2CLoss(actor, critic, ...)\noptimizer = torch.optim.Adam(loss_module.parameters(), lr=1e-3)\n\nfor batch in collector:\n    loss_values = loss_module(batch)\n    loss_values[\"loss\"].backward()\n    optimizer.step()\n</code></pre></p> <p>TorchTrade Adaptation (Only Environment Changes): <pre><code>from torchtrade.envs.offline import SequentialTradingEnv, SequentialTradingEnvConfig\n\n# Environment setup - ONLY CHANGE\nenv = SequentialTradingEnv(df, SequentialTradingEnvConfig(...))\n\n# Everything else stays EXACTLY the same\ncollector = SyncDataCollector(env, policy, ...)\nloss_module = A2CLoss(actor, critic, ...)\noptimizer = torch.optim.Adam(loss_module.parameters(), lr=1e-3)\n\nfor batch in collector:\n    loss_values = loss_module(batch)\n    loss_values[\"loss\"].backward()\n    optimizer.step()\n</code></pre></p> <p>That's it! The collector, loss function, optimizer, and training loop remain identical. This doesn't only allow the use of any TorchRL algorithm, but any of the other useful components of TorchRL - replay buffers, transforms, modules, data structures - and provides seamless integration into the entire TorchRL ecosystem.</p>"},{"location":"examples/#example-file-structure","title":"Example File Structure","text":"<p>Each algorithm directory is fully self-contained \u2014 configs, scripts, and all training outputs (checkpoints, logs, Hydra outputs) are written to that directory:</p> <pre><code>examples/online_rl/\n\u251c\u2500\u2500 &lt;algorithm&gt;/                      # ppo, dqn, dsac, iql, ppo_chronos\n\u2502   \u251c\u2500\u2500 config.yaml                   # Algorithm config\n\u2502   \u251c\u2500\u2500 env/                          # Environment configs\n\u2502   \u2502   \u251c\u2500\u2500 sequential.yaml           # Basic sequential trading\n\u2502   \u2502   \u2514\u2500\u2500 sequential_sltp.yaml      # Sequential with stop-loss/take-profit\n\u2502   \u251c\u2500\u2500 train.py                      # Training script (offline backtesting)\n\u2502   \u251c\u2500\u2500 live.py                       # Live trading script (optional)\n\u2502   \u251c\u2500\u2500 utils.py                      # Helper functions\n\u2502   \u2514\u2500\u2500 outputs/                      # Hydra outputs, checkpoints, logs\n\u2502\n\u2514\u2500\u2500 grpo/                             # GRPO (onestep-only)\n    \u251c\u2500\u2500 config.yaml                   # Algorithm config\n    \u251c\u2500\u2500 env/\n    \u2502   \u2514\u2500\u2500 onestep.yaml              # One-step environment config\n    \u251c\u2500\u2500 train.py\n    \u251c\u2500\u2500 utils.py\n    \u2514\u2500\u2500 outputs/\n</code></pre> <p>Key Features:</p> <ul> <li> <p>Each algorithm directory is self-contained \u2014 everything you need to run, train, and deploy lives in one place</p> </li> <li> <p>Training outputs (checkpoints, logs) are written to the algorithm's own directory</p> </li> <li> <p>GRPO only supports onestep environments</p> </li> <li> <p>No spot/futures split - users override <code>leverage</code> and <code>action_levels</code> for  futures</p> </li> <li> <p>All use 1Hour timeframe by default</p> </li> </ul>"},{"location":"examples/#what-each-component-does","title":"What Each Component Does","text":"<p><code>config.yaml</code> - Configuration Management</p> <p>The configuration file uses Hydra to manage all hyperparameters and settings. This includes:</p> <ul> <li>Environment settings: Symbol, timeframes, initial cash, transaction fees, window sizes</li> <li>Network architecture: Hidden dimensions, activation functions, layer configurations</li> <li>Training hyperparameters: Learning rate, batch size, discount factor (gamma), entropy coefficient</li> <li>Collector settings: Frames per batch, number of parallel environments</li> <li>Logging: Wandb project name, experiment tracking settings</li> </ul> <p>By centralizing all parameters in YAML, you can easily experiment with different configurations without modifying code. Hydra also allows you to override any parameter from the command line:</p> <pre><code># Override multiple parameters\npython train.py env.symbol=\"ETH/USD\" optim.lr=1e-4 loss.gamma=0.95\n</code></pre> <p>Example config.yaml (PPO): <pre><code>defaults:\n  - env: sequential          # Load env/sequential.yaml (switch with env=sequential_sltp)\n  - _self_\n\ncollector:\n  device:\n  frames_per_batch: 10000\n  total_frames: 100_000_000\n\nlogger:\n  mode: online\n  backend: wandb\n  project_name: TorchTrade-Online\n  group_name: ${env.name}\n  exp_name: ppo-${env.name}\n  test_interval: 1_000_000\n  num_test_episodes: 1\n\nmodel:\n  network_type: batchnorm_mlp\n  hidden_size: 128\n  dropout: 0.1\n  num_layers: 4\n\noptim:\n  lr: 2.5e-4\n  eps: 1.0e-6\n  weight_decay: 0.0\n  max_grad_norm: 0.5\n  anneal_lr: True\n  device:\n\nloss:\n  gamma: 0.9\n  mini_batch_size: 3333\n  ppo_epochs: 3\n  gae_lambda: 0.95\n  clip_epsilon: 0.1\n  anneal_clip_epsilon: True\n  critic_coeff: 1.0\n  entropy_coeff: 1.0\n  loss_critic_type: l2\n</code></pre></p> <p>Note how <code>config.yaml</code> uses <code>defaults: - env: sequential</code> to load the environment config from <code>env/sequential.yaml</code>. The env section is kept separate \u2014 see the <code>env/</code> configs above.</p> <p><code>env/</code> - Environment Configs</p> <p>Environment configs are separate YAML files that define the trading environment. Switch environments via CLI with <code>env=sequential_sltp</code> (see Running Examples).</p> <p><code>env/sequential.yaml</code> \u2014 Basic sequential trading (<code>SequentialTradingEnv</code>): <pre><code># @package env\nname: SequentialTradingEnv\nsymbol: \"BTC/USD\"\ntime_frames: [\"1Hour\"]\nwindow_sizes: [24]\nexecute_on: \"1Hour\"\nleverage: 2\naction_levels: [-1.0, 0.0, 1.0]\ninitial_cash: 10000\ntransaction_fee: 0.0\nslippage: 0.0\nbankrupt_threshold: 0.1\ninclude_base_features: false\nmax_traj_length: null\nrandom_start: true\nmargin_type: isolated\nmaintenance_margin_rate: 0.004\nseed: 0\ntrain_envs: 5\neval_envs: 1\ndata_path: Torch-Trade/btcusdt_spot_1m_03_2023_to_12_2025\ntest_split_start: \"2025-01-01\"\n</code></pre></p> <p><code>env/sequential_sltp.yaml</code> \u2014 Sequential with stop-loss/take-profit (<code>SequentialTradingEnvSLTP</code>): <pre><code># @package env\nname: SequentialTradingEnvSLTP\nsymbol: \"BTC/USD\"\ntime_frames: [\"1Hour\"]\nwindow_sizes: [24]\nexecute_on: \"1Hour\"\nleverage: 1\naction_levels: [0.0, 1.0]\nstoploss_levels: [-0.01, -0.02, -0.03, -0.04]\ntakeprofit_levels: [0.02, 0.04, 0.06, 0.08, 0.1]\ninclude_hold_action: true\ninclude_close_action: false\ninitial_cash: 10000\ntransaction_fee: 0.0\nslippage: 0.0\nbankrupt_threshold: 0.1\ninclude_base_features: false\nmax_traj_length: null\nrandom_start: true\nmargin_type: isolated\nmaintenance_margin_rate: 0.004\nseed: 0\ntrain_envs: 10\neval_envs: 1\ndata_path: Torch-Trade/btcusdt_spot_1m_03_2023_to_12_2025\ntest_split_start: \"2025-01-01\"\n</code></pre></p> <p><code>env/onestep.yaml</code> \u2014 One-step environment for GRPO (<code>OneStepTradingEnv</code>): <pre><code># @package env\nname: OneStepTradingEnv\nsymbol: \"BTC/USD\"\ntime_frames: [\"1Hour\"]\nwindow_sizes: [24]\nexecute_on: \"1Hour\"\nleverage: 1\ninitial_cash: 10000\ntransaction_fee: 0.0\nslippage: 0.0\nbankrupt_threshold: 0.1\nstoploss_levels: [-0.02]\ntakeprofit_levels: [0.04]\ninclude_hold_action: true\nseed: 0\ntrain_envs: 6\neval_envs: 1\ndata_path: Torch-Trade/btcusdt_spot_1m_03_2023_to_12_2025\ntest_split_start: \"2025-01-01\"\n</code></pre></p> <p><code>utils.py</code> - Helper functions (<code>make_env()</code>, <code>make_actor()</code>, <code>make_critic()</code>, <code>make_loss()</code>, <code>make_collector()</code>) that keep the training script clean.</p> <p><code>train.py</code> - Main training loop: loads config via Hydra, creates components, collects data, trains, evaluates, and checkpoints.</p> <p><code>live.py</code> - Live trading script that loads a trained policy and executes it against a live exchange API. The DQN and PPO examples include <code>live.py</code> to demonstrate the smooth transition from backtesting/training to live execution \u2014 the same policy trained offline can be deployed directly to a live environment with minimal code changes.</p>"},{"location":"examples/#available-examples","title":"Available Examples","text":"<p>The following examples demonstrate the flexibility of TorchTrade across different algorithms, environments, and use cases. These examples are meant to be starting points for further experimentation and adaptation - customize them according to your needs, ideas, and environments.</p> <p>Hyperparameters Not Tuned</p> <p>All hyperparameters in our examples are NOT tuned. The configurations provided are starting points for experimentation, not optimized settings. You should tune hyperparameters (learning rates, network architectures, reward functions, etc.) according to your specific trading environment, market conditions, and objectives.</p>"},{"location":"examples/#online-rl-offline-backtesting-environments","title":"Online RL (Offline Backtesting Environments)","text":"<p>These examples use online RL algorithms (learning from interaction as it happens) with historical market data for backtesting. This allows you to train policies on past data before deploying them to live trading environments. We typically split the training data into training and test environments to evaluate the generalization performance of learned policies on unseen market conditions.</p> <p>Located in <code>examples/online_rl/</code>:</p> <ul> <li>PPO - <code>ppo/</code> - Standard policy gradient</li> <li>PPO + Chronos - <code>ppo_chronos/</code> - Time series embedding with Chronos T5 models</li> <li>DQN - <code>dqn/</code> - Deep Q-learning with experience replay and target networks</li> <li>IQL - <code>iql/</code> - Implicit Q-Learning</li> <li>Discrete SAC - <code>dsac/</code> - Discrete Soft Actor-Critic</li> <li>GRPO - <code>grpo/</code> - Group Relative Policy Optimization (onestep-only, no env switching)</li> </ul> <p>All algorithms except GRPO support environment switching via CLI - see Running Examples below.</p>"},{"location":"examples/#offline-rl","title":"Offline RL","text":"<p>These examples use offline RL algorithms that learn from pre-collected datasets without requiring live environment interaction during training. The data can be collected from interactions with offline backtesting environments or from real online live trading sessions. We provide simple example offline datasets at HuggingFace/Torch-Trade.</p> <p>Located in <code>examples/offline_rl/</code>:</p> Example Algorithm Environment Key Features iql/ IQL SequentialTradingEnv Offline RL from pre-collected trajectories <p>Note: Thanks to the compatibility with TorchRL, you can easily add other offline RL methods like CQL, TD3+BC, and Decision Transformers from TorchRL to do offline RL with TorchTrade environments.</p>"},{"location":"examples/#llm-actors","title":"LLM Actors","text":"<p>TorchTrade provides LLM-based trading actors that leverage language models for trading decision-making. Both frontier API models and local models are supported, each with offline (backtesting) and online (live trading) examples.</p> <p>Located in <code>examples/llm/</code>:</p> Example Actor Description frontier/offline.py FrontierLLMActor Backtesting with frontier LLM APIs (OpenAI, Anthropic, etc.) frontier/live.py FrontierLLMActor Live trading with frontier LLM APIs local/offline.py LocalLLMActor Backtesting with local LLMs (vLLM/transformers) local/live.py LocalLLMActor Live trading with local LLMs <p>Local models can be loaded from HuggingFace Models or quantized via Unsloth for memory-efficient inference.</p> <p>Future Work</p> <p>Fine-tuning LLMs on reasoning traces from frontier models, and integrating Vision-Language Models (VLMs) to process trading chart plots.</p>"},{"location":"examples/#rule-based-actors","title":"Rule-Based Actors","text":"<p>TorchTrade provides RuleBasedActor for creating trading strategies using technical indicators and market signals. These actors integrate seamlessly with TorchTrade environments for both backtesting and live trading, serving as baselines or components in hybrid approaches.</p> <p>Located in <code>examples/rule_based/</code>:</p> Example Description offline.py Backtesting a mean reversion strategy on historical data live.py Live trading with the mean reversion strategy <p>Combine with custom feature preprocessing to add technical indicators for rule-based strategies.</p> <p>Future Work</p> <p>Hybrid approaches that combine rule-based policies with neural network policies as actors, leveraging the strengths of both deterministic strategies and learned behaviors.</p>"},{"location":"examples/#live-trading","title":"Live Trading","text":"<p>The DQN and PPO examples include a <code>live.py</code> script alongside the <code>train.py</code> script, demonstrating the smooth transition from offline backtesting to live trading. The same model architecture and <code>utils.py</code> helpers are reused \u2014 only the environment changes from an offline <code>SequentialTradingEnv</code> to a live exchange environment. This mirrors the core design philosophy: swap the environment, keep everything else the same.</p> <p>Live scripts support loading pre-trained weights via <code>--weights</code> and store all live transitions in a replay buffer (saved periodically for crash safety), enabling offline analysis or further training on real market data.</p> <p>Located alongside each algorithm in <code>examples/online_rl/</code>:</p> Example Exchange Algorithm Description dqn/live.py Binance Futures DQN Live futures trading with DQN ppo/live.py Alpaca PPO Live spot trading with PPO actor <p>Usage: <pre><code># Train offline first\npython examples/online_rl/dqn/train.py\n\n# Deploy trained weights to Binance testnet\npython examples/online_rl/dqn/live.py --weights dqn_policy_100.pth --demo\n\n# Deploy PPO to Alpaca paper trading\npython examples/online_rl/ppo/live.py --weights ppo_policy_100.pth --paper\n</code></pre></p>"},{"location":"examples/#transforms","title":"Transforms","text":"<p>Inspired by work such as R3M and VIP that utilize large pretrained models for representation learning, we created the ChronosEmbeddingTransform using Chronos forecasting models to embed historical trading data. This demonstrates the flexibility and adaptability of TorchTrade for integrating pretrained models as transforms for enhanced feature representations.</p> <p>Located in <code>examples/transforms/</code>:</p> Example Transform Description chronos_embedding_example.py ChronosEmbeddingTransform Time series embedding with Chronos T5 models <p>Note: If you would like us to add additional transforms for other pretrained models (similar to ChronosEmbedding), we welcome GitHub issues with your requests. We're happy to implement these given the availability of model weights and resources.</p>"},{"location":"examples/#running-examples","title":"Running Examples","text":"<p>All examples use Hydra for configuration management with centralized environment configs:</p> <pre><code># Run with default configuration (sequential, spot, 1Hour)\nuv run python examples/online_rl/ppo/train.py\n\n# Switch environment via CLI\nuv run python examples/online_rl/ppo/train.py env=sequential_sltp\nuv run python examples/online_rl/ppo/train.py env=onestep\n\n# Configure for futures trading\nuv run python examples/online_rl/ppo/train.py \\\n    env.leverage=5 \\\n    env.action_levels='[-1.0,0.0,1.0]'\n\n# Override multiple parameters\nuv run python examples/online_rl/ppo/train.py \\\n    env=sequential_sltp \\\n    env.symbol=\"ETH/USD\" \\\n    env.leverage=10 \\\n    optim.lr=1e-4 \\\n    loss.gamma=0.95\n</code></pre>"},{"location":"examples/#available-environment-configs","title":"Available Environment Configs","text":"Config Environment Class SLTP Use Case <code>sequential</code> SequentialTradingEnv No Basic sequential trading <code>sequential_sltp</code> SequentialTradingEnvSLTP Yes Sequential with bracket orders <code>onestep</code> OneStepTradingEnv Yes One-step for GRPO/contextual bandits <p>Spot vs Futures:</p> <ul> <li> <p>Spot (default): <code>leverage: 1</code>, <code>action_levels: [0.0, 1.0]</code></p> </li> <li> <p>Futures: Override with <code>env.leverage=5 env.action_levels='[-1.0,0.0,1.0]'</code></p> </li> </ul>"},{"location":"examples/#common-hydra-overrides","title":"Common Hydra Overrides","text":"Parameter Example Description <code>env.symbol</code> <code>\"BTC/USD\"</code> Trading pair/symbol <code>env.initial_cash</code> <code>10000</code> Starting capital <code>env.time_frames</code> <code>'[\"1min\",\"5min\"]'</code> Multi-timeframe observations <code>optim.lr</code> <code>1e-4</code> Learning rate <code>loss.gamma</code> <code>0.99</code> Discount factor <code>collector.frames_per_batch</code> <code>2000</code> Frames collected per iteration <code>total_frames</code> <code>100000</code> Total training frames"},{"location":"examples/#creating-your-own-examples","title":"Creating Your Own Examples","text":"<p>Copy an existing example closest to your use case and customize:</p> <ul> <li>Start with <code>ppo/</code> for standard RL, <code>grpo/</code> for one-step RL, <code>ppo_chronos/</code> for time series embeddings</li> <li>Use <code>env=&lt;config_name&gt;</code> to switch environments without copying code</li> </ul>"},{"location":"examples/results/","title":"Results","text":"<p>Example training results from TorchTrade's offline environments. These demonstrate what trained policies look like \u2014 not tuned benchmarks.</p> <p>Not Benchmarks</p> <p>These results use default hyperparameters and are not optimized. They serve as visual examples of what training output looks like, not as performance targets.</p>"},{"location":"examples/results/#dqn-vs-ppo-sequentialtradingenv-futures-2x-leverage","title":"DQN vs PPO \u2014 SequentialTradingEnv (Futures, 2x Leverage)","text":"<p>In this example we compare DQN and PPO on BTC/USD with 1Hour timeframe, <code>action_levels=[-1, 0, 1]</code>, <code>leverage=2</code>, and no transaction fees \u2014 purely to demonstrate general learning behavior. The shown results are not realistic and should not be taken as indicative of real trading performance. Note that without fees, policies typically overtrade since there is no cost to frequent position changes.</p> <p>A common observation is overfitting: the policy learns to trade profitably on the training data but performance degrades on unseen test data. This is expected and highlights the importance of evaluating on held-out data and applying regularization techniques.</p>"},{"location":"examples/results/#training-curves","title":"Training Curves","text":""},{"location":"examples/results/#dqn-episode-history","title":"DQN Episode History","text":"<p>Training Data:</p> <p></p> <p>Test Data (not seen during training):</p> <p></p>"},{"location":"examples/results/#ppo-episode-history","title":"PPO Episode History","text":"<p>Training Data:</p> <p></p> <p>Test Data (not seen during training):</p> <p></p>"},{"location":"guides/custom-environment/","title":"Building Custom Environments","text":"<p>This guide shows how to extend TorchTrade for custom trading environments.</p>"},{"location":"guides/custom-environment/#when-to-build-a-custom-environment","title":"When to Build a Custom Environment","text":"<p>Build a custom environment when you need:</p> <ul> <li>Custom asset types: Options, forex, commodities</li> <li>Complex order types: Market-on-close, iceberg orders</li> <li>Custom state: Order book data, sentiment, news</li> <li>Specific trading rules: Pattern day trading, portfolio constraints</li> <li>New exchange integrations: Unsupported brokers/APIs</li> </ul>"},{"location":"guides/custom-environment/#environment-architecture","title":"Environment Architecture","text":"<p>TorchTrade environments inherit from TorchRL's <code>EnvBase</code>:</p> <pre><code>EnvBase (TorchRL)\n    \u2193\nBaseTorchTradeEnv (Abstract base - optional)\n    \u2193\nYourCustomEnv\n</code></pre>"},{"location":"guides/custom-environment/#required-methods","title":"Required Methods","text":"Method Purpose Returns <code>_reset()</code> Initialize episode state TensorDict with initial observation <code>_step(tensordict)</code> Execute action, update state TensorDict with next observation <code>_set_seed(seed)</code> Set random seed for reproducibility None <code>observation_spec</code> Define observation space <code>Composite</code> <code>action_spec</code> Define action space <code>Categorical</code> or <code>Bounded</code> <code>reward_spec</code> Define reward space <code>Unbounded</code>"},{"location":"guides/custom-environment/#example-1-simple-custom-environment","title":"Example 1: Simple Custom Environment","text":"<p>Minimal environment from scratch:</p> <pre><code>from torchrl.envs import EnvBase\nfrom torchrl.data import Categorical, Composite, Unbounded\nfrom tensordict import TensorDict\nimport torch\n\nclass SimpleCustomEnv(EnvBase):\n    \"\"\"\n    Minimal custom trading environment.\n\n    State: [price, position]\n    Actions: 0 (HOLD), 1 (BUY), 2 (SELL)\n    Reward: Log return\n    \"\"\"\n\n    def __init__(self, prices: torch.Tensor, **kwargs):\n        super().__init__(**kwargs)\n        self.prices = prices\n        self.current_step = 0\n        self.position = 0  # 0 or 1\n        self.entry_price = 0.0\n\n        # Define specs\n        self._observation_spec = Composite({\n            \"price\": Unbounded(shape=(1,)),\n            \"position\": Unbounded(shape=(1,)),\n        })\n\n        self._action_spec = Categorical(n=3)\n        self._reward_spec = Unbounded(shape=(1,))\n\n    def _reset(self, tensordict=None, **kwargs):\n        \"\"\"Reset to initial state\"\"\"\n        self.current_step = 0\n        self.position = 0\n        self.entry_price = 0.0\n\n        return TensorDict({\n            \"price\": torch.tensor([self.prices[0].item()]),\n            \"position\": torch.tensor([0.0]),\n        }, batch_size=self.batch_size)\n\n    def _step(self, tensordict: TensorDict) -&gt; TensorDict:\n        \"\"\"Execute one step\"\"\"\n        action = tensordict[\"action\"].item()\n        current_price = self.prices[self.current_step].item()\n\n        # Execute action\n        reward = 0.0\n        if action == 1 and self.position == 0:  # BUY\n            self.position = 1\n            self.entry_price = current_price\n        elif action == 2 and self.position == 1:  # SELL\n            reward = (current_price - self.entry_price) / self.entry_price\n            self.position = 0\n            self.entry_price = 0.0\n\n        # Move to next step\n        self.current_step += 1\n        done = self.current_step &gt;= len(self.prices) - 1\n\n        # Build output tensordict\n        next_price = self.prices[self.current_step].item() if not done else current_price\n\n        return TensorDict({\n            \"price\": torch.tensor([next_price]),\n            \"position\": torch.tensor([float(self.position)]),\n            \"reward\": torch.tensor([reward]),\n            \"done\": torch.tensor([done]),\n        }, batch_size=self.batch_size)\n\n    def _set_seed(self, seed: int):\n        \"\"\"Set random seed\"\"\"\n        torch.manual_seed(seed)\n\n# Usage\nprices = torch.randn(1000).cumsum(0) + 100  # Random walk prices\nenv = SimpleCustomEnv(prices, batch_size=[])\n\nobs = env.reset()\nfor _ in range(100):\n    action = env.action_spec.rand()  # Random action\n    obs = env.step(action)\n    if obs[\"done\"]:\n        break\n</code></pre>"},{"location":"guides/custom-environment/#example-2-extending-existing-environments","title":"Example 2: Extending Existing Environments","text":"<p>Extend <code>SequentialTradingEnv</code> to add custom features:</p> <pre><code>from torchtrade.envs.offline import SequentialTradingEnv, SequentialTradingEnvConfig\nfrom tensordict import TensorDict\nimport torch\n\nclass CustomLongOnlyEnv(SequentialTradingEnv):\n    \"\"\"\n    Extended SequentialTradingEnv with sentiment data.\n    \"\"\"\n\n    def __init__(self, df, config: SequentialTradingEnvConfig, sentiment_data: torch.Tensor):\n        super().__init__(df, config)\n        self.sentiment_data = sentiment_data  # Timeseries sentiment scores\n\n        # Extend observation spec\n        from torchrl.data import Unbounded\n        self._observation_spec[\"sentiment\"] = Unbounded(shape=(1,))\n\n    def _reset(self, tensordict=None, **kwargs):\n        \"\"\"Add sentiment to observations\"\"\"\n        obs = super()._reset(tensordict, **kwargs)\n\n        # Add current sentiment\n        sentiment_idx = self.sampler.reset_index\n        obs[\"sentiment\"] = torch.tensor([self.sentiment_data[sentiment_idx].item()])\n\n        return obs\n\n    def _step(self, tensordict: TensorDict) -&gt; TensorDict:\n        \"\"\"Add sentiment to step observations\"\"\"\n        obs = super()._step(tensordict)\n\n        # Add current sentiment\n        sentiment_idx = self.sampler.current_index\n        obs[\"sentiment\"] = torch.tensor([self.sentiment_data[sentiment_idx].item()])\n\n        return obs\n\n# Usage\nimport pandas as pd\n\ndf = pd.read_csv(\"prices.csv\")\nsentiment = torch.randn(len(df))  # Random sentiment scores\n\nconfig = SequentialTradingEnvConfig(\n    time_frames=[\"1min\", \"5min\"],\n    window_sizes=[12, 8],\n    execute_on=(5, \"Minute\"),\n)\n\nenv = CustomLongOnlyEnv(df, config, sentiment)\n\n# Policy network sees sentiment in observations\nobs = env.reset()\nprint(obs.keys())  # [..., 'sentiment']\n</code></pre>"},{"location":"guides/custom-environment/#design-patterns","title":"Design Patterns","text":""},{"location":"guides/custom-environment/#1-composition-over-inheritance","title":"1. Composition Over Inheritance","text":"<p>Prefer composing existing components:</p> <pre><code>class CustomEnv(SequentialTradingEnv):\n    def __init__(self, df, config, custom_component):\n        super().__init__(df, config)\n        self.custom_component = custom_component  # Inject custom logic\n\n    def _step(self, tensordict):\n        obs = super()._step(tensordict)\n        # Modify obs with custom_component\n        obs[\"custom_feature\"] = self.custom_component.compute(obs)\n        return obs\n</code></pre>"},{"location":"guides/custom-environment/#2-observation-spec-extension","title":"2. Observation Spec Extension","text":"<p>Always update observation specs when adding new fields:</p> <pre><code># In __init__\nself._observation_spec[\"new_field\"] = Unbounded(shape=(N,))\n</code></pre>"},{"location":"guides/custom-environment/#3-state-management","title":"3. State Management","text":"<p>TorchTrade provides structured state management classes for consistent state handling across all environments. See State Management for full details.</p> <p>Quick Reference - PositionState:</p> <p>The <code>PositionState</code> dataclass encapsulates position-related state in a structured way:</p> <pre><code>from torchtrade.envs.core.state import PositionState\n\nclass CustomEnv(EnvBase):\n    def __init__(self):\n        super().__init__()\n        # Use PositionState to group position-related variables\n        self.position = PositionState()\n        # Provides: position.current_position, position.position_size,\n        #          position.position_value, position.entry_price,\n        #          position.unrealized_pnlpc, position.hold_counter\n\n        # Other environment state\n        self.current_step = 0\n        self.cash = 1000.0\n        self.current_timestamp = None\n\n    def _reset(self, tensordict=None, **kwargs):\n        \"\"\"Reset all state including position\"\"\"\n        self.position.reset()  # Resets all position fields to defaults\n        self.current_step = 0\n        self.cash = 1000.0\n        # ... reset other state\n\n    def _step(self, tensordict):\n        \"\"\"Use position state in step logic\"\"\"\n        if action == \"BUY\" and self.position.position_size == 0:\n            self.position.position_size = 100\n            self.position.entry_price = current_price\n            self.position.current_position = 1.0\n        # ... rest of step logic\n</code></pre> <p>Benefits of PositionState: - Groups related state variables together (better organization) - Provides a single <code>.reset()</code> method for all position fields - Makes position state explicit and easier to track - Used consistently across all TorchTrade environments</p> <p>Source: <code>torchtrade/envs/core/state.py</code></p> <p>Quick Reference - HistoryTracker:</p> <p>Use <code>HistoryTracker</code> to record episode data for analysis and visualization:</p> <pre><code>from torchtrade.envs.core.state import HistoryTracker\n\nclass CustomEnv(EnvBase):\n    def __init__(self):\n        super().__init__()\n        self.history = HistoryTracker()\n\n    def _step(self, tensordict):\n        # ... execute step logic ...\n\n        self.history.record_step(\n            price=current_price,\n            action=action.item(),\n            reward=reward,\n            portfolio_value=self.cash + self.position.position_value,\n            position=self.position.position_size,\n            action_type=\"long\",  # or \"short\", \"hold\", \"close\", etc.\n        )\n\n    def _reset(self, tensordict=None, **kwargs):\n        self.history.reset()  # Clear history at episode start\n        # ... rest of reset logic\n</code></pre> <p>Access history via <code>history.to_dict()</code> \u2014 returns prices, actions, rewards, portfolio values, positions, and action types.</p> <p>Source: <code>torchtrade/envs/core/state.py</code></p>"},{"location":"guides/custom-environment/#testing-custom-environments","title":"Testing Custom Environments","text":""},{"location":"guides/custom-environment/#1-spec-compliance","title":"1. Spec Compliance","text":"<p>Verify specs match actual outputs:</p> <pre><code>env = CustomEnv(...)\n\n# Check reset\nobs = env.reset()\nassert env.observation_spec.is_in(obs), \"Reset observation doesn't match spec\"\n\n# Check step\naction = env.action_spec.rand()\nobs = env.step(action)\nassert env.observation_spec.is_in(obs), \"Step observation doesn't match spec\"\nassert env.reward_spec.is_in(obs[\"reward\"]), \"Reward doesn't match spec\"\n</code></pre>"},{"location":"guides/custom-environment/#2-episode-completion","title":"2. Episode Completion","text":"<p>Ensure episodes terminate correctly:</p> <pre><code>env = CustomEnv(...)\nobs = env.reset()\n\nfor i in range(10000):  # Safety limit\n    action = env.action_spec.rand()\n    obs = env.step(action)\n    if obs[\"done\"]:\n        print(f\"Episode ended at step {i}\")\n        break\nelse:\n    raise AssertionError(\"Episode never ended!\")\n</code></pre>"},{"location":"guides/custom-environment/#3-reward-sanity","title":"3. Reward Sanity","text":"<p>Check reward values are reasonable:</p> <pre><code>rewards = []\nfor episode in range(100):\n    obs = env.reset()\n    episode_reward = 0\n    while not obs[\"done\"]:\n        action = env.action_spec.rand()\n        obs = env.step(action)\n        episode_reward += obs[\"reward\"].item()\n    rewards.append(episode_reward)\n\nprint(f\"Mean reward: {sum(rewards)/len(rewards):.2f}\")\nprint(f\"Reward range: [{min(rewards):.2f}, {max(rewards):.2f}]\")\n</code></pre>"},{"location":"guides/custom-environment/#common-pitfalls","title":"Common Pitfalls","text":"Issue Problem Solution Spec mismatch Observation shape != spec shape Update <code>_observation_spec</code> in <code>__init__</code> Forgotten batch_size TensorDict missing batch_size Always pass <code>batch_size=self.batch_size</code> Missing done signal Episode never ends Set <code>done=True</code> in terminal state Mutable state State persists across episodes Reset ALL state variables in <code>_reset()</code> Incorrect device Tensors on wrong device Use <code>self.device</code> for all tensors"},{"location":"guides/custom-environment/#next-steps","title":"Next Steps","text":"<ul> <li>Offline Environments - Existing environment architecture</li> <li>TorchRL EnvBase - Base class documentation</li> </ul>"},{"location":"guides/custom-features/","title":"Feature Engineering","text":"<p>TorchTrade allows you to add custom technical indicators and features to your market observations. This guide shows you how to preprocess your OHLCV data with custom features before it's fed to your policy.</p>"},{"location":"guides/custom-features/#how-it-works","title":"How It Works","text":"<p>The <code>feature_preprocessing_fn</code> parameter in environment configs transforms raw OHLCV data into custom features. This function is called on each resampled timeframe during environment initialization.</p> <p>IMPORTANT: All feature columns must start with <code>features_</code> prefix (e.g., <code>features_close</code>, <code>features_rsi_14</code>). Only columns with this prefix will be included in the observation space.</p> <p>Timeframe Format Matters</p> <p>When specifying <code>time_frames</code>, use canonical forms to avoid confusion:</p> <ul> <li>\u2705 Use: <code>\"1hour\"</code>, <code>\"2hours\"</code>, <code>\"1day\"</code></li> <li>\u274c Avoid: <code>\"60min\"</code>, <code>\"120min\"</code>, <code>\"24hour\"</code>, <code>\"1440min\"</code></li> </ul> <p>Why? Different formats create different observation keys:</p> <ul> <li><code>time_frames=[\"60min\"]</code> \u2192 observation key: <code>\"market_data_60Minute\"</code></li> <li><code>time_frames=[\"1hour\"]</code> \u2192 observation key: <code>\"market_data_1Hour\"</code></li> </ul> <p>These are treated as DIFFERENT timeframes. Models trained with one format won't work with the other. The framework will issue a warning if you use non-canonical forms like <code>\"60min\"</code> to guide you toward cleaner observation keys.</p>"},{"location":"guides/custom-features/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/custom-features/#example-1-adding-technical-indicators","title":"Example 1: Adding Technical Indicators","text":"<pre><code>import pandas as pd\nimport ta  # Technical Analysis library\nfrom torchtrade.envs.offline import SequentialTradingEnv, SequentialTradingEnvConfig\n\ndef custom_preprocessing(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Add technical indicators as features.\n\n    IMPORTANT: All feature columns must start with 'features_' prefix.\n    \"\"\"\n    # Basic OHLCV features (always include these)\n    df[\"features_open\"] = df[\"open\"]\n    df[\"features_high\"] = df[\"high\"]\n    df[\"features_low\"] = df[\"low\"]\n    df[\"features_close\"] = df[\"close\"]\n    df[\"features_volume\"] = df[\"volume\"]\n\n    # RSI (Relative Strength Index)\n    df[\"features_rsi_14\"] = ta.momentum.RSIIndicator(\n        df[\"close\"], window=14\n    ).rsi()\n\n    # MACD (Moving Average Convergence Divergence)\n    macd = ta.trend.MACD(df[\"close\"])\n    df[\"features_macd\"] = macd.macd()\n    df[\"features_macd_signal\"] = macd.macd_signal()\n    df[\"features_macd_histogram\"] = macd.macd_diff()\n\n    # Bollinger Bands\n    bollinger = ta.volatility.BollingerBands(df[\"close\"], window=20, window_dev=2)\n    df[\"features_bb_high\"] = bollinger.bollinger_hband()\n    df[\"features_bb_mid\"] = bollinger.bollinger_mavg()\n    df[\"features_bb_low\"] = bollinger.bollinger_lband()\n\n    # Fill NaN values (important!)\n    df.fillna(0, inplace=True)\n\n    return df\n\n# Use in environment config\nconfig = SequentialTradingEnvConfig(\n    feature_preprocessing_fn=custom_preprocessing,\n    time_frames=[\"1min\", \"5min\", \"15min\"],  # Note: use \"1hour\" not \"60min\"\n    window_sizes=[12, 8, 8],\n    execute_on=(5, \"Minute\"),\n    initial_cash=1000\n)\n\nenv = SequentialTradingEnv(df, config)\n</code></pre>"},{"location":"guides/custom-features/#example-2-normalized-features-recommended","title":"Example 2: Normalized Features (Recommended)","text":"<p>Feature normalization is critical for stable RL training. The recommended approach is to normalize features during preprocessing using sklearn's StandardScaler, which avoids device-related issues with TorchRL's VecNorm transforms.</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef normalized_preprocessing(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Normalize features using StandardScaler for stable training.\n\n    This approach is preferred over VecNormV2/ObservationNorm transforms\n    which can have device compatibility issues on GPU.\n    \"\"\"\n    # Basic OHLCV\n    df[\"features_open\"] = df[\"open\"]\n    df[\"features_high\"] = df[\"high\"]\n    df[\"features_low\"] = df[\"low\"]\n    df[\"features_close\"] = df[\"close\"]\n    df[\"features_volume\"] = df[\"volume\"]\n\n    # Price changes (returns)\n    df[\"features_return\"] = df[\"close\"].pct_change()\n\n    # Normalize features\n    scaler = StandardScaler()\n    feature_cols = [col for col in df.columns if col.startswith(\"features_\")]\n\n    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n\n    # Fill NaN values\n    df.fillna(0, inplace=True)\n\n    return df\n\nconfig = SequentialTradingEnvConfig(\n    feature_preprocessing_fn=normalized_preprocessing,\n    ...\n)\n</code></pre> <p>Alternative approaches: - TorchRL transforms - VecNormV2 and ObservationNorm are available but may have device compatibility issues - Network level - Use BatchNorm, LayerNorm, or other normalization layers in your policy network</p> <p>Advanced Normalization</p> <p>StandardScaler uses fixed statistics from training data. For data with regime changes, consider rolling window normalization or per-regime scalers. For most use cases, StandardScaler is sufficient.</p>"},{"location":"guides/custom-features/#important-rules","title":"Important Rules","text":"<ol> <li>Feature prefix: All columns MUST start with <code>features_</code> (e.g., <code>features_rsi_14</code>). Columns without this prefix are ignored.</li> <li>Handle NaN: Technical indicators produce NaN at the start. Always call <code>df.fillna(0, inplace=True)</code> (or <code>ffill</code>/<code>bfill</code>).</li> <li>Return the DataFrame: Your function must <code>return df</code>.</li> <li>No lookahead bias: Only use past data. Never use <code>.shift(-1)</code> or future values.</li> </ol>"},{"location":"guides/custom-features/#common-technical-indicators","title":"Common Technical Indicators","text":""},{"location":"guides/custom-features/#quick-reference-table","title":"Quick Reference Table","text":"Category Indicator ta Library Code Use Case Momentum RSI <code>ta.momentum.RSIIndicator(close, window=14).rsi()</code> Overbought/oversold detection Stochastic <code>ta.momentum.StochasticOscillator(high, low, close).stoch()</code> Momentum confirmation Williams %R <code>ta.momentum.WilliamsRIndicator(high, low, close, lbp=14).williams_r()</code> Short-term overbought/oversold Trend SMA <code>ta.trend.SMAIndicator(close, window=20).sma_indicator()</code> Trend direction EMA <code>ta.trend.EMAIndicator(close, window=20).ema_indicator()</code> Responsive trend following MACD <code>ta.trend.MACD(close).macd()</code> Trend changes ADX <code>ta.trend.ADXIndicator(high, low, close, window=14).adx()</code> Trend strength Volatility Bollinger Bands <code>ta.volatility.BollingerBands(close, window=20)</code> Volatility and price bounds ATR <code>ta.volatility.AverageTrueRange(high, low, close, window=14).average_true_range()</code> Volatility measurement Keltner <code>ta.volatility.KeltnerChannel(high, low, close)</code> Alternative to Bollinger Volume OBV <code>ta.volume.OnBalanceVolumeIndicator(close, volume).on_balance_volume()</code> Accumulation/distribution VPT <code>ta.volume.VolumePriceTrendIndicator(close, volume).volume_price_trend()</code> Volume-price confirmation ADI <code>ta.volume.AccDistIndexIndicator(high, low, close, volume).acc_dist_index()</code> Money flow"},{"location":"guides/custom-features/#usage-pattern","title":"Usage Pattern","text":"<pre><code>import ta\n\ndef add_indicators(df: pd.DataFrame) -&gt; pd.DataFrame:\n    # Basic OHLCV\n    df[\"features_close\"] = df[\"close\"]\n    df[\"features_volume\"] = df[\"volume\"]\n    # ... other OHLCV ...\n\n    # Pick indicators from table above\n    df[\"features_rsi_14\"] = ta.momentum.RSIIndicator(df[\"close\"], window=14).rsi()\n    df[\"features_sma_20\"] = ta.trend.SMAIndicator(df[\"close\"], window=20).sma_indicator()\n    df[\"features_atr\"] = ta.volatility.AverageTrueRange(\n        df[\"high\"], df[\"low\"], df[\"close\"], window=14\n    ).average_true_range()\n\n    df.fillna(0, inplace=True)\n    return df\n</code></pre>"},{"location":"guides/custom-features/#performance-tips","title":"Performance Tips","text":"<ul> <li>Vectorize: Use pandas operations (<code>df[\"close\"].pct_change()</code>) instead of loops \u2014 100x faster.</li> <li>Check NaN: Add <code>df.isna().sum()</code> during development to catch indicator issues before <code>fillna</code>.</li> </ul>"},{"location":"guides/custom-features/#recommended-libraries","title":"Recommended Libraries","text":"Library Indicators Installation Best For ta 40+ <code>pip install ta</code> Standard indicators, easy API pandas-ta 130+ <code>pip install pandas-ta</code> Comprehensive collection TA-Lib 150+ <code>pip install TA-Lib</code> Performance, industry standard sklearn N/A <code>pip install scikit-learn</code> Feature scaling, normalization <p>Recommendation: Start with <code>ta</code> for simplicity, use <code>TA-Lib</code> if you need maximum performance.</p>"},{"location":"guides/custom-features/#next-steps","title":"Next Steps","text":"<ul> <li>Reward Functions - Design reward signals that work with your features</li> <li>Understanding the Sampler - How multi-timeframe sampling works</li> <li>Transforms - Alternative feature engineering with Chronos embeddings</li> <li>Offline Environments - Apply custom features to environments</li> </ul>"},{"location":"guides/metrics/","title":"Performance Metrics","text":"<p>TorchTrade provides a comprehensive set of trading performance metrics to evaluate agent performance. These metrics help assess risk-adjusted returns, drawdown characteristics, and trading efficiency.</p>"},{"location":"guides/metrics/#overview","title":"Overview","text":"<p>The <code>torchtrade.metrics</code> module provides standard trading metrics including:</p> Metric Description Typical Good Value Total Return Overall portfolio return &gt; 0% Sharpe Ratio Risk-adjusted return (volatility) &gt; 1.0 (&gt; 2.0 excellent) Sortino Ratio Risk-adjusted return (downside volatility) &gt; 1.5 Calmar Ratio Return per unit of maximum drawdown &gt; 1.0 Max Drawdown Largest peak-to-trough decline &lt; 20% Max DD Duration Length of maximum drawdown period Shorter is better Win Rate Percentage of profitable periods &gt; 50% Profit Factor Total profits / total losses &gt; 1.5 Number of Trades Total trades executed Depends on strategy"},{"location":"guides/metrics/#available-metrics","title":"Available Metrics","text":""},{"location":"guides/metrics/#individual-metrics","title":"Individual Metrics","text":"<pre><code>from torchtrade.metrics import (\n    compute_max_drawdown,\n    compute_sharpe_ratio,\n    compute_sortino_ratio,\n    compute_calmar_ratio,\n    compute_win_rate,\n)\nimport torch\n\nportfolio_values = torch.tensor([1000, 1100, 1050, 900, 950, 1200])\nreturns = torch.tensor([0.01, -0.02, 0.03, -0.01, 0.02, 0.04])\n\n# Max Drawdown - largest peak-to-trough decline\ndd = compute_max_drawdown(portfolio_values)\n# Returns: max_drawdown, max_drawdown_duration, current_drawdown, peak_value, trough_value\n\n# Sharpe Ratio - risk-adjusted return (excess return / volatility)\nsharpe = compute_sharpe_ratio(returns, periods_per_year=525600)\n\n# Sortino Ratio - like Sharpe but only penalizes downside volatility\nsortino = compute_sortino_ratio(returns, periods_per_year=525600)\n\n# Calmar Ratio - annualized return / max drawdown\ncalmar = compute_calmar_ratio(portfolio_values, periods_per_year=525600)\n\n# Win Rate - percentage of steps where reward &gt; 0\nwin_metrics = compute_win_rate(returns)\n# Returns: win_rate, avg_win, avg_loss, profit_factor\n</code></pre>"},{"location":"guides/metrics/#usage-with-historytracker","title":"Usage with HistoryTracker","text":"<p>Every TorchTrade environment records episode data in a <code>HistoryTracker</code> (see State Management). After a rollout, extract the history and feed it to the metrics:</p> <pre><code>import torch\nfrom torchtrade.metrics import compute_all_metrics\n\n# Run evaluation rollout\nobs = env.reset()\ndone = False\nwhile not done:\n    action = policy(obs)\n    obs = env.step(action)\n    done = obs[\"done\"].item()\n\n# Get history from environment\nhistory = env.history  # HistoryTracker instance\n\n# Compute all metrics\nmetrics = compute_all_metrics(\n    portfolio_values=torch.tensor(history.portfolio_values),\n    rewards=torch.tensor(history.rewards),\n    action_history=history.actions,\n    periods_per_year=525600,  # See periods table below\n)\n# Returns dict with: total_return, sharpe_ratio, sortino_ratio, calmar_ratio,\n# max_drawdown, max_dd_duration, num_trades, win_rate, avg_win, avg_loss, profit_factor\n</code></pre>"},{"location":"guides/metrics/#logging-to-weights-biases","title":"Logging to Weights &amp; Biases","text":"<pre><code>import wandb\n\nwandb.init(project=\"torchtrade-training\")\n\nfor epoch in range(num_epochs):\n    # ... training code ...\n\n    # Evaluate and compute metrics from history (as above)\n    history = eval_env.history\n    metrics = compute_all_metrics(\n        portfolio_values=torch.tensor(history.portfolio_values),\n        rewards=torch.tensor(history.rewards),\n        action_history=history.actions,\n        periods_per_year=525600,\n    )\n\n    wandb.log({f\"eval/{k}\": v for k, v in metrics.items()})\n</code></pre>"},{"location":"guides/metrics/#custom-metrics","title":"Custom Metrics","text":"<p>Create custom metrics following the same pattern \u2014 accept <code>torch.Tensor</code> rewards or portfolio values, return a scalar:</p> <pre><code>def compute_max_consecutive_wins(rewards: torch.Tensor) -&gt; int:\n    \"\"\"Maximum number of consecutive winning periods.\"\"\"\n    wins = rewards &gt; 0\n    max_streak = current = 0\n    for is_win in wins:\n        current = current + 1 if is_win else 0\n        max_streak = max(max_streak, current)\n    return max_streak\n</code></pre>"},{"location":"guides/metrics/#periods-per-year-configuration","title":"Periods Per Year Configuration","text":"<p>Choose <code>periods_per_year</code> based on your <code>execute_on</code> frequency:</p> Execute On Periods Per Year Calculation 1 minute 525,600 60 \u00d7 24 \u00d7 365 5 minutes 105,120 12 \u00d7 24 \u00d7 365 15 minutes 35,040 4 \u00d7 24 \u00d7 365 1 hour 8,760 24 \u00d7 365 4 hours 2,190 6 \u00d7 365 1 day 365 365 <p>Match <code>periods_per_year</code> to your <code>execute_on</code> config setting.</p>"},{"location":"guides/metrics/#best-practices","title":"Best Practices","text":"<ul> <li>Compare to benchmarks: Always compute buy &amp; hold return as a baseline</li> <li>Use multiple metrics: Don't rely on a single metric \u2014 check returns (Sharpe, Sortino), risk (max drawdown), and efficiency (win rate, profit factor)</li> <li>Include costs: Set realistic <code>transaction_fee</code> and <code>slippage</code> in your environment config</li> <li>Watch for extreme Sharpe: If Sharpe &gt; 10 or &lt; -10, check that <code>periods_per_year</code> matches your execution frequency</li> </ul>"},{"location":"guides/metrics/#next-steps","title":"Next Steps","text":"<ul> <li>Reward Functions - Design rewards to optimize for specific metrics</li> <li>Feature Engineering - Add features that correlate with performance</li> <li>Offline Environments - Backtest strategies with historical data</li> <li>Examples - See metrics in complete training scripts</li> </ul>"},{"location":"guides/metrics/#references","title":"References","text":"<ul> <li>Sharpe Ratio - Original paper and interpretation</li> <li>Sortino Ratio - Downside risk-adjusted returns</li> <li>Calmar Ratio - Return per unit of drawdown</li> <li>Maximum Drawdown - Peak-to-trough decline</li> </ul>"},{"location":"guides/reward-functions/","title":"Reward Functions","text":"<p>Reward functions shape your agent's behavior. TorchTrade provides a flexible system for defining custom reward functions that go beyond simple log returns.</p>"},{"location":"guides/reward-functions/#default-behavior","title":"Default Behavior","text":"<p>By default, all environments use log returns:</p> <pre><code>reward = log(portfolio_value_t / portfolio_value_t-1)\n</code></pre> <p>You can potentially improve learning and generalization by customizing this reward function. See below for some references on reward function design.</p>"},{"location":"guides/reward-functions/#how-it-works","title":"How It Works","text":"<p>Custom reward functions receive a <code>HistoryTracker</code> object and return a float reward:</p> <pre><code>def my_reward(history) -&gt; float:\n    \"\"\"Custom reward function.\"\"\"\n    return reward_value\n</code></pre> <p>Pass your reward function via the environment config:</p> <pre><code>from torchtrade.envs.offline import SequentialTradingEnv, SequentialTradingEnvConfig\n\nconfig = SequentialTradingEnvConfig(\n    reward_function=my_reward,\n    ...\n)\nenv = SequentialTradingEnv(df, config)\n</code></pre>"},{"location":"guides/reward-functions/#historytracker-fields","title":"HistoryTracker Fields","text":"<p>The <code>history</code> object passed to your reward function is a <code>HistoryTracker</code> instance with these attributes:</p> Field Type Description <code>portfolio_values</code> list[float] Portfolio value at each step <code>base_prices</code> list[float] Asset price at each step <code>actions</code> list[float] Actions taken at each step <code>rewards</code> list[float] Rewards received at each step <code>positions</code> list[float] Position sizes (positive=long, negative=short, 0=flat) <code>action_types</code> list[str] Action types (\"hold\", \"buy\", \"sell\", \"long\", \"short\") <p>All lists grow by one element per step. Use indexing (e.g., <code>history.portfolio_values[-1]</code>) to access recent values.</p>"},{"location":"guides/reward-functions/#built-in-reward-functions","title":"Built-in Reward Functions","text":"<p>TorchTrade provides three built-in reward functions in <code>torchtrade.envs.core.default_rewards</code>:</p> <pre><code>from torchtrade.envs.core.default_rewards import (\n    log_return_reward,       # Default - log(value_t / value_t-1)\n    sharpe_ratio_reward,     # Running Sharpe ratio\n    drawdown_penalty_reward, # Log return with drawdown penalty\n)\n\nconfig = SequentialTradingEnvConfig(\n    reward_function=sharpe_ratio_reward,\n    ...\n)\n</code></pre>"},{"location":"guides/reward-functions/#log_return_reward-default","title":"<code>log_return_reward</code> (default)","text":"<pre><code>reward = log(portfolio_value[-1] / portfolio_value[-2])\n</code></pre> <ul> <li>Scale-invariant, symmetric, handles compounding</li> <li>Returns <code>-10.0</code> on bankruptcy, <code>0.0</code> if insufficient history</li> </ul>"},{"location":"guides/reward-functions/#sharpe_ratio_reward","title":"<code>sharpe_ratio_reward</code>","text":"<p>Running Sharpe ratio over all historical portfolio values. Encourages risk-adjusted returns. Clipped to <code>[-10.0, 10.0]</code>.</p>"},{"location":"guides/reward-functions/#drawdown_penalty_reward","title":"<code>drawdown_penalty_reward</code>","text":"<p>Log return plus a penalty when drawdown from peak exceeds 10%. Discourages large drawdowns while rewarding gains.</p>"},{"location":"guides/reward-functions/#custom-examples","title":"Custom Examples","text":""},{"location":"guides/reward-functions/#example-1-transaction-cost-penalty","title":"Example 1: Transaction Cost Penalty","text":"<pre><code>import numpy as np\n\ndef cost_aware_reward(history) -&gt; float:\n    \"\"\"Log return scaled by trade frequency.\"\"\"\n    if len(history.portfolio_values) &lt; 2:\n        return 0.0\n\n    old_value = history.portfolio_values[-2]\n    new_value = history.portfolio_values[-1]\n\n    if old_value &lt;= 0:\n        return -10.0\n\n    log_return = np.log(new_value / old_value)\n\n    # Penalize frequent action changes\n    if len(history.actions) &gt;= 2 and history.actions[-1] != history.actions[-2]:\n        log_return -= 0.001  # Small penalty for switching\n\n    return float(log_return)\n</code></pre>"},{"location":"guides/reward-functions/#example-2-sparse-terminal-reward","title":"Example 2: Sparse Terminal Reward","text":"<pre><code>def terminal_reward(history) -&gt; float:\n    \"\"\"Only reward at episode end based on total return.\"\"\"\n    if len(history.portfolio_values) &lt; 2:\n        return 0.0\n\n    # Give zero reward during episode\n    # At terminal step, the environment handles this naturally\n    # Use total return as final reward\n    initial_value = history.portfolio_values[0]\n    current_value = history.portfolio_values[-1]\n\n    if initial_value &lt;= 0:\n        return -10.0\n\n    return float(np.log(current_value / initial_value))\n</code></pre>"},{"location":"guides/reward-functions/#example-3-multi-objective-reward","title":"Example 3: Multi-Objective Reward","text":"<pre><code>def multi_objective_reward(history) -&gt; float:\n    \"\"\"Combine returns with drawdown penalty.\"\"\"\n    if len(history.portfolio_values) &lt; 2:\n        return 0.0\n\n    old_value = history.portfolio_values[-2]\n    new_value = history.portfolio_values[-1]\n\n    if old_value &lt;= 0 or new_value &lt;= 0:\n        return -10.0\n\n    # Component 1: Log return\n    log_ret = float(np.log(new_value / old_value))\n\n    # Component 2: Drawdown from peak\n    peak = max(history.portfolio_values)\n    drawdown = (peak - new_value) / peak if peak &gt; 0 else 0.0\n    dd_penalty = -5.0 * drawdown if drawdown &gt; 0.1 else 0.0\n\n    return log_ret + dd_penalty\n</code></pre>"},{"location":"guides/reward-functions/#when-to-use-each-reward","title":"When to Use Each Reward","text":"Reward Type Use Case Pros Cons Log Return (default) General purpose Simple, stable Ignores costs, risk Sharpe Ratio Risk-adjusted performance Considers volatility Requires history, unstable early Drawdown Penalty Risk management Controls max loss May exit winners early Terminal Sparse Episodic tasks Clear objective Sparse signal, slow learning Cost Penalty Reduce overtrading Discourages frequent trades May miss opportunities"},{"location":"guides/reward-functions/#design-principles","title":"Design Principles","text":""},{"location":"guides/reward-functions/#1-dense-vs-sparse-rewards","title":"1. Dense vs Sparse Rewards","text":"<p>Dense rewards (every step): - Faster learning - More signal for gradient updates - Risk: May optimize for short-term gains</p> <p>Sparse rewards (terminal only): - Aligns with true objective (final return) - Less noise - Risk: Slower learning, credit assignment problem</p>"},{"location":"guides/reward-functions/#2-normalize-rewards","title":"2. Normalize Rewards","text":"<p>Keep rewards in a consistent range for stable learning:</p> <pre><code># Clip extreme values\nreward = np.clip(reward, -10.0, 10.0)\n</code></pre>"},{"location":"guides/reward-functions/#3-avoid-lookahead-bias","title":"3. Avoid Lookahead Bias","text":"<p>Only use information available at decision time \u2014 use past values from <code>history</code>, never future data.</p>"},{"location":"guides/reward-functions/#4-consider-environment-type","title":"4. Consider Environment Type","text":"<p>Sequential Environments: Full history available for complex rewards One-Step Environments: Limited to immediate reward (minimal history) Futures Environments: Consider liquidation risk in reward</p>"},{"location":"guides/reward-functions/#references","title":"References","text":""},{"location":"guides/reward-functions/#research-papers-on-reward-design","title":"Research Papers on Reward Design","text":"<ul> <li>Reward Shaping for Reinforcement Learning in Financial Trading - Comprehensive study on reward function design for trading applications</li> <li>Deep Reinforcement Learning for Trading - Research on RL approaches and reward engineering for financial markets</li> <li>Reward Function Design in Deep Reinforcement Learning for Financial Trading - Analysis of different reward formulations and their impact on trading performance</li> </ul>"},{"location":"guides/reward-functions/#next-steps","title":"Next Steps","text":"<ul> <li>Feature Engineering - Engineer features that support your reward function</li> <li>Understanding the Sampler - How data flows through environments</li> <li>Loss Functions - Training objectives that work with rewards</li> <li>Offline Environments - Apply custom rewards to environments</li> </ul>"},{"location":"guides/sampler/","title":"Understanding the Sampler","text":"<p>The <code>MarketDataObservationSampler</code> (found in <code>torchtrade/envs/offline/sampler.py</code>) handles multi-timeframe data sampling in TorchTrade's offline environments. It resamples high-frequency data (1-minute bars) to multiple timeframes and creates synchronized observation windows while preventing lookahead bias. This allows RL agents to observe market patterns across different time scales simultaneously, from short-term momentum to long-term trends.</p>"},{"location":"guides/sampler/#what-is-the-sampler","title":"What Is the Sampler?","text":"<p>The sampler:</p> <ol> <li>Resamples 1-minute OHLCV to multiple timeframes (5m, 15m, 1h)</li> <li>Applies feature preprocessing to each timeframe</li> <li>Creates sliding windows of market data</li> <li>Prevents lookahead bias by correct bar indexing</li> </ol> <pre><code>1-Minute Data \u2192 Sampler \u2192 Multi-Timeframe Observations\n                  \u251c\u2500\u2500 Resample to timeframes\n                  \u251c\u2500\u2500 Apply preprocessing\n                  \u2514\u2500\u2500 Create windows\n</code></pre>"},{"location":"guides/sampler/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/sampler/#how-it-works","title":"How It Works","text":"<p>The sampler takes your 1-minute OHLCV data, resamples it to multiple timeframes, and provides synchronized observation windows at each execution step. Here's a direct example of using the sampler:</p> <pre><code>import pandas as pd\nfrom torchtrade.envs.offline.sampler import MarketDataObservationSampler\nfrom torchtrade.envs.offline.utils import TimeFrame, TimeFrameUnit\n\n# Load your OHLCV data\ndf = pd.read_csv(\"btcusdt_1m.csv\")\n# Required columns: timestamp, open, high, low, close, volume\n\n# Create sampler\nsampler = MarketDataObservationSampler(\n    df=df,\n    time_frames=[\n        TimeFrame(1, TimeFrameUnit.Minute),\n        TimeFrame(5, TimeFrameUnit.Minute),\n        TimeFrame(15, TimeFrameUnit.Minute),\n    ],\n    window_sizes=[12, 8, 8],\n    execute_on=TimeFrame(5, TimeFrameUnit.Minute),\n)\n\n# Get observations\nobs_dict, timestamp, truncated = sampler.get_sequential_observation()\n\n# obs_dict contains:\n# {\n#   \"market_data_1Minute\": torch.Tensor([12, num_features]),\n#   \"market_data_5Minute\": torch.Tensor([8, num_features]),\n#   \"market_data_15Minute\": torch.Tensor([8, num_features]),\n# }\n\n# Reset for new episode\nsampler.reset(random_start=True)\n</code></pre>"},{"location":"guides/sampler/#usage-in-offline-environments","title":"Usage in Offline Environments","text":"<p>The sampler is used in all offline environments (SequentialTradingEnv, SequentialTradingEnvSLTP, OneStepTradingEnv) and allows flexible selection of timeframes through the environment configuration:</p> <pre><code>from torchtrade.envs.offline import SequentialTradingEnv, SequentialTradingEnvConfig\n\n# Configure multi-timeframe sampling\nconfig = SequentialTradingEnvConfig(\n    time_frames=[\"1min\", \"5min\", \"15min\", \"1hour\"],\n    window_sizes=[12, 8, 8, 24],\n    execute_on=(5, \"Minute\"),\n)\n\nenv = SequentialTradingEnv(df, config)\n\n# Observations contain all timeframes\nobs = env.reset()\n# obs[\"market_data_1Minute\"]: (12, features)\n# obs[\"market_data_5Minute\"]: (8, features)\n# obs[\"market_data_15Minute\"]: (8, features)\n# obs[\"market_data_1Hour\"]: (24, features)\n</code></pre>"},{"location":"guides/sampler/#how-resampling-works","title":"How Resampling Works","text":""},{"location":"guides/sampler/#timeframe-alignment","title":"Timeframe Alignment","text":"<p>The sampler resamples your 1-minute OHLCV data to multiple timeframes (5min, 15min, 1hour, etc.) and ensures all observations are synchronized at each execution step.</p> <p>Example: execute_on=5Minute</p> <pre><code>Time (minutes):     0    5    10   15   20   25   30\n                    |----|----|----|----|----|----|\n1-minute bars:      60 bars available\n5-minute bars:      |  A  |  B  |  C  |  D  |  E  |  F  |\n15-minute bars:     |      X      |      Y      |      Z      |\n\nExecute at:              \u2191         \u2191         \u2191\n                        t=5      t=10      t=15\n</code></pre> <p>At t=10 (executing on 5-minute bar B): - 1-minute data: Last 12 bars (from recent history) - 5-minute data: Bar A (completed at t=5) - 15-minute data: Bar X (completed at t=0)</p>"},{"location":"guides/sampler/#lookahead-bias-prevention","title":"Lookahead Bias Prevention","text":"<p>The Problem: In real trading, you can't use a bar's data until it has fully closed. A 15-minute bar spanning 0-15 minutes isn't complete until minute 15.</p> <p>The Solution: The sampler indexes higher timeframe bars by their END time, not their START time:</p> <pre><code># Without fix (WRONG - causes lookahead bias):\n# 15-min bar covering [0-15] is indexed at t=0\n# At t=10, agent could see bar [0-15] before it closes at t=15 \u274c\n\n# With fix (CORRECT - in sampler.py lines 71-77):\n# 15-min bar covering [0-15] is indexed at t=15 (its END time)\n# At t=10, agent can only see bars that closed BEFORE t=10 \u2705\n</code></pre> <p>Detailed Example at t=10:</p> <p>When your agent executes at minute 10, here's what data is available:</p> <pre><code>\u2705 CAN use (completed bars only):\n  - 1-min bars: [1, 2, 3, ..., 9] (bar 10 is still forming)\n  - 5-min bar A [0-5]: Closed at t=5, fully complete\n  - 15-min bar covering previous period: Only if it ended before t=10\n\n\u274c CANNOT use (incomplete bars):\n  - 5-min bar B [5-10]: Still forming, closes at t=10\n  - 15-min bar X [0-15]: Still forming, closes at t=15\n</code></pre> <p>Why This Matters: Without this protection, your agent would train on future information (looking into bars that haven't closed yet), leading to unrealistic backtest results that won't work in live trading.</p> <p>Implementation Detail (from <code>sampler.py:71-77</code>):</p> <p>Higher timeframes (coarser than <code>execute_on</code>) are shifted forward by their period during resampling. This ensures bars are indexed by their END time. When the agent queries data at execution time, <code>searchsorted</code> automatically excludes any bars that haven't closed yet.</p>"},{"location":"guides/sampler/#common-configuration-patterns","title":"Common Configuration Patterns","text":"Pattern time_frames window_sizes execute_on Use Case Single Timeframe <code>[\"1min\"]</code> <code>[100]</code> <code>(1, \"Minute\")</code> High-frequency, simple features Multi-Timeframe <code>[\"1min\", \"5min\", \"15min\"]</code> <code>[12, 8, 8]</code> <code>(5, \"Minute\")</code> Capture multiple market rhythms Hierarchical <code>[\"1min\", \"5min\", \"15min\", \"60min\", \"240min\"]</code> <code>[12, 8, 8, 24, 48]</code> <code>(5, \"Minute\")</code> Complex strategies, trend analysis Long-Term <code>[\"60min\", \"240min\", \"1440min\"]</code> <code>[24, 24, 30]</code> <code>(60, \"Minute\")</code> Position trading, low frequency"},{"location":"guides/sampler/#key-parameters","title":"Key Parameters","text":"Parameter Type Description Example <code>time_frames</code> list[str] Timeframes as strings (e.g., \"1min\", \"5min\", \"1h\") <code>[\"1min\", \"5min\", \"15min\"]</code> <code>window_sizes</code> list[int] Lookback window per timeframe <code>[12, 8, 8]</code> <code>execute_on</code> tuple (value, \"Minute\"/\"Hour\") <code>(5, \"Minute\")</code> <code>feature_preprocessing_fn</code> callable Transform OHLCV before windowing <code>add_indicators</code>"},{"location":"guides/sampler/#window-size-selection","title":"Window Size Selection","text":"<p>Choose window sizes based on the information needed:</p> <p>For 1-minute timeframe: - 12 bars = 12 minutes of data (short-term) - 60 bars = 1 hour of data (medium-term) - 240 bars = 4 hours of data (long-term)</p> <p>For 5-minute timeframe: - 8 bars = 40 minutes - 12 bars = 1 hour - 24 bars = 2 hours</p> <p>Rule of thumb: Higher timeframes need fewer bars (they already capture more history per bar).</p>"},{"location":"guides/sampler/#common-issues","title":"Common Issues","text":"Issue Symptom Solution NaN values in observations Training crashes Fill NaN in <code>feature_preprocessing_fn</code> with <code>df.fillna(0)</code> Episode too short Episode ends after few steps Check data length covers <code>max(window_sizes) * max(time_frames) + episode_length</code> Misaligned timeframes Unexpected data patterns Use <code>execute_on</code> that's a multiple of all <code>time_frames</code> Memory issues OOM errors Reduce <code>window_sizes</code> or number of <code>time_frames</code> Slow sampling Environment init takes long Cache preprocessing results or simplify indicator calculations"},{"location":"guides/sampler/#performance-tips","title":"Performance Tips","text":"<ul> <li>Vectorize preprocessing: Use pandas operations (<code>df[\"close\"].rolling(20).mean()</code>) instead of loops.</li> <li>Appropriate window sizes: Larger windows = more memory. Keep <code>sum(window_sizes) \u00d7 num_features</code> &lt; 1000 total values per observation.</li> </ul>"},{"location":"guides/sampler/#technical-reference","title":"Technical Reference","text":"<ul> <li>Source: <code>torchtrade/envs/offline/sampler.py</code></li> <li>Resampling Logic: Uses pandas <code>resample().agg()</code> with OHLCV aggregation rules</li> <li>Indexing: Execution times mapped to 1-minute bar indices, then resampled timeframes aligned</li> </ul>"},{"location":"guides/sampler/#next-steps","title":"Next Steps","text":"<ul> <li>Feature Engineering - Add technical indicators via preprocessing</li> <li>Reward Functions - Design rewards that work with your sampled data</li> <li>Offline Environments - Apply sampler configuration to environments</li> </ul>"}]}